\documentclass[a4paper,12pt]{report}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[italian]{babel}
\usepackage{setspace}
\usepackage{amsmath,amssymb,bm,graphicx}
%\usepackage{times,courier,mathtime}
\usepackage{rotating}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsfig}

\usepackage{makeidx}

\makeindex

\newcommand{\newchap}[1]{
\chapter{#1}
\markright{Capitolo \thechapter. {#1}\hfill}
}
\renewcommand{\ttdefault}{pcr}
\newcommand{\kw}[1]{\textbf{#1}}
\newcommand{\comment}[1]{\textit{#1}}
\newcommand{\Line}[0]{\hrule\rule{0cm}{0cm}}

\renewcommand\emptyset{\varnothing}

\newtheorem{defi}{Definizione}
\newtheorem{teo}{Teorema}

\begin{document}

%\doublespacing
%\onehalfspacing
\setstretch{1.5}

\frenchspacing

\newpage
\thispagestyle{empty}

{\setstretch{1}
\begin{center}
\textsc{\large{Università degli Studi di Milano}}\\
\textsc{\small{Facoltà di Scienze Matematiche, Fisiche e Naturali}}\\
\hspace*{\fill}\\
\textsc{\small{Corso di Laurea in Informatica}}\\
\begin{figure}[htp]
\begin{center}
\includegraphics[width=2cm]{eps/universitas.eps}
\end{center}
\end{figure}
\hspace*{\fill}\\
\hspace*{\fill}\\
{\setstretch{1.5}
\textsc{\large{\textbf{Studio di euristiche per il miglioramento di\\algoritmi di ranking per il World--Wide Web}}}\\
}
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\end{center}
\begin{tabular}{rl}
Relatore: & Dr. Sebastiano Vigna\\
Correlatori: & Dr. Paolo Boldi\\
& Dr. Massimo Santini
\end{tabular}
\begin{flushright}
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
Tesi di Laurea di:\\
Marco Olivo\\
Matr. n. 592150
\end{flushright}
\begin{center}
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\hspace*{\fill}\\
\textsc{\footnotesize{Anno Accademico 2002-2003}}
\end{center}
}

%%%
%%% Fine copertina
%%%

\newpage
\thispagestyle{empty}

\begin{flushright}
\hspace*{\fill}\\
\textit{A Milano, la mia città}\\
\end{flushright}

%%%
%%% Fine dedica
%%%

\newpage
\thispagestyle{empty}

\begin{flushright}
\hspace*{\fill}\\
{\it Desidero ringraziare anzitutto\\
Sebastiano Vigna, Paolo Boldi e Massimo Santini\\
non solo per avermi seguito in tutti questi mesi come\\
nessun altro avrebbe mai fatto, ma anche --- anzi,\\
soprattutto --- per avermi dato l'occasione di poter coronare un\\
mio grande sogno: quello di costruire un motore di ricerca.\\
Senza di loro questa tesi non sarebbe mai esistita...\\
\hspace*{\fill}\\
Un grazie particolare a mio zio Gian Paolo,\\
per essermi stato sempre vicino e per avermi sempre trattato\\
da adulto, anche quando adulto non lo ero affatto.\\
\hspace*{\fill}\\
Ed infine, ma non certo per ordine di importanza,\\
ringrazio i miei genitori per avermi dato tutto, sacrificando\\
spesso i loro sogni per accontentarmi. \`E grazie a voi se\\
sono arrivato alla Laurea, ed in tempi da record.\\
\hspace*{\fill}\\
Grazie mamma e papà.
}
\end{flushright}

%%%
%%% Fine ringraziamenti
%%%

\newpage
\setcounter{page}{1}
\renewcommand{\thepage}{\roman{page}}

\tableofcontents

%%%
%%% Fine indice
%%%

\newpage
\pagestyle{myheadings}
\renewcommand{\thepage}{\arabic{page}}

\addtolength{\parskip}{0.2\baselineskip}

\newchap{Introduzione}

Uno dei problemi aperti dell'informatica e dell'\textit{information
retrieval} più affascinanti degli ultimi anni è quello di rispondere
alle interrogazioni poste da esseri umani ai motori di ricerca.

I motori di ricerca si distinguono dalle basi di dati e dai sistemi di
\textit{information retrieval} tradizionali per diverse ragioni: il
World--Wide Web (web) è immenso\footnote{Si calcola che, compressa, la
porzione di web ad oggi raggiungibile occupi circa 50 terabyte, per un
totale di oltre tre miliardi di pagine.}, raddoppia di dimensione ogni
dodici mesi e si aggiorna con una frequenza impredicibile; inoltre i
dati non sono strutturati e sono altamente eterogenei nel contenuto e
nella qualità.

Il recupero di tali quantità di dati è reso difficile non soltanto
dalla loro mole, ma anche da una serie di fattori legati
all'affidabilità dei server su cui tali dati vengono ospitati e dalle
reti attraversate per raggiungerli.

La studio dei motori di ricerca è relativamente recente, e le
pubblicazioni in letteratura su questo argomento, per quanto esso sia
molto dibattuto, tendono ad essere molto poche e soprattutto molto
poco approfondite; tra i motivi che spingono i ricercatori che se ne
occupano a non divulgare tutti i dettagli del loro lavoro vi sono
forti interessi economici legati all'uso commerciale dei motori di
ricerca.

\bigskip
Lo scopo di questa tesi è stata la costruzione di un motore di ricerca
scalabile, preciso e soprattutto flessibile che potesse competere con
i motori di ricerca commerciali --- se non per numero di pagine
trattate, almeno per qualità dei risultati.  Per raggiungere questi
obbiettivi si è partiti da una implementazione accurata dei migliori
algoritmi noti in letteratura, a cui è seguita una fase di affinamento
basata da un lato su considerazioni di tipo ingegneristico e
dall'altro su un attento confronto dei risultati ottenuti con quelli
restituiti dai motori di ricerca commerciali in risposta a varie
interrogazioni.

Lo schema seguito da un motore di ricerca nel momento in cui un utente
inserisce un'interrogazione è, in prima approssimazione, il seguente.
Anzitutto il motore opera una scrematura delle pagine che soddisfano
l'interrogazione sottopostagli, in maniera tale da eliminare subito un
numero significativo di pagine che probabilmente non interessano
all'utente; ciononostante, a causa delle dimensioni del web, il numero
di pagine rimanenti è in generale molto elevato. Pertanto,
l'ordinamento relativo di tali pagine è forse la cosa più importante
che un motore di ricerca si deve occupare di fornire, in quanto è
proprio tale ordinamento a dare all'utente la sensazione di aver
trovato quel che stava cercando.

Una tecnica possibile, ad esempio, è quella di preferire le pagine a
cui si riferiscono molte altre pagine, oppure quelle in cui i termini
dell'interrogazione compaiono in posizioni ravvicinate. Altre
tecniche, più sofisticate, si basano sulla struttura di
interconnessione tra le pagine del web.

\bigskip
Nei primi capitoli vengono anzitutto presentati ed analizzati alcuni
algoritmi noti proposti da tempo in letteratura e che si reputa
vengano utilizzati dai più famosi motori di ricerca commerciali per
decidere l'ordine di presentazione dei risultati.  Alcuni di questi
algoritmi sono stati utilizzati nel motore di ricerca che costituisce
lo scopo di questa tesi.

Nei capitoli successivi si analizzano alcune euristiche tese anzitutto
a migliorare la qualità dei risultati e che cercano di fornire
risposte più mirate e precise alle interrogazioni sottoposte al motore
di ricerca, in maniera da emulare e, per quanto possibile, migliorare
lo stato attuale dell'arte, rappresentato in questo caso dai motori di
ricerca commerciali; in seconda battuta, vengono presentate altre
euristiche volte a diminuire i tempi di risposta alle interrogazioni,
in maniera da rendere realistico l'utilizzo del motore di ricerca
sviluppato.

Infine, è stata studiata ed implementata una tecnica per aggregare i
risultati dei vari algoritmi tra di loro in maniera efficiente ed in
modo altamente parametrico, così da rendere possibile la
sperimentazione dell'impatto dei vari algoritmi sulla qualità della
risposta.

%%%
%%% Fine introduzione
%%%

\newpage

\newchap{Algoritmi di ranking}

Un \emph{algoritmo di ranking} è un algoritmo che è preposto
all'assegnazione di un punteggio a ciascuna delle pagine di una
collezione di pagine web: tale punteggio dovrebbe riflettere la
significatività di quella pagina in senso assoluto o relativo ad una
\textit{query}\footnote{Nel corso di questa tesi diremo intercambiabilmente
\textit{query} od interrogazione, intendendo lo stesso significato.}.
Talvolta, in questa tesi, saranno anche chiamati \emph{ranker}, ossia
``assegnatori di punteggi''. Pur essendo possibile fornire numerose
tipologie di punteggi, per quanto riguarda la ricerca su web ci si
affida tipicamente ad algoritmi che si possono suddividere, \textit{in
primis}, in due famiglie:

\begin{itemize}
\item algoritmi su grafo;
\item algoritmi sul contenuto.
\end{itemize}

La prima classe di algoritmi annovera al suo interno algoritmi quali
PageRank\index{PageRank} e Hits\index{Hits}, il primo molto usato nei
motori di ricerca, come ad esempio Google\index{Google}~\cite{google}:
essi sono noti da qualche anno in letteratura e verranno presentati
per illustrare l'attuale stato dell'arte.  In questi algoritmi vi è la
necessità di manipolare il grafo\index{grafo} del web\footnote{O una
porzione significativa di esso. Per grafo del web, rimandando ad una
definizione più formale nel paragrafo~\ref{paragrafo_grafo_web}, qui
intendiamo il grafo che si ottiene considerando le pagine come nodi e
i link tra le pagine come archi del grafo.}: questo è possibile grazie
al recupero delle pagine in rete e ad una successiva
elaborazione. Questi algoritmi, proprio perché hanno come input
primario il grafo\index{grafo}, sono anche detti
\emph{algoritmi esogeni}, in quanto per stabilire il punteggio delle
pagine essi si basano sulle proprietà strutturali del
grafo\index{grafo}, e non sul contenuto testuale delle pagine.

La seconda classe di algoritmi invece è stata solamente in parte
trattata in letteratura; tra gli algoritmi noti si ricordano
LSI\index{LSI} e Proximity\index{Proximity}, molto utilizzati per la
ricerca su web. Questa seconda classe di algoritmi per assegnare il
punteggio tende a considerare il contenuto intrinseco delle pagine, e
per questo motivo questi algoritmi sono spesso chiamati
\emph{algoritmi endogeni}.

Ognuno degli algoritmi che verranno illustrati si compone inoltre di
due parti, una che viene precomputata, l'altra che viene calcolata al
momento dell'interrogazione. Solitamente la parte che viene
precomputata è anche la più costosa, mentre la parte da calcolare
quando perviene l'interrogazione dovrebbe essere più rapida.  In
alcuni algoritmi, quali PageRank, la parte calcolata al momento
dell'interrogazione è del tutto inesistente, se escludiamo
l'operazione di restituire i risultati trovati.

%%%
%%% Fine presentazione tipi di algoritmi
%%%

\newpage

\newchap{Algoritmi su grafo}
\label{capitolo_algoritmi_su_grafo}

Come visto, una classe di algoritmi di \textit{ranking} per funzionare
necessita di disporre di un grafo\index{grafo} del web, o di una
sottoparte di esso.  In questa sezione si presenteranno i più noti
algoritmi di questo tipo.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/pagerank1.eps}}
\caption{Le pagine \texttt{A} e \texttt{C} puntano a \texttt{B}.}
\end{center}
\end{figure}

\newpage

\section{Il grafo del web}
\label{paragrafo_grafo_web}

Si consideri un qualunque insieme di pagine web: si può pensare alle
pagine come a dei nodi di un grafo mentre gli archi sono rappresentati
dai link con cui le pagine si puntano.  Questa semplice visione del
web permette di costruire un grafo a partire da un qualunque insieme
di pagine recuperate tramite \textit{crawl} e la costruzione di questo
grafo permette di poter fare delle analisi riguardanti la struttura
ipertestuale del web e di eseguire noti algoritmi quali PageRank e
Hits, che verranno approfonditi nelle sezioni seguenti.

\subsection{Generalità sui grafi: notazioni e definizioni}

\begin{defi}
\label{definizione_1}
Un \emph{grafo (orientato)} $G = (V,E)$ è dato da un insieme $V$ di nodi
e un insieme $E \subseteq V \times V$ di archi. La notazione $x \to_G
y$ indica che c'è un arco in $G$ da $x$ a $y$ (cioè che $(x,y) \in
E$); se $G$ è chiaro dal contesto, scriveremo semplicemente $x \to
y$. In questo caso, $x$ si chiama \emph{sorgente} dell'arco, e $y$
\emph{destinazione} dell'arco.  L'insieme degli archi che hanno $x$
come sorgente (destinazione, rispettivamente) si indica con $G(x,-)$
($G(-,x)$); tali archi vengono chiamati archi \emph{uscenti} da
(\emph{entranti} in) $x$.  I nodi destinazione di archi uscenti da $x$
vengono chiamati \emph{successori} di $x$; i nodi sorgente di archi
entranti in $x$ vengono chiamati \emph{predecessori} di $x$.  Quando
non ci sia rischio di confusione, indicheremo con $G(x,-)$ e $G(-,x)$
anche l'insieme dei successori e predecessori di $x$.  Un nodo senza
successori è chiamato \emph{pozzo}.
\end{defi}

\begin{defi}
\label{definizione_2}
Sia $G$ un grafo, e $x,y \in V$ due nodi. Un \emph{cammino (orientato)
da $x$ a $y$} è una sequenza $x_0, x_1, \dots, x_n$ ($n \geq 0$) di
nodi tali che $x = x_0$, $y = x_n$ e $x_{i-1} \to x_i$ per ogni $i =
1, \dots, n$.  Se esiste un cammino da $x$ a $y$ si dice che $y$ è
\emph{raggiungibile} da $x$.  L'insieme dei nodi raggiungibili da $x$
si denota con $G^*(x,-)$; l'insieme dei nodi da cui $x$ è
raggiungibile sarà analogamente denotato con $G^*(-,x)$.  Un cammino
non vuoto (cioè con $n\neq 0$) da un nodo $x$ a se stesso viene
chiamato \emph{ciclo}. Se non ci sono cicli il grafo viene chiamato
\emph{aciclico}.
\end{defi}

\begin{defi}
\label{definizione_3}
Sia $G$ un grafo, e $x,y \in V$ due nodi; diciamo che i due nodi sono
\emph{coraggiungibili} nel grafo sse $x \in G^*(y,-)$ e $y \in
G^*(x,-)$, cioè sse $x$ è raggiungibile da $y$ e viceversa.
\`E banale dimostrare che la relazione di coraggiungibilità è una
relazione di equivalenza fra nodi. Le classi di equivalenza di questa
relazione vengono chiamate \emph{componenti (fortemente) connesse} del
grafo $G$. Se c'è un'unica componente fortemente connessa diremo che
$G$ è fortemente connesso.
\end{defi}

\begin{defi}
\label{definizione_4}
Sia $G$ un grafo. Definiamo un nuovo grafo $G'$ come segue:
\begin{itemize}
\item i nodi di $G'$ sono le componenti fortemente connesse di $G$;
\item c'è un arco dal nodo $c_1$ al nodo $c_2$ sse esiste
un nodo $x_1$ nella classe $c_1$ e un nodo $x_2$ nella classe $c_2$
tali che $x_1 \to_G x_2$.
\end{itemize}

Il grafo $G'$ viene chiamato \emph{grafo ridotto} di $G$.
\end{defi}

\`E facile verificare che il grafo $G'$ è aciclico.

\subsection{Grafi e processi stocastici}

\begin{defi}
\label{definizione_5}
Una \emph{matrice stocastica} è una matrice quadrata $M$ con le
seguenti proprietà:
\begin{itemize}
\item gli elementi della matrice sono reali nell'intervallo $[0,1]$;
\item la somma degli elementi di ciascuna riga è $1$.
\end{itemize}
\end{defi}

Indichiamo con $m_{ij}$ l'elemento di $M$ che si trova sulla $i$-esima
riga e sulla $j$-esima colonna.  Si può pensare a una matrice
stocastica come alla descrizione di un processo markoviano finito di
memoria uno (cioè, una catena di Markov finita): $m_{ij}$ rappresenta
la probabilità che il sistema passi nello stato $j$ quando si trova
nello stato $i$~\cite{markov}.

Indichiamo con $M^n$ l'$n$-esima potenza della matrice $M$ (cioè, $M$
moltiplicata con se stessa $n$ volte); con $m_{ij}^{(n)}$ indicheremo
l'elemento di indici $(i,j)$ nella matrice $M^n$.  Nella precedente
interpretazione $m_{ij}^{(n)}$ è la probabilità che il sistema
raggiunga lo stato $j$ in $n$ passi se parte dallo stato $i$.

\begin{defi}
\label{definizione_6}
Una matrice stocastica $M$ è detta:
\begin{itemize}
\item \emph{irriducibile}\footnote{Si noti che, se si chiama $G = (V,E)$
il grafo che ha per nodi gli stati della catena dove $(i,j) \in E$ sse
$m_{ij} > 0$, allora l'irriducibilità corrisponde a chiedere che $G$
sia fortemente connesso.} sse per ogni $i, j$ esiste un $n > 0$ tale
che $m_{ij}^{(n)} > 0$;
\item \emph{aperiodica} sse per ogni $i$, il massimo comun
divisore di $\{n \geq 1 \mid m_{ii}^{(n)} > 0\}$ è 1.
\end{itemize}
\end{defi}

\begin{defi}
\label{definizione_7}
Sia $M$ una matrice stocastica di dimensione $n$, e ${\vec p}$ una
distribuzione di probabilità su $n$ stati (cioè, un vettore riga
$n$-dimensionale con componenti in $[0,1]$ aventi somma $1$). Diremo
che ${\vec p}$ è una \emph{distribuzione stabile} per $M$ sse ${\vec
p} \cdot M = {\vec p}$. Si noti che se la catena è finita esiste
sempre una distribuzione stabile: l'equiprobabile.
\end{defi}

\begin{teo}
\label{teorema_1}
Se $M$ è una matrice stocastica irriducibile ed aperiodica, allora:

\begin{enumerate}
\item $M$ ammette un'unica distribuzione stabile ${\vec p}$; 
\item tale distribuzione soddisfa
\[
	p_j = \lim_{n \to \infty} m_{ij}^{(n)};
\]
\item la convergenza al limite avviene con velocità esponenziale, cioè
esistono $c \geq 0$ e $\rho < 1$ tali che
\[
	|p_j - m_{ij}^{(n)}| \leq c \cdot \rho^n;
\]
\item per ogni distribuzione iniziale ${\vec q}$, 
\[
	p_j = \lim_{n \to \infty} m_{ij}^{(n)} \cdot q_j.
\]
\item Se indichiamo con $N^{(n)}_{\vec q}(i)$ il numero di volte che il
processo (a partire dalla distribuzione iniziale $\vec q$) si trova
nello stato $i$ durante i primi $n$ passi, allora
\[
p_i = \lim_{n \to \infty} \frac { N^{(n)}_{\vec q}(i) } n
\]
per ogni $\vec q$.
\end{enumerate}

\end{teo}

Si noti che, in uniformità con l'argomento trattato, nel corso di
questa tesi i nodi del grafo verranno chiamati \emph{pagine} mentre
gli archi entranti (\emph{uscenti}, rispettivamente) verranno chiamati
\emph{link entranti} (uscenti).

\subsection{Grafo: rappresentazione e compressione}
\label{paragrafo_compressione_grafo_web}

Da un punto di vista della rappresentazione, il grafo del web ammette
diverse rappresentazioni. Tuttavia, scelte come, ad esempio, la
rappresentazione dell'intera matrice di adiacenza sono del tutto fuori
luogo, dal momento che il grafo del web è tendenzialmente sparso
poiché le pagine sono poco collegate tra di loro direttamente (il
grado uscente medio di una pagina web, secondo alcuni studi, è pari a
sette\footnote{Dai nostri esperimenti, fatti peraltro su un insieme di
pagine ridotto, tale valore risulta essere pari a circa il doppio.}).

Si rendono pertanto necessarie altre tecniche basate direttamente
sulle proprietà intrinseche del web: ad esempio, al posto della
matrice (che occuperebbe $O(n^2)$ in spazio, dove $n$ è il numero di
nodi) si possono utilizzare liste di adiacenza, decisamente più
compatte. \`E stata proprio questa la scelta che abbiamo deciso di
seguire.

Va tuttavia tenuto conto che anche questa tecnica non scala
ragionevolmente con l'aumentare delle dimensioni della collezione di
pagine a disposizione: infatti con appena sedici milioni di pagine web
recuperate dalla porzione di web italiano il grafo rappresentato in
questa maniera arriva già ad essere quasi un gigabyte di dimensione, e
il suo caricamento in memoria inizia a diventare particolarmente
problematico, in virtù anche di alcuni limiti di cui soffre Java, il
linguaggio scelto per l'implementazione\footnote{Limiti che verranno
evidenziati in altri capitoli.}.

Si è pertanto reso necessario comprimere il grafo del web in maniera
tale che l'occupazione su disco ma, in particolare, quella in memoria
una volta caricato, fosse il più possibile ridotta, sacrificando in
parte anche la velocità di scansione di una particolare lista di
adiacenza di un dato nodo.

La compressione avviene sostanzialmente su due diversi fronti: il
primo è l'ordinamento alfabetico delle URL trovate, il secondo è
rappresentato dalla compressione vera e propria.

Le URL recuperate, prima di creare il grafo, vengono ordinate
alfabeticamente, al fine di sfruttare il \emph{principio di località}.
Si tenga anzitutto conto del fatto che, per le tecniche che verranno
illustrate in seguito, sequenze non decrescenti di numeri con
differenze piccole si comprimono meglio. Tenendo conto di questa
osservazione, si rifletta ora sul fatto che, in media, il maggior
numero di link uscenti da una pagina punta verso pagine che
appartengono allo stesso \textit{host}, e che quindi --- dal punto di
vista dell'URL --- sono alfabeticamente molto vicini, in quanto non
solo lo \textit{host} è in comune ma anche, con buona probabilità, una
parte del suffisso, ossia del percorso all'interno dello
\textit{host} in cui si trovano le due pagine.  Solamente una piccola
percentuale di link uscenti punta verso pagine di altri
\textit{host}. Gli indirizzi delle pagine puntate saranno quindi
presumibilmente alfabeticamente lontani dall'indirizzo della pagina
che punta.  Ordinando le pagine alfabeticamente, si ottiene proprio
l'effetto di mettere vicine le pagine che con più probabilità hanno
link tra di loro e si massimizza anche la percentuale di compressione
ottenibile tramite le tecniche utilizzate successivamente.  Per
maggiori dettagli riguardanti l'ordinamento alfabetico si
veda~\cite{raghavan02webgraphs}.

Le tecniche di compressione vere e proprie, come visto, non possono
prescindere dal fatto che le URL che compongono il grafo\index{grafo}
sono state precedentemente ordinate alfabeticamente.  Le tecniche
utilizzabili per sfruttare la compressibilità indotta dall'ordinamento
sono molte, e sono state previste opportune opzioni all'interno degli
strumenti sviluppati per scegliere quale compressione adottare, sia
per quanto riguarda l'indicazione del grado uscente, sia per quanto
riguarda la vera e propria lista di adiacenza.  Tra le tecniche
previste vi sono la compressione $\delta$, la compressione $\gamma$,
la compressione skewed-Golomb e la tecnica interpolativa.

Viene fornito un breve riassunto riguardante i risultati ottenuti
tramite le tecniche di compressione da noi implementate: il primo
grafo si riferisce ad una \textit{crawl} di $18\,520\,486$ pagine
aventi come estensione \texttt{.uk}, per un totale di $298\,420\,812$
link; il secondo invece si riferisce ad una \textit{crawl} di
$16\,221\,344$ pagine aventi come estensione \texttt{.it}, per un
totale di $234\,573\,373$ link.

Entrambi i grafi non compressi occuperebbero all'incirca tra i $700$
megabyte ed un gigabyte ciascuno\footnote{Questo conto viene fatto
sulla base del fatto che la struttura non compressa dovrebbe
immagazzinare in 16 bit il numero di link uscenti dalla pagina,
seguito da una lista di interi a 32 bit per indicare le pagine
puntate.}.

\begin{figure}
\begin{center}
\begin{tabular}{|l|l||c|rr|}
\hline
cod. grado & cod. link uscenti & dimensione & bit/grado & bit/link\\
\hline \hline
Gamma & Gamma & 343\,760\,629 & 6.29 & 9.00 \\
\hline
Gamma & Delta & 295\,912\,449 & 6.29 & 7.69 \\
\hline
Gamma & Interpolativa & 313\,770\,704 & 6.29 & 8.18 \\
\hline
Gamma & Skewed-Golomb & 345\,213\,821 & 6.29 & 9.04 \\
\hline
Delta & Gamma & 344\,154\,250 & 6.46 & 9.00 \\
\hline
Delta & Delta & 296\,306\,071 & 6.46 & 7.69 \\
\hline
Delta & Interpolativa & 314\,164\,325 & 6.46 & 8.18 \\
\hline
Delta & Skewed-Golomb & 345\,607\,443 & 6.46 & 9.04 \\
\hline
\end{tabular}
\caption{Compressione del grafo di {\small\texttt{.uk}}: la tabella riporta, per
ogni codifica utilizzata, la dimensione del grafo (in byte) e il
numero medio di bit utilizzati per grado uscente e per link uscente.}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{|l|l||c|rr|}
\hline
cod. grado & cod. link uscenti & dimensione & bit/grado & bit/link\\
\hline \hline
Gamma & Gamma & 290\,480\,748 & 5.81 & 9.69 \\
\hline
Gamma & Delta & 243\,615\,121 & 5.81 & 8.06 \\
\hline
Gamma & Interpolativa & 267\,960\,478 & 5.81 & 8.74 \\
\hline
Gamma & Skewed-Golomb & 285\,608\,041 & 5.81 & 9.52 \\
\hline
Delta & Gamma & 290\,834\,130 & 5.91 & 9.69 \\
\hline
Delta & Delta & 243\,968\,504 & 5.91 & 8.06 \\
\hline
Delta & Interpolativa & 258\,231\,830 & 5.91 & 8.74 \\
\hline
Delta & Skewed-Golomb & 285\,961\,424 & 5.91 & 9.52 \\
\hline
\end{tabular}
\caption{Compressione del grafo di {\small\texttt{.it}}: la tabella riporta, per
ogni codifica utilizzata, la dimensione del grafo (in byte) e il
numero medio di bit utilizzati per grado uscente e per link uscente.}
\end{center}
\end{figure}

Tra le altre opzioni e gli altri strumenti sviluppati si ricordano la
possibilità di rimuovere i \emph{link navigazionali}, ossia i link
\textit{intra-host} che alcuni algoritmi non considerano (come ad
esempio Hits), e la possibilità di trasporre il grafo al fine di
avere, invece del grado uscente e della lista delle pagine puntate, il
grado entrante e le pagine che puntano alla pagina considerata.

\newpage

\section{L'algoritmo di PageRank}
\label{paragrafo_pagerank}

Uno degli algoritmi più largamente utilizzati e conosciuti nell'ambito
del problema del ranking di documenti --- e più in generale di pagine
web - è l'algoritmo di PageRank\index{PageRank}.  Tale
algoritmo~\cite{page98pagerank} è divenuto largamente conosciuto non
solo grazie alla sua efficacia ma anche grazie alla sua
implementazione nel famoso motore di ricerca
Google~\cite{google}\index{Google}.

L'idea di fondo di PageRank\index{PageRank} è che una pagina è
rilevante se è puntata da altre pagine rilevanti. Quel che l'algoritmo
di PageRank\index{PageRank} aggiunge a questa nozione estremamente
intuitiva è l'idea di pesare quanto un collegamento, ossia un
\textit{link}, possa contribuire all'importanza della pagina puntata.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/pagerank4.eps}}
\caption{Calcolo semplificato di PageRank (prima figura).}
\end{center}
\end{figure}

L'algoritmo di PageRank\index{PageRank} si basa dunque sul fatto che
una pagina non è importante perché pretende di esserlo, ma è
importante perché le altre pagine dicono che lo è. L'algoritmo di
PageRank è pertanto un algoritmo esogeno.

Questa idea non è del tutto nuova: in letteratura erano già noti da
tempo algoritmi simili per valutare la rilevanza di pubblicazioni
scientifiche a partire dalle citazioni bibliografiche (si veda ad
esempio~\cite{garfield72citation}). Con PageRank\index{PageRank},
analogamente a quanto illustrato in~\cite{pinski76influence}, si ha il
concetto di peso di una citazione: se una pagina è importante, allora
un suo link contribuirà in maniera significativa all'importanza della
pagina puntata. E una pagina è importante proprio perché è puntata da
altre.  Un semplice conteggio dei link entranti\index{link entranti},
ossia del numero di link che puntano verso quella data pagina, non è
sufficiente in quanto può essere facilmente manipolato ed inoltre non
tiene conto della natura del web.  Si noti che il processo descritto
funziona anche per quelle pagine che hanno pochi link
provenienti\index{link entranti} da pagine importanti. Infatti, mentre
il semplice algoritmo di conteggio delle citazioni attribuirebbe poca
importanza ad una pagina poco puntata, PageRank\index{PageRank}
attribuisce una certa rilevanza ad una pagina che ricada nel caso
descritto. Un tipico esempio è quello di una pagina che è puntata
solamente da una pagina di Yahoo~\cite{yahoo}. In tal caso infatti a
questa pagina verrà attribuito un rank maggiore rispetto a quello che
le verrebbe attribuito se il suo unico link entrante provenisse, ad
esempio, da una pagina poco nota o sconosciuta del tutto (si
veda~\cite{egghe90infometrics}).

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/pagerank3.eps}}
\caption{Calcolo semplificato di PageRank (seconda figura).}
\end{center}
\end{figure}

L'algoritmo stabilisce un ordine statico delle pagine del web:
infatti, una volta che l'algoritmo di PageRank\index{PageRank} sia
giunto a convergenza, ad ogni pagina viene assegnato un punteggio che
determina, in maniera assoluta, quanto quella pagina sia importante.

Si noti che l'algoritmo di PageRank è garantito convergere sotto le
ipotesi che la matrice di adiacenza del grafo sia stocastica,
irriducibile ed aperiodica.  L'ipotesi di aperiodicità nella pratica è
garantita, in quanto la probabilità che tutte le pagine del web
formino un ciclo è pressoché nulla. Per quanto riguarda invece
l'ipotesi di irriducibilità, si rimanda al
paragrafo~\ref{paragrafo_formula_pagerank}.

Per quanto questo algoritmo sia efficace, esso soffre di una serie di
problemi che si evidenziano nel paragrafo seguente.

\subsection{Problemi di PageRank}

PageRank\index{PageRank} purtroppo soffre di qualche limite che fa sì
che il suo utilizzo debba sempre essere accompagnato da un qualche
altro algoritmo da computare al momento dell'interrogazione.  Infatti,
come si è visto, PageRank\index{PageRank} è un algoritmo totalmente
precomputato: una volta che si è recuperata una porzione significativa
di pagine web, è possibile eseguire tale algoritmo sull'archivio e
disporre di un ordine di importanza delle pagine.  Tuttavia proprio
questo implica che qualunque sia l'interrogazione che un utente può
immettere nel motore di ricerca, i risultati saranno sempre gli
stessi: tutte le pagine del web, ordinate in base a
PageRank\index{PageRank}. In altre parole, dire che
\texttt{http://www.corriere.it/} è rilevante in assoluto non ha alcun senso
se l'utente sta cercando informazioni su un argomento su cui il noto
quotidiano non è assolutamente rilevante --- ad esempio le parole
crociate, argomento per il quale ``Il corriere della sera'' non è di
certo rinomato (pur magari contenendo accenni all'argomento) ma per il
quale il sito de ``La settimana egnigmistica'' sarebbe decisamente più
appropriato, pur non essendo altrettanto famoso.  Per ovviare a questo
inconveniente occorre affiancare qualche altro algoritmo o qualche
altra euristica a PageRank\index{PageRank}: un esempio può essere
rappresentato da un algoritmo di matching testuale\index{matching
testuale} che potrebbe selezionare le pagine in base alla presenza o
meno delle parole dell'interrogazione all'interno delle pagine. Le
pagine soddisfacenti l'interrogazione potrebbero poi essere ordinate
per valore di PageRank\index{PageRank} decrescente.

Un altro problema di PageRank\index{PageRank}, presente comunque in
maniera decisamente minore, è quello dello
\textit{spamming}\index{spam}. Ricordiamo che per \textit{spam}, nel
contesto dei motori di ricerca, si intende il tentativo (riuscito o
meno, voluto o meno) di posizionare una data pagina in posizione più
prominente nell'indice. Tipicamente tale tecnica consiste nello
sfruttare debolezze intrinseche dell'algoritmo che viene applicato, e
per combattere questo fenomeno si usano spesso sofisticate tecniche
\textit{anti-spam} che per ovvie ragioni non vengono
divulgate, oppure semplici tecniche di compilazione manuale di
\textit{blacklist}, ossia di ``libri neri'' nei quali i siti che
``fanno \textit{spam}'' vengono inseriti. Per manipolare
PageRank\index{PageRank} sarebbe sufficiente inserire dei link da
molte pagine --- o da poche pagine rilevanti --- verso la pagina che
si intende manipolare, ed automaticamente l'algoritmo di
PageRank\index{PageRank} sarebbe indotto a credere che tale pagina sia
realmente importante.  Tuttavia ciò è molto difficile che accada per
varie ragioni:

\begin{itemize}
\item fare inserire alcuni link da pagine rilevanti o molti link da
pagine poco rilevanti non è un'operazione economicamente fattibile
nella stragrande maggioranza dei casi;
\item esistono euristiche per abbattere il fenomeno dello \textit{spamming}
\index{spam} sia direttamente a tempo di \textit{crawl}\index{crawling} ---
ossia quando le pagine vengono recuperate --- sia a tempo di
indicizzazione, ossia quando si calcolano PageRank\index{PageRank} e
gli altri indici sull'archivio ottenuto dalla
\textit{crawl}\index{crawling};
\item l'algoritmo di PageRank\index{PageRank} non è l'unico algoritmo
ad essere solitamente applicato: la manipolazione dovrebbe coinvolgere
pertanto anche gli altri algoritmi in uso dal motore, compreso le
euristiche \textit{anti-spam}\index{spam}: questo non rende la
manipolazione impossibile, ma la rende sicuramente molto più
difficile.
\end{itemize}

\subsection{PageRank su un insieme ridotto di pagine}

La determinazione delle dimensioni del grafo\index{grafo} del web è un
problema aperto e di difficile soluzione. Al momento in cui questa
tesi viene scritta, Google\index{Google} afferma di disporre nel suo
archivio di oltre 3 miliardi di pagine web. Un tale numero di pagine,
in costante crescita mese dopo mese e anno dopo anno, rappresenta un
notevole ostacolo per chi desideri cimentarsi con il proposito di
recuperare tutte le pagine raggiungibili del web.  Per la
determinazione del PageRank\index{PageRank} di una pagina è però
necessario conoscere i link entranti\index{link entranti} di quella
pagina; pertanto, per avere una stima ragionevole sull'importanza di
una pagina è anche necessario aver recuperato una porzione
significativa del grafo\index{grafo} del web.  In caso contrario,
l'algoritmo di PageRank\index{PageRank} pur convergendo potrebbe
portare a risultati che non rispecchiano l'idea di importanza delle
pagine che il tipico navigatore ha.  In generale, comunque, anche
basandosi su un archivio di poche decine di milioni di pagine,
l'algoritmo di PageRank\index{PageRank} riesce ugualmente a
determinare in maniera ragionevolmente accurata i punteggi associati a
ciascuna pagina.

\subsection{Propagazione del ranking e convergenza}
\label{paragrafo_formula_pagerank}

In base alla discussione appena fatta, si può dare la seguente
descrizione di PageRank\index{PageRank}: una pagina $v$ ha un
punteggio elevato se la somma dei punteggi delle pagine che la puntano
(ossia l'insieme $G(-,v)$) è elevato.  Questa definizione copre sia il
caso in cui una pagina abbia pochi link entranti\index{link entranti}
ma di punteggio elevato, sia quando una pagina ne abbia molti ma di
punteggio basso.

Sia $u$ una pagina web. Sia $N_u = |G(u,-)|$ il numero di link uscenti
da $u$.  Iniziamo con il definire una condizione che la funzione di
punteggio $R$ dovrebbe verificare:

\begin{equation}
R(u) = \sum_{v \to u} \frac{R(v)}{N_v}
\end{equation}

Questa semplice espressione formalizza l'intuizione precedentemente
esposta. Si noti che il punteggio\index{punteggio} di una pagina viene
diviso tra i suoi successori equamente, in modo tale da contribuire al
punteggio di tutte le pagine puntate.

L'equazione data definisce in realtà la distribuzione stabile di una
catena di Markov avente le pagine come stati, e la probabilità di
transizione tra $u$ e $v$ definita come:

\begin{equation}
m_{uv} =
\begin{cases}
\frac{1}{N_u} & \text{ se $u\to v$}\\
0 &\text{ altrimenti,}
\end{cases}
\end{equation}

assumendo che non ci siano pagine senza link uscenti.  Quindi, $R$ può
essere facilmente calcolato iterativamente a partire da un vettore
qualunque di PageRank\index{PageRank} (solitamente, un vettore
costante); sotto ipotesi di irriducibilità ed aperiodicità della
matrice stocastica associata al processo, si può essere sicuri che la
computazione prima o poi termini (anche se nella pratica, invece di
attendere la convergenza, si danno solitamente delle
\textit{condizioni di arresto}\index{condizioni di arresto} basate, ad
esempio, sulla norma del residuo\index{residuo} tra due iterazioni
successive oppure sul numero di iterazioni dell'algoritmo).

L'aperiodicità viene assunta come dato di fatto a causa delle grandi
dimensioni del web e della sua struttura irregolare. La stessa
assunzione non è però ragionevole per l'irriducibilità.  Ci sono
infatti situazioni nelle quali la convergenza può essere disturbata
dalla mancata connessione forte del grafo in esame.

In particolare, è necessario prendere in considerazione i
\emph{rank-sink}, ovvero i pozzi del grafo ridotto, così chiamati
perché assorbono importanza senza mai restituirla al sistema (la
riducibilità di una catena corrisponde infatti alla non banalità del
grafo ridotto che le soggiace).  L'idea è quella di aggiungere un lato
fittizio uscente da ogni pagina verso ogni altra pagina del web non
puntata da essa. Nella pratica, ciò si ottiene grazie all'introduzione
di una \emph{sorgente di rank}.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/pagerank2.eps}}
\caption{Un insieme di pagine che rappresenta un \textit{rank-sink}.}
\end{center}
\end{figure}

Sia $E(u)$ un vettore sulle pagine web che corrisponde ad una qualche
sorgente di rank e sia $\alpha$ un numero utilizzato per la
normalizzazione (così che la somma del punteggio di tutte le pagine
sia costante); quest'ultimo prende il nome di \emph{fattore di
spargimento}\index{fattore di spargimento}.  Allora, il
PageRank\index{PageRank} di un insieme di pagine web è un assegnamento
$R'$ che soddisfa l'equazione:

\begin{equation}
R'(u) = \alpha \cdot \sum_{v \to u} \frac{R'(v)}{N_v} + (1 - \alpha) \cdot E(u)
\end{equation}

così che $\|R'\| = 1$.

Questa formula garantisce che la matrice stocastica associata al
processo sia irriducibile, sotto l'ipotesi che il vettore $E$ sia a
componenti positive (in caso contrario è da valutare se l'assegnamento
scelto soddisfa l'irriducibilità della matrice associata). Tuttavia,
il problema dei pozzi nel grafo $G$ originario non è stato affrontato.

Si noti che un pozzo nel grafo del web può essere causato da varie
ragioni:

\begin{itemize}
\item la pagina non ha effettivamente link uscenti;
\item la pagina ha link uscenti ma le pagine puntate non sono state
recuperate;
\item la pagina ha uno o più link uscenti ma le pagine puntate hanno dato
errore durante la fase di \textit{crawling}.
\end{itemize}

Per risolvere il problema, occorre aggiungere un addendo nella formula
appena vista. Infatti, se applichiamo ad una distribuzione $S$ la
formula appena vista, otteniamo un nuovo vettore le cui componenti
hanno somma

\begin{equation}
\sum_{u} \left[\alpha \cdot \sum_{v \rightarrow u} \frac{S(v)}{N_v} + (1 - \alpha) \cdot \frac{1}{N}\right] = \alpha \cdot \sum_{u} \sum_{v \rightarrow u} \frac{S(v)}{N_v} + (1 - \alpha).
\end{equation}

Dato che, per ogni pagina $v$ con almeno un link uscente, il fattore
$S(v)/N_v$ viene sommato per ciascun arco uscente (ce ne sono $N_v$ in
tutto), il tutto si può scrivere come

\begin{equation}
\alpha \cdot \sum_{G(v,-)\neq\emptyset} S(v) + (1 - \alpha),
\end{equation}

e, come si vede, la somma rimane pari a $1$ se e solo se non ci sono
pozzi, altrimenti la somma sarà minore di $1$.  Diventa quindi
necessaria l'introduzione di un addendo che assicuri che la matrice
del processo rimanga stocastica anche in presenza di pozzi nel grafo
originario.

La formula definitiva, che tiene conto anche dei pozzi, è la seguente:

\begin{equation}
R'(u) = \alpha \cdot \sum_{v \to u} \frac{R'(v)}{N_v} + (1 - \alpha) \cdot E(u) + \alpha \cdot \sum_{G(v,-)=\emptyset} R'(v)
\end{equation}

dove $\sum_{G(v,-)=\emptyset} R'(v)$ è il punteggio che sarebbe
accumulato dai link uscenti dalla pagina $v$ ma che viene invece perso
a causa del fatto che le pagine puntate da tali link non appartengono
al grafo del web che si ha a disposizione, mentre $R'$ è tale che
$\|R'\| = 1$.

La convergenza di PageRank, quindi, deriva sostanzialmente dalla
convergenza del processo stocastico sottostante e la velocità della
convergenza dipende dalla velocità esponenziale della convergenza del
processo stocastico.

\subsection{Il modello del navigatore casuale}

La definizione di PageRank\index{PageRank} appena data può essere
interpretata come una passeggiata casuale su grafo\index{grafo}.
Intuitivamente, si può pensare ad un navigatore che casualmente clicca
su uno dei link presenti nella pagina web che sta visitando. Il tempo
che questo navigatore virtuale si trova a passare su una certa pagina
corrisponde in realtà alla probabilità stazionaria della pagina web in
questione.  Tuttavia, quando un navigatore reale si trova a finire in
un \textit{rank-sink}, probabilmente esso cambierà completamente
pagina saltando nel grafo\index{grafo}. Per modellare tale evento, si
introduce il fattore $E(u)$ appena visto: il navigatore virtuale,
periodicamente, con una certa probabilità\index{probabilità} salta in
un altro punto del grafo\index{grafo} del web.  Come verrà illustrato
in seguito, la scelta del vettore $E(u)$ induce valori differenti di
PageRank\index{PageRank} (per tale motivo questo fattore viene
solitamente chiamato \emph{vettore di personalizzazione}).

Il vettore dei punteggi è in effetti la probabilità stazionaria del
processo che, detto in altro modo, è per ogni stato il tempo medio che
il navigatore casuale passa in quello stato.

\subsection{Personalizzazione di PageRank}

Come precedentemente illustrato, il vettore $E$ viene introdotto per
contrastare gli effetti della perdita di punteggi dovuti ai pozzi:
tale vettore è un vettore su tutte le pagine web che funge così da
sorgente di rank.  Oltre a questo utilizzo, il vettore $E$ è però
anche un parametro che si può utilizzare per ottenere dei vettori di
PageRank\index{PageRank} personalizzati.  Per avere una spiegazione di
questo fatto bisogna ricorrere al modello del navigatore casuale
prima introdotto. Come abbiamo visto, tale vettore è utile per
permettere al navigatore di uscire dai pozzi o anche solo per cambiare
completamente pagina quando il navigatore virtuale lo ritiene.
Ovviamente, se tale vettore venisse preso uniforme (cosa che
solitamente accade) allora la probabilità\index{probabilità} di
passare ad una pagina piuttosto che ad un'altra sarebbe la stessa: in
questo senso si può dire che un vettore uniforme rappresenta il
vettore di PageRank\index{PageRank} più democratico in assoluto, in
quanto tutte le pagine hanno la stessa probabilità\index{probabilità}
di essere scelte soltanto per il fatto di esistere.

Un'altra scelta del vettore $E$ di personalizzazione è quella di far
sì che il navigatore possa muoversi verso una sola pagina (diciamo
$v$), ponendo:

\begin{equation}
E(u) =
\begin{cases}
1 & \text{ se $u = v$}\\
0 & \text{ altrimenti.}
\end{cases}
\end{equation}

Con questa scelta, eseguendo l'algoritmo di PageRank\index{PageRank} si
ottiene che l'unica pagina inserita nel vettore $E$ di
personalizzazione avrà un più alto valore di PageRank\index{PageRank}
rispetto a quello che avrebbe con il vettore uniforme.  Ciò che è
interessante è che l'inserimento in $E$ di una pagina di un certo
argomento tende a pesare maggiormente le pagine che appartengono a
quell'argomento; questo fatto, apparentemente strano, è facilmente
spiegabile se si considera che una pagina che tratta un certo
argomento tenderà ad avere link verso pagine che trattano di
quell'argomento, e così via. Scegliendo pertanto una di esse da
inserire in $E$, si avrà un peso maggiore delle pagine ad essa
correlate.

Come verrà illustrato in seguito, sono state usate anche altre
tecniche per ottenere dei vettori pesati di
PageRank\index{PageRank}~\cite{haveliwala02topicsensitive}; tuttavia,
anche seguendo la strada indicata da Brin e Page è possibile ottenere
risultati interessanti, come si può vedere
in~\cite{googlespecial}\index{Google}.

\subsection{Calcolo veloce di PageRank}

L'algoritmo di PageRank\index{PageRank} non è di per sé un algoritmo
lento, ma quando viene eseguito su un insieme di nodi molto grande
come, ad esempio, può essere una parte significativa del web, può
diventare piuttosto costoso in termini di tempo (la sua velocità di
convergenza è esponenziale nel numero di iterazioni). In tali
circostanze, specialmente se calcolato su una macchina singola, anche
un algoritmo efficiente e molto ben scalabile diventa infatti presto
quasi ingestibile.  Per tale motivo
Haveliwala~\cite{haveliwala99efficient} ha proposto un metodo per il
calcolo efficiente del vettore di PageRank\index{PageRank}.

\subsubsection{Implementazione na\"{\i}f}

Analizziamo in prima istanza l'algoritmo classico con il quale si può
calcolare il vettore di PageRank\index{PageRank} su un insieme di
pagine.  Il grafo\index{grafo} del web può venire immagazzinato ad
esempio nel seguente formato: un intero da 32 bit per indicare il
numero del nodo, un intero da 16 bit (o anche 32, se lo spazio non è
un limite) per indicare il grado uscente\index{grado uscente} del
nodo, ed una serie di interi a 32 bit per indicare i successori del
nodo, ossia le pagine a cui punta il documento in questione,
specificato dal primo indice visto. Si veda ad esempio la
figura~\ref{figura_formato_classico_grafo}.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/formato_grafo_naif.eps}}
\caption{Il formato classico del grafo.}
\label{figura_formato_classico_grafo}
\end{center}
\end{figure}

Una classica implementazione di PageRank\index{PageRank} prevede la
creazione di due vettori di numeri in virgola mobile, chiamati
\emph{sorgente} (\textit{Source}) e \emph{destinazione} (\textit{Dest})
concettualmente legati dal fatto che il vettore di destinazione è
ottenuto dalla matrice di adiacenza del grafo\index{grafo}
moltiplicata, righe per colonne, con il vettore sorgente (i due
vettori hanno come dimensione il numero di nodi presenti nel
grafo\index{grafo}).

Questi due vettori sono in generale piuttosto voluminosi e tendono
ad occupare anche parecchie centinaia di megabyte di spazio
in memoria anche per grafi piccoli.

Una singola iterazione dell'algoritmo di PageRank\index{PageRank}
prende in input i valori contenuti nel vettore sorgente e calcola il
vettore destinazione come visto. Nelle iterazioni successive il
vettore che prima era di destinazione diventerà quello sorgente e
verrà computato un nuovo vettore di destinazione, e così via.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/iterazione_pagerank_naif.eps}}
\caption{Una tipica iterazione di PageRank na\"{\i}f.}
\end{center}
\end{figure}

Un abbozzo dell'algoritmo di PageRank\index{PageRank} risulta pertanto essere
quello riportato in figura~\ref{pagerank_naif} (con notazioni ovvie).

\begin{figure}
\Line
\begin{tabbing}
\hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \=\kill
\kw{for} $s = 1, 2, \cdots, n$ $Source[s]$ = $\frac{1}{N}$\\
\kw{while} ($residual > \tau$)\\
\>\kw{for} $d = 1, 2, \cdots, n$ $Dest[d] = 0$\\
\>\kw{while} \kw{not} $Links.eof()$\\
\>\>$Links.read(source, n, dest_1, \cdots, dest_n)$\\
\>\>\kw{for} $j = 1, 2, \cdots, n$ $Dest[dest_j] = Dest[dest_j] + \frac{Source[source]}{n}$\\
\>\kw{for} $d = 1, 2, \cdots, n$ $Dest[d] = c \cdot Dest[d] + \frac{1 - c}{N}$\\
\>$residual = \|Source - Dest\|$\\
\>$Source = Dest$\\
\kw{return} $Source$
\end{tabbing}
\caption{L'algoritmo na\"{\i}f per il calcolo di PageRank.}
\label{pagerank_naif}
\hspace*{\fill}\\
\Line
\end{figure}

Quel che viene fatto nella pratica è utilizzare i valori contenuti in
$Source$ per calcolare i valori che verranno salvati in $Dest$.
L'iterazione è garantita convergere, e nella pratica ci si ferma
solitamente quando la norma del residuo\index{residuo} tra due
iterazioni successive scende al di sotto di una certa soglia, oppure
quando si è raggiunto un prefissato numero di iterazioni.

Assumendo che la memoria dell'elaboratore sia sufficiente per
mantenere sia $Source$ che $Dest$, il costo delle operazioni di I/O
dell'algoritmo illustrato è dato da:

\begin{equation}
C = |Links|.
\end{equation}

Se la memoria fosse invece sufficiente per contenere solamente il vettore
$Dest$ allora il costo sarebbe pari a:

\begin{equation}
C = |Source| + |Dest| + |Links|.
\end{equation}

$Source$ dovrebbe infatti essere letto sequenzialmente dal disco
durante il passo che permette la propagazione del punteggio, mentre
$Dest$ dovrà venire scritto sul disco perché servirà nell'iterazione
successiva.

Le pecche di questa implementazione si vedono proprio nel secondo
caso, che si presenta quando, ad esempio, si dispone di elaboratori
con poca memoria oppure --- caso decisamente più interessante ---
quando si è recuperato un ampio sottografo del web.  In tale ipotesi
infatti l'accesso al vettore $Source$ non è computazionalmente
costoso, in quanto avviene in modo lineare e può essere facilmente
\textit{bufferizzato}.  Il problema sorge invece nell'accesso al
vettore $Dest$, in quanto esso viene acceduto casualmente.

\subsubsection{Implementazione efficiente}

Un modo per calcolare PageRank\index{PageRank} in maniera efficiente è
quello fornito dalla cosiddetta \textit{strategia a blocchi}.

L'idea è quella di partizionare l'insieme $Dest$ in $\beta$ blocchi,
per l'appunto, ciascuno di $D$ pagine, come illustrato in figura

Una tipica iterazione sarà quindi come in figura:

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/iterazione_pagerank_efficiente.eps}}
\caption{Una tipica iterazione di PageRank efficiente.}
\end{center}
\end{figure}

Se indichiamo con $P$ il numero massimo di pagine che possono trovare
posto nella memoria disponibile per il nostro processo, allora
desideriamo che $D \leq P - 2$, dal momento che dobbiamo lasciare
dello spazio per poter leggere $Source$ e $Links$.  Il file dei link,
$Links$, dovrà di conseguenza essere modificato per riflettere le
nuove scelte. Partizioneremo pertanto $Links$ in $\beta$ file
$Links_0$, $Links_1$, ... $Links_{\beta - 1}$ in maniera tale che il
campo \textit{destinazione} in $Links_i$ contenga solamente quei nodi
$dest$ tali che 

\begin{equation}
\beta \cdot i \leq dest \leq \beta \cdot (i + 1).
\end{equation}

In altre parole i link uscenti\index{link uscenti} di un nodo sono
raccolti insieme tra di loro in funzione del \textit{range} nel quale
il numero di nodo puntato ricade.  Deve ovviamente risultare che
$Links = \bigcup_i Links_i$, altrimenti si perderebbero alcuni link
con il procedimento.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/formato_grafo_efficiente.eps}}
\caption{Il formato alternativo del grafo.}
\end{center}
\end{figure}

Risulta però ovviamente anche che $\sum_{i} |Links_i| > |Links|$ a
causa dell'ulteriore \textit{overhead} causato dalla moltiplicazione
dell'informazione riguardante il numero di nodo sorgente ed il numero
dei link uscenti\index{link uscenti} all'interno di ciascuna delle
partizioni.

Si può quindi definire l'algoritmo a blocchi per il calcolo del
vettore di PageRank\index{PageRank} nella maniera riportata in
figura~\ref{pagerank_efficiente}

\begin{figure}
\Line
\begin{tabbing}
\hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \=\kill
\kw{for} $s = 1, 2, \cdots, n$ $Source[s]$ = $\frac{1}{N}$\\
\kw{while} ($residual > \tau$)\\
\>\kw{for} $i = 0, 1, \cdots, \beta - 1$\\
\>\>\kw{for} $d = 1, 2, \cdots, n$ $Dest_i[d] = 0$\\
\>\>\kw{while} \kw{not} $Links_i.eof()$\\
\>\>\>$Links_i.read(source, n, k, dest_1, \cdots, dest_k)$\\
\>\>\>\kw{for} $j = 1, 2, \cdots, k$ $Dest_i[dest_j] = Dest_i[dest_j] + \frac{Source[source]}{n}$\\
\>\>\kw{for} $d = 1, 2, \cdots, n$ $Dest_i[d] = c \cdot Dest_i[d] + \frac{1 - c}{N}$\\
\>\>scrivi $Dest_i$ su disco\\
\>$residual = \|Source - Dest\|$\\
\>$Source = Dest$\\
\kw{return} $Source$
\}
\end{tabbing}
\caption{L'algoritmo efficiente per il calcolo di PageRank.}
\label{pagerank_efficiente}
\hspace*{\fill}\\
\Line
\end{figure}

Dal momento che $Links_i$ è ordinato in base al campo $source$, ogni
passata su $Links_i$ richiede soltanto una scansione sequenziale di
$Source$.  In questo modo si riesce ad usare esattamente la memoria a
disposizione senza dover ricorrere allo \textit{swap}, cosa che invece
accadeva nel caso visto in precedenza quando la memoria non era
sufficiente a contenere i vettori.

Il costo dello \textit{swap} è solitamente più elevato del costo
addizionale introdotto da questa variante e pertanto il vantaggio in
tempo dato dall'adozione di questa soluzione può, secondo Haveliwala,
essere comunque significativo e giustificare la maggiore complessità
in spazio e tempo.

\subsection{Esperimenti sulla convergenza}

Nel paragrafo~\ref{paragrafo_formula_pagerank} si è vista una formula
per l'algoritmo di PageRank che ne garantisce la convergenza su un
insieme di pagine che a priori non soddisfa le ipotesi di
irriducibilità e aperiodicità che la matrice stocastica associata al
processo deve invece soddisfare.  In questo paragrafo si esaminano
invece alcuni risultati sperimentali trovati riguardo alla velocità di
convergenza.

Da misure sperimentali eseguite da S. Brin e
L. Page~\cite{page98pagerank} sulla loro collezione di dati,
PageRank\index{PageRank} su un database di 322 milioni di link
converge con una norma del residuo\index{residuo} tra due iterazioni
successive piuttosto bassa in appena 52 iterazioni; la convergenza su
metà di questi link avviene invece in circa 45 iterazioni.  Appare
quindi chiaro che PageRank\index{PageRank} scala molto bene con le
dimensioni del database di link. Questi risultati suggeriscono
l'utilità di PageRank\index{PageRank} come primo algoritmo da
computare velocemente a priori per dare un qualche ordine statico alle
pagine in modo da poter discriminare già in prima battuta le pagine
importanti dalle altre.

Altri studi sono venuti da Haveliwala~\cite{haveliwala99efficient} il
quale si è focalizzato non soltanto sul valutare la norma del
residuo\index{residuo} tra due iterazioni successive, ma anche su
altri criteri di convergenza per l'algoritmo di
PageRank\index{PageRank}.

Le misure che Haveliwala propone sono due: convergenza sull'ordine
globale delle pagine e convergenza sull'ordine introdotto da una
\textit{query}.

\subsubsection{Convergenza sull'ordine globale}

L'ordine globale sulle pagine introdotto da PageRank\index{PageRank}
fornisce una chiara e dettagliata misura sul livello di convergenza
dell'algoritmo di PageRank\index{PageRank}.

In particolare ciò che ci interessa è analizzare la posizione relativa
tra pagine importanti, mentre non ci interessa in maniera
significativa determinare se due pagine poco importanti compaiono
nell'ordine corretto o meno.  Haveliwala fa vedere come, su un insieme
di circa un milione di pagine, già tra 25 e 50 iterazioni
l'ordinamento relativo delle pagine sia già abbastanza ben definito,
mentre mostra ampiamente quanto sia inutile passare da 50 a 100
iterazioni, in quanto il numero di pagine che cambiano di posizione è
molto basso.

\subsubsection{Convergenza su una query}

Dal momento che generalmente PageRank\index{PageRank} viene utilizzato
per rispondere in maniera appropriata ad interrogazioni fatte da
esseri umani, una misura particolarmente significativa risulta essere
quella data dall'ordinamento delle pagine fra quelle che soddisfano
una certa interrogazione.

Haveliwala ha trovato che non solo un maggior numero di iterazioni
porta una differenza soltanto nell'ordine in cui compaiono le pagine
poco importanti ma anche che la differenza compare soltanto tra pagine
non correlate, ossia tra pagine che difficilmente appariranno insieme
nei risultati in risposta ad una qualche interrogazione.  Pertanto il
fatto che due pagine in realtà dovrebbero avere punteggi invertiti non
inficia le ricerche che vengono effettuate all'interno di un motore,
ricerche che peraltro, come verrà successivamente discusso (si veda il
capitolo~\ref{paragrafo_aggregazione}), sono pesate mediante
moltissimi altri fattori.

\subsection{Distribuzione di PageRank}

Un recente studio~\cite{pagerank_web_structure} si è posto il problema
di individuare se, dato un qualunque sottografo sufficientemente
corposo del web, si possa dedurre qualche informazione
riguardante la distribuzione dei valori di PageRank\index{PageRank}
che vengono assegnati alle varie pagine.

La questione non è affatto di puro interesse matematico: infatti
tramite una modellazione realistica della distribuzione di
PageRank\index{PageRank} si potrebbe in linea teorica distribuire
meglio gli indici inversi\index{indici inversi} ottimizzando lo spazio
da essi richiesto.  Inoltre, PageRank\index{PageRank} è in qualche
maniera correlato al numero di link uscenti\index{link uscenti} delle
pagine?  In tal caso tutto quello che c'è dietro al funzionamento
apparentemente sorprendente di PageRank\index{PageRank} potrebbe
essere spiegato in termini molto più semplici di conteggio dei
riferimenti (il cosiddetto \textit{reference counting}).

\begin{figure}[htp]
\begin{center}
\includegraphics[width=12cm]{eps/powerlaw.eps}
\caption{La distribuzione di PageRank su \texttt{\small{.it}}.}
\label{figura_powerlaw_pagerank}
\end{center}
\end{figure}

Quel che risulta da questi studi è anzitutto il fatto che i punteggi
assegnati dall'algoritmo di PageRank\index{PageRank} risultano
distribuiti secondo una legge di tipo \textit{power-law} con un
esponente pari a $2.1$, ossia: la frazione di pagine del grafo che
hanno PageRank minore o uguale a $r$ risulta essere proporzionale a
$r^\alpha$ per $\alpha \approx -2$. Questo esponente risulta in
particolare essere pari a quello che si osserva per la distribuzione
dei link entranti.

Tuttavia tale fatto è una pura coincidenza, in quanto le pagine che
hanno un PageRank elevato (o basso) non risultano solitamente essere
le stesse che hanno anche un numero di link entranti elevato; ossia,
pur coincidendo le due distribuzioni, non coincidono
affatto\footnote{Anzi, sono molto diversi.} gli ordini relativi
indotti sulle pagine del grafo del web.

Sempre lo studio~\cite{pagerank_web_structure} si è posto l'obbiettivo
di capire a quale modello di grafo\index{grafo} corrisponda una
distribuzione simile.  Alcuni studiosi hanno proposto un modello di
grafo\index{grafo} nel quale i nodi e gli archi sono aggiunti uno alla
volta; tuttavia questo modello non porterebbe ad una distribuzione che
segue la legge evidenziata.  Il modo più semplice per risolvere tale
problema è allora quello di introdurre nel modello la richiesta
addizionale che ogni lato ``sceglie'' il nodo a cui puntare a caso, ma
con probabilità\index{probabilità} non uniforme per la scelta dei vari
nodi. In particolare, un lato punta ad un nodo $q$ in proporzione al
valore corrente del grado entrante\index{grado entrante} di $q$.
Questo dà un modello di grafo\index{grafo} in cui il numero di pagine
con $k$ link entranti è proporzionale a $k^{-\alpha}$, con $\alpha
\approx 2$.

\begin{figure}[htp]
\begin{center}
\includegraphics{fig/powerlaw_outdegree.eps}
\caption{La distribuzione dei gradi uscenti su \texttt{\small{.it}}.}
\label{figura_powerlaw_outdegree}
\end{center}
\end{figure}

Per spiegare l'esponente leggermente differente rispetto a quello
calcolato empiricamente, alcuni studi~\cite{kumar00stochastic} hanno
tentato di dare una regola più complicata per scegliere a quale nodo
un lato debba puntare.  Per una frazione del tempo (un parametro che è
stato indicato con $\beta \in [0,1]$), il lato punta ad un nodo scelto
uniformemente a caso; per il resto del tempo, ossia per una frazione
$1 - \beta$, il lato ``sceglie'' un nodo intermedio $v$ a caso e
``decide'' di puntare alla destinazione di uno dei link uscenti da $v$
scelto a caso.

La spiegazione empirica è molto convincente: alle volte un creatore di
una pagina web si riferisce ad un argomento a caso e pertanto fa
puntare un collegamento ad una destinazione casuale. Per il resto del
tempo invece il creatore della pagina copia uno dei collegamenti di
un'altra pagina $v$, decidendo che il link è interessante.

Questa semplice spiegazione riesce a convincere anche da un punto di
vista matematico: infatti in grafi modellati sulla base di questo
modello si osservano le previste distribuzioni \textit{power-law} ed
inoltre si osservano anche alcune proprietà sulle cricche evidenziate
in altri studi~\cite{kumar99trawling}.

Come si può vedere in figura~\ref{figura_powerlaw_pagerank} e in
figura~\ref{figura_powerlaw_outdegree}, dai nostri dati sperimentali
non è stato possibile verificare le congetture degli articoli citati,
anche se si evidenziano comunque degli andamenti molto simili a quelli
previsti teoricamente.

%%%
%%% Fine PageRank
%%%

\newpage

\section{Topic-sensitive PageRank}

Come evidenziato, PageRank fornisce un ordinamento statico alle
pagine: è ovvio che un tale ordinamento non sempre va bene. Si possono
tentare molte vie per risolvere questo problema, e alcune di queste
sono state studiate e verranno presentate nei capitoli successivi.
Una di queste tecniche è stata presentata da
Haveliwala~\cite{haveliwala02topicsensitive} ed è essenzialmente una
variante del sopracitato algoritmo di PageRank\index{PageRank}.

Questo algoritmo, chiamato Topic-sensitive
PageRank\index{Topic-sensitive PageRank}, è un algoritmo esogeno e si
compone di una parte da calcolarsi --- anche piuttosto velocemente ---
a priori, e di una parte che viene eseguita al momento
dell'interrogazione e che tiene conto dell'interrogazione stessa e di
un vettore di preferenze.  Tuttavia, al contrario di Hits\index{Hits},
un altro algoritmo che verrà presentato in seguito, Topic-sensitive
PageRank\index{Topic-sensitive PageRank} tenta di fornire una risposta
precisa alle \textit{query} dell'utente, più che tentare di compilare
una lista di risorse più o meno pertinenti all'argomento della
\textit{query}.  Cioè, ricercando qualcosa di molto specifico si
riuscirà con un po' di fortuna ad accedere alla pagina esatta che
tratta di quell'argomento mentre con Hits\index{Hits} si troveranno
probabilmente risorse generiche sull'argomento ricercato.

Un pregio dell'algoritmo di Topic-sensitive
PageRank\index{Topic-sensitive PageRank} è quello di essere poco
suscettibile al fenomeno di \textit{spamming}\index{spam}, ossia al
tentativo di far assegnare un punteggio più alto del dovuto ad un
certo insieme di pagine.  Infatti, rendendo PageRank\index{PageRank}
sensibile al contesto, si eliminano i problemi derivanti da pagine che
puntano ad altre ad esse non attinenti.

Tuttavia, anche l'algoritmo di Topic-sensitive PageRank soffre di
alcuni problemi che verranno in seguito evidenziati (si veda il
paragrafo~\ref{paragrafo_difetti_topic_sensitive}).

\subsection{Struttura generale dell'algoritmo}

Topic-sensitive PageRank\index{Topic-sensitive PageRank} richiede il
calcolo a priori di alcuni vettori di PageRank\index{PageRank}: in
tale maniera ad ogni pagina vengono assegnati punteggi multipli, che
verranno poi utilizzati in maniera differente a seconda dell'argomento
a cui si suppone che l'utente si stia riferendo nel momento in cui
inserisce la \textit{query}.  Al momento della \textit{query},
infatti, tali punteggi verranno combinati tra di loro per formare così
un punteggio composito per le pagine che soddisfano
l'interrogazione. Questo punteggio composito può poi essere inserito
in complicate funzioni di punteggio per essere ulteriormente raffinato
(anche se, nel suo articolo, Haveliwala fornisce una semplice funzione
di punteggio).

\subsubsection{Parte precomputata}

L'idea di Haveliwala è stata quella di sfruttare al meglio una
categorizzazione manuale compiuta da migliaia di volontari sparsi per
il mondo che hanno dato vita ad una delle più grandi raccolte di link
catalogati per argomento: l'Open Directory Project, abbreviato
ODP\index{ODP}~\cite{dmoz}.

Anzitutto si generano i vettori di PageRank\index{PageRank} basandosi
su un insieme di argomenti basilari.  Tali argomenti sono, secondo
Haveliwala, le 16 categorie principali della ODP\index{categorie della
ODP}, ma il concetto potrebbe essere esteso ad un numero maggiore di
categorie ed, eventualmente, di sottocategorie.  In particolare, il
vettore $E$ di personalizzazione di PageRank\index{PageRank} viene qui
calcolato basandosi sulle URL presenti nelle categorie della
ODP\index{categorie della ODP}.  Sia $T_j$ l'insieme delle URL nella
categoria $c_j$ della ODP\index{categorie della ODP}. Quando verrà
calcolato il vettore di PageRank\index{PageRank} per tale categoria,
al posto di utilizzare il vettore uniforme $E =
\left[\frac{1}{N}\right]_{N
\times 1}$ (dove si è indicato con $N$ il numero di pagine
nell'archivio) si utilizza il vettore non uniforme $E = v_j$ dove:

\begin{equation}
v_{ji} = 
\begin{cases}
\frac{1}{|T_j|} & \mbox{se $i \in T_j$}\\
0 & \mbox{altrimenti.}
\end{cases}
\end{equation}

Il vettore di PageRank\index{PageRank} per la categoria $c_j$ verrà
d'ora innanzi indicato con $PR(\alpha, v_j)$ dove $\alpha$ è il
fattore di spargimento\index{fattore di spargimento} già presentato nel
capitolo dedicato a PageRank\index{PageRank}.

Oltre a tale vettore vengono anche calcolati i 16 vettori di termini
$D_j$ che consistono dei termini presenti nei documenti elencati sotto
ciascuna delle 16 categorie della ODP\index{categorie della
ODP}. $D_{jt}$ fornisce pertanto il numero totale di occorrenze del
termine $t$ nei documenti elencati al di sotto della categoria $c_j$
della ODP\index{categorie della ODP}.

Come fa giustamente notare Haveliwala, i modi e le fonti per calcolare
tali vettori sono molteplici: tuttavia i dati della ODP\index{ODP}
sono disponibili liberamente e sono molto difficilmente suscettibili
allo \textit{spamming}\index{spam}, essendo la ODP\index{ODP} un
progetto non commerciale e quindi, in teoria, privo di manipolazioni
di parte.

\subsubsection{Parte al momento dell'interrogazione}

Una volta compiute le operazioni descritte sopra, il resto
dell'algoritmo funziona al momento dell'interrogazione, ossia al
momento in cui un utente inserisce la \textit{query} nel motore di
ricerca.  Data una interrogazione $q$, sia $q'$ il contesto di $q$. In
altri termini, se la \textit{query} è stata fatta selezionando una
parola di una pagina web $u$, sia $q'$ fatta dalle parole di $u$. Per
le altre \textit{query} (che sono certamente ben più diffuse) sia $q'
= q$.  Si calcolano le probabilità\index{probabilità} per ciascuna
delle 16 categorie della ODP\index{categorie della ODP}, condizionate
su $q'$. Sia $q'_i$ il termine $i$-esimo in $q'$. Data
l'interrogazione $q$ si può pertanto calcolare per ogni $c_j$ la
seguente probabilità:

\begin{equation}
P(c_j|q') = \frac{P(c_j) \cdot P(q'|c_j)}{P(q')} = \frac{P(c_j) \cdot \prod_{i}{P(q'_i|c_j)}}{P(q')}
\end{equation}

Sebbene nell'articolo non siano completamente chiarite le assunzioni
sul modello stocastico del testo delle pagine che giustifichino tali
passaggi, $P(q'_i|c_j)$ può essere facilmente calcolato dal vettore
dei termini $D_j$ in quanto tale probabilità\index{probabilità} altro
non è che il numero di occorrenze totali presenti in $D_j$ diviso per
il numero di occorrenze del termine $q'_i$ in $D_j$.  La quantità
$P(c_j)$ invece non è così immediata da calcolare. Nell'articolo
citato, Haveliwala la considera uniforme ma suggerisce l'idea che tale
probabilità potrebbe essere personalizzata a seconda dei gusti
dell'utente, al fine da rendere maggiormente ``pesanti'' alcune
categorie rispetto ad altre (in maniera tale che ad un ipotetico
agricoltore a cui interessino le ``API'' non compaiano le ``API'' di
un qualunque sistema operativo).  Inoltre la personalizzazione di
$P(c_j)$ risulta essere un buon espediente per variare la
distribuzione di probabilità dei risultati senza però variare il
vettore di personalizzazione $E$.

Il passo successivo alla stima della probabilità sopra citata risulta
essere quello di utilizzare un indice testuale (ad esempio un indice
inverso) per estrarre tutti i documenti che contengono i termini
dell'interrogazione originale $q$. Infine, si calcola un punteggio
composito di ciascuno di questi documenti come segue. Sia $rank_{jd}$
il punteggio del documento $d$ dato dal vettore $PR(\alpha, v_j)$
(cioè il vettore di PageRank\index{PageRank} per la categoria
$c_j$). Per il documento $d$ viene calcolato un punteggio $s_{qd}$
come segue:

\begin{equation}
s_{qd} = \sum_{j}{P(c_j|q')} \cdot rank_{jd}
\end{equation}

I risultati vengono poi ordinati in base a questo
punteggio\index{punteggio} finale.  Ovviamente, in sede di
implementazione, sarebbe possibile complicare a piacere la formula
appena citata per introdurre eventuali altri fattori di peso, come ad
esempio la Proximity\index{Proximity} di un documento oppure il
punteggio di altri algoritmi.

\`E da notare che il calcolo appena illustrato ha un'interpretazione
probabilistica\index{probabilità} in termini del modello del
navigatore casuale cui si è già fatto cenno in
PageRank\index{PageRank}.  Con probabilità $1 - \alpha$ infatti il
navigatore che si trova sulla pagina $u$ segue uno tra i link presenti
in $u$ (dove la scelta tra uno di questi è uniformemente casuale). Con
probabilità $\alpha \cdot P(c_j|q')$ invece il navigatore salta ad una
delle pagine in $T_j$ (dove la scelta tra una di queste pagine è
uniformemente casuale).  La probabilità --- vista sulla visita a lungo
termine --- che il navigatore si trovi a navigare sulla pagina $v$ è
esattamente data dal punteggio composito $s_{qd}$ definito
sopra. Pertanto gli argomenti della ODP\index{categorie della ODP}
influiscono sul punteggio finale in proporzione alla loro affinità con
la \textit{query} (od il suo contesto).

\subsection{Effetti del peso con la ODP}

Haveliwala ha calcolato quanto sia grande l'effetto di pesare i
vettori di PageRank\index{PageRank} con le pagine della
ODP\index{ODP}.  Anzitutto una prima considerazione che è stata fatta
è stata quella di vedere cosa accade per diversi valori del parametro
$\alpha$. Si consideri anzitutto il caso estremo in cui $\alpha = 1$:
in tale ipotesi le URL nell'insieme $T_j$ avranno punteggio
$1/|T_j|$ oppure $0$. Viceversa, se $\alpha$ è prossimo a $0$,
il contenuto di $T_j$ tende a diventare irrilevante per il punteggio
finale.

La scelta euristica operata da Haveliwala è stata di scegliere
$\alpha$ pari a $0.25$.  Tuttavia, in fase di sperimentazione,
Haveliwala afferma che è emerso che una scelta di $\alpha$ tra $0.05$
e $0.25$ non varia significativamente i punteggi indotti.

\subsection{Punteggio context-sensitive}

Come si è discusso prima, nell'articolo di Haveliwala che presenta
Topic-sensitive PageRank\index{Topic-sensitive PageRank} si fa in più
occasioni riferimento all'idea di estrapolare in una qualche maniera
il contesto dall'interrogazione che l'utente inserisce. Ad esempio,
viene proposta l'idea di prendere come contesto l'insieme delle parole
vicine ad una parola che viene selezionata su una pagina per essere
poi ricercata tramite il motore di ricerca.  L'idea è molto
interessante, ma occorrerebbero certamente
\textit{plug-in} software e si entrerebbe in un mondo che con i motori di
ricerca e gli algoritmi di \textit{ranking} ha poco a che fare. 
Ciononostante, l'idea è molto
interessante da un punto di vista socio-linguistico: alcune parole, in
certe lingue, hanno doppi significati (\emph{polisemia}) e vengono
spesso utilizzate in ambiti totalmente diversi per denotare concetti
completamente diversi. Un esempio è sicuramente il termine ``Apple''
in lingua inglese, che indica la mela (il frutto) ma anche una nota
industria nel campo dell'informatica, e il sopracitato termine
``api'', che può assumere nella nostra lingua almeno 3 significati: il
nome della catena di distributori di benzina, il plurale del nome
dell'insetto ``ape'', ed infine l'acronimo per ``Application
Programming Interface'', oramai entrato in uso comune tra gli addetti
ai lavori.  Considerare le parole che stanno accanto alla parola
``Api'' può portare in tal senso a disambiguare il senso della frase.

Al momento dell'interrogazione, in tal caso, si calcolerebbe
$P(c_j|q')$ come descritto precedentemente, usando per $q'$ anche i
termini adiacenti al termine ricercato. Nel nostro esempio, nel primo
caso (distributore di benzina) $argmax_{c_j}P(c_j|q')$ potrebbe
essere la categoria della ODP\index{categorie della ODP} (italiana)
AFFARI, mentre nel caso ad esempio del terzo significato
(``Application Programming Interface'') la categoria giusta sarebbe
COMPUTER.

\`E proprio in casi simili che la scelta del vettore di
PageRank\index{PageRank} da applicare contribuisce a disambiguare in
maniera definitiva il termine. Tuttavia, a nostro modo di vedere,
questo approccio è di difficile applicabilità nell'ambito dello
sviluppo di un motore di ricerca, e richiede certamente un notevole
\textit{feedback} da parte degli utenti per risultare veramente utile.
Si tenga inoltre conto che le ricerche normalmente fatte dall'utente
medio tipico sono di una o due parole direttamente inserite nel motore
di ricerca~\cite{silverstein98}, e non evidenziate in una pagina come
suggerisce Haveliwala: pertanto la determinazione del contesto
potrebbe essere impossibile in casi simili.  In altri casi nei quali
l'utente si trova di fronte a risultati fuorvianti, egli è già di per
sé tipicamente portato a raffinare la ricerca con termini più
specifici, e in questo senso un notevole aiuto viene portato da
algoritmi quali Proximity\index{Proximity} (vedi
paragrafo~\ref{paragrafo_proximity}). Infatti con tali algoritmi si
riescono già a scremare grosse quantità di risultati in maniera da
rispondere al meglio ad interrogazioni più sofisticate (cioè a quelle
interrogazioni che portano con sé il contesto in cui vengono
effettuate).

\subsection{Difetti di Topic-sensitive PageRank}
\label{paragrafo_difetti_topic_sensitive}

Come evidenziato da Haveliwala stesso e come si può anche intuire dal
suo lavoro, vi sono alcuni problemi che riguardano l'algoritmo di
Topic-sensitive PageRank\index{Topic-sensitive PageRank}.

Anzitutto viene fatta una suddivisione molto blanda degli argomenti
per i quali calcolare i vettori di PageRank\index{PageRank}. Questo è
dovuto essenzialmente al fatto che, oltre allo spazio su disco che
richiede ciascun vettore di PageRank\index{PageRank} (spazio comunque
relativamente trascurabile), il tempo richiesto per calcolare un
numero elevato di vettori di PageRank\index{PageRank} può essere alto.
Un altro problema che si può evidenziare è che inserire una ricerca
nel suo contesto può essere una operazione non semplice. Haveliwala
propone in maniera non meglio precisata la determinazione del contesto
in base alle pagine da cui l'utente esegue la sua ricerca: come farlo,
quali eventuali \textit{plug-in} software adottare e a quali standard
aderire rimangono tuttavia lasciati al lettore. C'è da notare infine
che nessun motore di rilievo (commerciale o meno) dichiara di
utilizzare tale algoritmo.

%%%
%%% Fine Topic-sensitive PageRank
%%%

\newpage

\section{Hits}
\label{paragrafo_hits}

Mentre gli algoritmi illustrati finora sono concepiti per rispondere
in maniera precisa alle richieste dell'utente, con l'algoritmo di
Hits\index{Hits}~\cite{kleinberg99authoritative} l'attenzione si
sposta di più verso l'individuare di pagine autorevoli riguardo
all'argomento trattato. In particolare l'algoritmo di Hits\index{Hits}
è un algoritmo applicabile soltanto a seguito di una prima
classificazione delle pagine: infatti esso richiede, proprio come
\textit{input}, un insieme di pagine determinate in una qualche
maniera --- ad esempio selezionate tramite gli algoritmi visti --- e
che soddisfino l'interrogazione dell'utente.  Anche Hits è, al pari
degli algoritmi visti finora, un algoritmo esogeno.

Uno degli aspetti più sorprendenti di questo algoritmo, peraltro molto
poco utilizzabile su vasta scala (come sarebbe, ad esempio, in un vero
e proprio motore di ricerca), è senza dubbio il fatto che, come nel
caso di PageRank, per determinare le pagine autorevoli si faccia
ricorso soltanto ad una analisi della struttura del grafo\index{grafo}
del web, e non invece a tecniche di matching testuale\index{matching
testuale}.

L'algoritmo di Hits\index{Hits} è in particolare noto per permettere
l'individuazione di \textit{community} all'interno del web in quanto
permette di estrarre moltissima informazione soltanto dall'analisi dei
link tra le pagine, e quindi da una semplice analisi del
grafo\index{grafo}.  Sebbene la tecnica proposta non sia solamente
applicabile al web, certamente è proprio in questo ambito che essa
risulta più utile: infatti il web è un insieme di pagine tra loro più
o meno fortemente collegate che continua a crescere in dimensioni ad
un ritmo esponenziale (si vedano~\cite{bharat98size}
e~\cite{netcraft}).

Un modo per cercare pagine autorevoli risulta essere quello di
consultare i repertori esistenti (es. Yahoo~\cite{yahoo},
DMOZ~\cite{dmoz}) compilati in maniera manuale o semiautomatica.
L'algoritmo Hits\index{Hits} si propone invece di fare tutto questo in
maniera totalmente automatica analizzando la struttura del
grafo\index{grafo} del web.

\subsection{Query e pagine autorevoli}

Anzitutto si deve tenere presente che le \textit{query} comunemente
ricevute dai motori di ricerca possono essere di varia natura: non
solo le si può distinguere banalmente tra di loro per l'argomento
trattato ma anche per il tipo di risultati che l'utente si aspetta di
ricevere.  Kleinberg\index{Kleinberg} fa la seguente classificazione:

\begin{itemize}
\item \textbf{interrogazioni specifiche}: ad esempio, ``Mozilla supporta xforms?''
\item \textbf{interrogazioni vaste}: ad esempio, ``Linux tutorial''
\item \textbf{interrogazioni volte a trovare pagine simili}: ad esempio, ``Trova
pagine simili a \texttt{http://java.sun.com/}''
\end{itemize}

Concentrandoci per il momento sui primi due tipi di \textit{query}, va
notato che mentre nel primo caso è molto difficile trovare la pagina
``giusta'' dal momento che ve ne sono molto poche che soddisfano
l'interrogazione ed è difficile recuperarle, nel secondo caso ve ne
sono invece molte, talmente tante che individuare esattamente quella
--- o quelle --- volute dall'utente risulta un'impresa molto
difficile.

La nozione di autorità\index{autorità}, relativamente ad una
interrogazione vasta, gioca un ruolo fondamentale. Anzitutto, come si
può stabilire se una pagina è autorevole?  Si può pensare, ad esempio,
che la pagina corrispondente all'indirizzo
\texttt{http://www.harvard.edu/} sia la pagina che per definizione un
utente desidera vedere elencata al primo posto nei risultati della
ricerca corrispondente alla \textit{query} \texttt{Harvard}.  Tuttavia
esistono innumerevoli pagine web che contengono il termine
\texttt{Harvard}: come individuare tra tutte queste proprio la pagina
giusta, cioè la \textit{home-page} della Harvard University? Inoltre
spesso e volentieri una pagina non avrà scritte dentro di essa (tutte)
le parole con la quale viene comunemente ricercata. Ne sono un
classico esempio i motori di ricerca, le cui \textit{home-page} certo
non contengono al loro interno le parole ``motore di ricerca''.

\subsection{Analisi della struttura dei link}

Analizzare la struttura a link del grafo\index{grafo} del web fornisce
la risposta a molte delle domande appena poste. I link contengono al
loro interno una parte considerevole di giudizio degli esseri umani,
ed è proprio questo giudizio ad essere quello che ci fa trovare le
pagine autorevoli.  In particolare, il creatore di una pagina web che
inserisce un link verso un'altra pagina web non fa altro che dare un
proprio giudizio alla pagina puntata: ``ti punto perché ti stimo''.
\`E questa senza dubbio la forma più democratica di votazione e
questo concetto è stato già largamente illustrato nel capitolo
dedicato all'algoritmo di PageRank\index{PageRank}.  Inoltre link di
questo tipo ci danno la possibilità di trovare potenziali pagine
autorevoli che altrimenti ci sarebbero sfuggite per il fatto che al
loro interno non contengono (tutte) le parole inserite
nell'interrogazione dell'utente.

Questa idea, che sembra ottima a prima vista, soffre tuttavia di
alcuni problemi che vanno subito chiariti e risolti: anzitutto
moltissimi link esistono per ragioni che sono esattamente l'opposto
del ``voto democratico'' cui si accennava. Ne sono un esempio i link
\textit{intra-host} (talvolta chiamati anche \textit{link navigazionali})
che servono, ad esempio, per ritornare all'\textit{home-page} oppure i
link presenti grazie alle sponsorizzazioni commerciali.

Inoltre, \textit{rilevanza} e \textit{popolarità} sono due concetti
ben distinti, ma entrambi servono per trovare le pagine autorevoli.
Infatti sarebbe molto sbagliato considerare valida questa euristica
per trovare pagine autorevoli: di tutte le pagine che soddisfano la
\textit{query}, restituisci quelle più puntate.  In questo caso infatti
verrebbero sempre restituiti link come
\texttt{http://www.google.com/}\index{Google} o
\texttt{http://www.microsoft.com/}, mentre questo non è certo quel che
vogliamo.

Il modello presentato da Kleinberg\index{Kleinberg} tiene conto di
tutti questi fattori ed evidenzia i modi per ottenere pagine
veramente autorevoli, senza troppi falsi positivi. Per fare ciò
l'algoritmo proposto si basa sulla individuazione dei cosiddetti
\textit{hub}\index{hub}, ossia delle pagine che più largamente
puntano a risorse autorevoli.  Si viene così a creare un circolo
virtuoso tra \textit{hub} ed \textit{autorità}\index{autorità}: una
autorità è una pagina puntata da molti hub, un hub è una pagina che
punta a molte autorità.

\subsection{Costruzione di un sottografo del web}

Supponiamo di ricevere una \textit{query} $q$ vasta (nel senso visto
prima): vogliamo determinare le pagina autorevoli basandoci su una
analisi della struttura dei link, ma prima dobbiamo determinare un
sottografo opportuno sul quale operare.  Per determinare tale
sottografo si potrebbero, ad esempio, considerare tutte le pagine che
soddisfano la
\textit{query}. Tuttavia un tale insieme di pagine può essere di svariati
milioni di pagine web, e pertanto potrebbe facilmente diventare
ingestibile per quanto riguarda i costi di calcolo. Inoltre un tale
approccio taglierebbe fuori le pagine magari più autorevoli ma che non
soddisfano l'interrogazione (si pensi all'esempio di prima sui motori di
ricerca).

Idealmente, vorremmo disporre di una collezione $S_q$ di pagine con
queste proprietà:

\begin{itemize}
\item $S_q$ è relativamente piccolo;
\item $S_q$ è ricco di pagine rilevanti;
\item $S_q$ contiene tutte --- o quasi --- le pagine più autorevoli.
\end{itemize}

Si noti che tali richieste sono rivolte essenzialmente a minimizzare
il costo computazionale di applicare un algoritmo decisamente non
banale ma anzi piuttosto costoso e che va purtroppo eseguito al
momento dell'interrogazione.

Come è possibile pertanto determinare tale insieme di pagine? 
Stabilito un parametro $t$ (che Kleinberg\index{Kleinberg} pone pari
a $200$), si estraggono le $t$ pagine con punteggio più elevato
secondo un qualche algoritmo di ricerca testuale (come ad esempio
Proximity\index{Proximity}) o secondo un qualche motore di ricerca
(Kleinberg\index{Kleinberg} ha usato i risultati di
Altavista\index{Altavista}); tali pagine vengono chiamate
\textit{root-set}\index{root-set} $R_q$. Questo insieme soddisfa in
generale i primi due punti della lista (è piccolo e ricco di pagine
rilevanti) ma è ben lontano dal contenere tutte o quasi le pagine più
autorevoli.  \`E altresì da notare che spesso vi sono pochissimi link
tra le pagine di questo insieme.  L'idea di Kleinberg\index{Kleinberg}
è di utilizzare tale insieme $R_q$ per costruire un insieme di pagine
$S_q$ che soddisfi anche l'ultimo punto desiderato.  Si consideri
infatti un'autorità\index{autorità} per la \textit{query} $q$: se non
è presente all'interno dei $t$ risultati trovati, è molto probabile
che sia puntata da almeno uno dei $t$ documenti. Pertanto possiamo
inserire delle autorità\index{autorità} all'interno di $S_q$
semplicemente seguendo i link che escono da $R_q$ e comprendendo le
pagine che puntano all'interno del root-set.  Nella pratica seguiremo
tutti i link uscenti dal root-set\index{root-set} $R_q$ per ogni
pagina del root-set, e al più $d$ link entranti nel root-set: questo
requisito diventa essenziale per evitare che vengano inserite dentro
$S_q$ troppe pagine, con la conseguenza che i tempi di calcolo
aumentino notevolmente.  $S_q$ viene detto
\textit{base-set}\index{base-set} per $q$.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/hits1.eps}}
\caption{Espansione di un \textit{root-set} in un \textit{base-set}.}
\end{center}
\end{figure}

Prima di procedere ad analizzare l'algoritmo di Hits\index{Hits} in
sé, ha senso spiegare una piccola euristica che si può applicare al
fine di eliminare i link utili solamente per la navigazione e che
inficierebbero l'algoritmo appena
presentato. Kleinberg\index{Kleinberg} propone di eliminare dal
grafo\index{grafo} tutti i link \textit{intra-host}, ossia tutti i
link interni ad uno stesso dominio. Questo è utile perché i link
\textit{intra-host} sono solitamente link puramente navigazionali e che non
portano all'individuazione di autorità\index{autorità} ma solo di
pagine inutili per gli scopi l'algoritmo.  Si possono naturalmente
pensare altre euristiche per eliminare link che rischiano di portare
\textit{spam}\index{spam} all'interno dei risultati, come ad esempio
non permettere ad una pagina di portare più di un numero fissato $m$
di pagine dello stesso dominio.  Secondo i test di
Kleinberg\index{Kleinberg} tuttavia anche senza questa ulteriore
restrizione l'algoritmo funziona molto bene.

In pseudo-codice, un algoritmo per determinare il
base-set\index{base-set} risulta essere quello riportato in
figura~\ref{algoritmo_baseset}:

\begin{figure}
\Line
\begin{tabbing}
\hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \=\kill
$S_q = R_q$\\
\kw{for} $p$ \kw{in} $R_q$\\
\>aggiungi $G(p,-)$ a $S_q$\\
\>\kw{if} $|G(-,p)| \leq d$ $S_q = S_q \cup G(-,p)$\\
\>\kw{else} $S_q = S_q \cup$ $\{d$ pagine di $G(-,p)\}$\\
\kw{return} $S_q$
\end{tabbing}
\caption{L'algoritmo per determinare il \textit{base-set}.}
\label{algoritmo_baseset}
\hspace*{\fill}\\
\Line
\end{figure}

\subsection{Determinazione di hub ed autorità}

Nella sezione precedente si è illustrato un metodo per calcolare un
sottografo del grafo\index{grafo} del web ``a tema'', ossia incentrato
sull'argomento che si sta ricercando.  Il problema è ora di estrarre
le pagine rilevanti e le autorità\index{autorità} da questo insieme
basandosi solamente sull'analisi ipertestuale del sottografo
preventivamente ottenuto.

Un'idea possibile è quella di ordinare questo insieme di pagine in base al
grado entrante\index{grado entrante} delle stesse. Purtroppo tale
approccio si rileva fallimentare perché spesso e volentieri le pagine
del sottografo contengono un insieme di link a pagine pubblicitarie
che verrebbero pertanto indicate come autorità\index{autorità} quando
non è detto che lo siano.  Pertanto, pagine con un alto grado
entrante\index{grado entrante} non sono necessariamente le pagine più
autorevoli.

\begin{figure}[htp]
\begin{center}
\includegraphics{fig/hits2.eps}
\caption{Un insieme molto collegato di hub ed autorità.}
\end{center}
\end{figure}

Invece, bisogna notare che un'autorità\index{autorità} sarà puntata da
molte pagine del base-set\index{base-set}: pertanto vi saranno alcune
pagine del base-set che avranno un insieme non disgiunto di link
uscenti\index{link uscenti}. Le pagine del base-set che puntano a
pagine autorevoli saranno dette hub\index{hub}, mentre le pagine
autorevoli sono tali perché puntate da molti hub\index{hub}. Questa
relazione che si rafforza mutuamente è la chiave di tutto l'algoritmo
di Hits\index{Hits}.

\subsection{L'algoritmo Hits}

La procedura per determinare gli hub\index{hub} e le
autorità\index{autorità} può essere implementata in modo iterativo ed
essere eseguita per un prestabilito numero di iterazioni, oppure
fintantoché la norma del residuo\index{residuo} tra due iterazioni
successive non scende al di sotto di una certa soglia, similmente a
quanto discusso in precedenza per l'algoritmo di
PageRank\index{PageRank}.

\begin{figure}[htp]
\begin{center}
\includegraphics{fig/hits3.eps}
\caption{Le operazioni basilari di Hits.}
\end{center}
\end{figure}

Usando le notazioni viste nella sezione precedente, un semplice
algoritmo in pseudo-codice per Hits\index{Hits} su un
grafo\index{grafo} $G$ è quello riportato in
figura~\ref{algoritmo_hits}, dove $G$ è un grafo composto da $n$
pagine e $k$ un numero naturale.

\begin{figure}
\Line
\begin{tabbing}
\hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \= \hspace{0.5cm} \=\kill
$z = (1, 1, 1, \cdots, 1) \in \mathbf{R}^n$\\
$x_0 = z$\\
$y_0 = z$\\
\kw{for} $i = 1, 2, \cdots, k$\\
\>$x'(p) := \sum_{q \to p} y(q)$\\
\>$y'(p) := \sum_{p \to q} x(q)$\\
\>$x_i$ = normalizza $x'_i$\\
\>$y_i$ = normalizza $y'_i$\\
\kw{return} $(x_k, y_k)$
\end{tabbing}
\caption{L'algoritmo Hits.}
\label{algoritmo_hits}
\hspace*{\fill}\\
\Line
\end{figure}

\subsection{Query volte a trovare pagine simili}

L'algoritmo descritto precedentemente può anche essere applicato ad un
altro tipo di problema, ossia al problema di trovare pagine simili ad
una pagina data.  Supponiamo di aver trovato una pagina $p$ di
interesse --- ad esempio, una pagina autorevole su un argomento di
nostro interesse --- e ci chiediamo quali altri altre pagine gli
utenti del web considerano essere simili a $p$ quando creano i link.

Se $p$ è una pagina molto referenziata, abbiamo un problema di
abbondanza: la struttura di link vicina a $p$ rappresenterà una
enorme quantità di opinioni indipendenti circa la relazione di $p$ con
le altre pagine. Usando la nozione vista di hub\index{hub} ed
autorità\index{autorità}, possiamo chiederci: nelle regioni della
struttura di link vicino a $p$, quali sono le autorità? Tali
autorità potrebbero infatti darci una idea molto chiara dell'argomento
trattato dalle pagine simili a $p$.

Pertanto, anziché inziare la ricerca con una interrogazione $q$,
inizieremo la ricerca con una pagina $p$ e cercheremo un insieme di
$t$ pagine che puntano a $p$. A questo punto creeremo il nostro
root-set\index{root-set} $R_p$ consistente nelle prime $t$ pagine
trovate e, successivamente, inseriremo altri elementi in $R_p$
affinché diventi un insieme $S_p$ come fatto in precedenza. Nel
sottografo risultante cercheremo poi hub\index{hub} ed
autorità\index{autorità}.

\subsection{Hits: sperimentazione}

L'algoritmo di Hits è stato implementato e fa parte del progetto
associato a questa tesi. Essendo tuttavia un algoritmo da calcolare
completamente a tempo di interrogazione, i tempi di esecuzione sono
parecchio elevati.  Vi sono stati vari problemi durante
l'implementazione di Hits: uno di questi è certamente il fatto che il
grafo deve essere caricato interamente in memoria, e che pertanto
occorrono delle tecniche di compressione efficienti ma al contempo
rapide nell'accesso; mantenere le liste di adiacenza non compresse è
stato invece causa di notevoli problemi di \textit{garbage collection}
(si veda il paragrafo~\ref{paragrafo_dettagli_implementativi}).

Un altro problema riscontrato sottoponendo manualmente delle
interrogazioni ad Hits è stato il fatto che Hits appare molto
suscettibile al fenomeno dello \textit{spamming}\index{spam}.
Infatti, ricercando parole molto diffuse i risultati appaiono
visibilmente falsati dalla presenza di pagine contenenti letteralmente
dozzine di parole chiave e collegamenti verso altre
pagine. L'algoritmo di Hits, a fronte dei problemi or ora evidenziati,
non è pertanto sembrato particolarmente utile per l'utilizzo nel
motore di ricerca, ed è pertanto stato inserito soltanto come
algoritmo ausiliario che può essere richiamato a scelta dell'utente in
seconda battuta, dopo aver ricevuto i risultati calcolati dagli altri
algoritmi.

%%%
%%% Fine Hits
%%%

%%%
%%% Fine sezione algoritmi su grafo
%%%

\newpage

\newchap{Algoritmi basati sul contenuto}

In questa sezione verranno presentati alcuni algoritmi che, a
differenza di quelli illustrati nei precedenti capitoli, si basano sul
contenuto delle pagine web per assegnare i punteggi e forniscono
pertanto una misura endogena di rilevanza.

Bisogna ricordare che gli algoritmi che vengono qui presentati sono
utilissimi per fornire un input ridotto agli altri algoritmi di
ranking illustrati precedentemente.  Infatti, come si è visto,
PageRank\index{PageRank} di per sé fornirebbe sempre la stessa lista
di risultati, sempre nello stesso ordine, essendo l'ordinamento di
PageRank\index{PageRank} un ordinamento statico dei risultati.
Combinando tuttavia PageRank\index{PageRank} con almeno uno degli
algoritmi qui presentati si può ottenere un insieme di risultati che
variano in funzione della interrogazione dell'utente, e che pertanto
sono certamente più interessanti sotto ogni punto di vista.

I punteggi da assegnare a questi algoritmi, come combinarli tra loro e
altre considerazioni sono invece materia di un capitolo successivo
riguardante uno studio portato avanti durante questo lavoro di tesi
(vedi capitolo~\ref{paragrafo_aggregazione}).

Per quanto riguarda le parti da precomputare, gli algoritmi che
verranno illustrati richiedono generalmente la generazione a priori
degli indici inversi, necessari per determinare quali documenti
soddisfano l'interrogazione. Fa eccezione il solo algoritmo di Latent
Semantic Indexing, che richiede invece il calcolo di una
decomposizione a valori singolari di una matrice.

\newpage

\section{Latent Semantic Indexing}

\subsection{Introduzione}

Il primo algoritmo endogeno che viene presentato è il cosiddetto
algoritmo di Latent-Semantic Indexing (comunemente riferito come LSI).
Tale algoritmo è un algoritmo dell'IR classico ed è stato storicamente
adottato per un lungo periodo da famosi motori di ricerca come
Altavista\index{Altavista} e si ritiene che venga tutt'oggi adottato
in alcuni motori di ricerca di news e di immagini.

L'algoritmo di LSI\index{LSI}~\cite{deerwester90indexing} è in realtà
contrapposto agli altri algoritmi di
\textit{matching} testuali\index{matching testuale}, in quanto non serve
ad estrarre le pagine che contengono certi termini e ad eliminare le
altre, ma serve per estrapolare relazioni ben più sottili del semplice
``è presente''/``non è presente''.  Infatti tale algoritmo non solo
registra quali documenti contengono quali parole, ma considera anche
l'intera collezione di documenti nel suo complesso per determinare
quali documenti contengono un certo insieme di parole. LSI\index{LSI}
cataloga i documenti che hanno molte parole in comune come
\emph{semanticamente correlati}, e quelli che invece ne hanno poche in
comune come \emph{semanticamente distanti}. Questo metodo, dall'idea
molto semplice, nella pratica si avvicina moltissimo a quel che
farebbe un essere umano chiamato a catalogare quello stesso insieme di
documenti, come dimostrano vari studi~\cite{landauer}. Sebbene
LSI\index{LSI} non utilizzi alcuna informazione semantica, i
\textit{pattern} che riconosce lo rendono tanto apparentemente quanto
sorprendentemente intelligente.

Quando si effettua una ricerca all'interno di una base di dati
indicizzata tramite l'algoritmo di LSI\index{LSI}, anzitutto vengono
confrontati i valori di somiglianza che sono stati calcolati per ogni
parola, e vengono restituiti i documenti che LSI\index{LSI} ritiene
che meglio soddisfino l'interrogazione.  Dal momento che due documenti
possono essere semanticamente correlati anche se non condividono
parole particolari, LSI\index{LSI} non richiede un \textit{match}
esatto per restituire risultati utili, cosa che invece richiedono gli
altri algoritmi di matching testuali\index{matching testuale} che si
illustreranno.

La complessità maggiore dell'algoritmo di Latent Semantic Indexing
risulta risiedere nella decomposizione della matrice, essendo gli
algoritmi noti cubici in tempo. Quando eseguito su una collezione di
dati ampia come quella del web, i tempi aumentano notevolmente al punto
che l'algoritmo di LSI non risulta praticamente applicabile.

Un altro difetto dell'algoritmo di LSI\index{LSI} è quello di
richiedere tempi molto elevati per essere eseguito. L'algoritmo di
LSI\index{LSI} infatti si compone essenzialmente di due parti, una
precomputata, molto dispendiosa, ed una calcolata invece al momento
dell'interrogazione.  La parte precomputata risulta essere una
decomposizione in valori singolari di una matrice enorme, e non è
pertanto realistico pensare ad aggiornamenti frequenti dell'insieme
delle parole, nonostante in letteratura siano note tecniche per
parallelizzare il calcolo\index{parallelizzazione}.

\subsection{Funzionamento}

Per funzionare LSI\index{LSI} richiede anzitutto la compilazione di
una lista di termini contenuti all'interno della collezione di
documenti da indicizzare. Tale lista dovrà essere opportunamente
epurata dalle cosiddette \textit{stop-word}\index{stop-word}, ossia
dalle parole molto comuni che compaiono in pressoché tutti i documenti
del nostro insieme ed anche più volte all'interno dello stesso
documento.  Esempi di tali parole sono gli aggettivi comuni, i verbi
comuni, gli articoli, le preposizioni.  Inoltre, dalla lista vengono
eliminati gli \textit{hapax legomena}, cioè i termini estremamente
rari che compaiono in un solo documento, frutto spesso di errori di
digitazione.

Una volta generata tale lista si genera la cosiddetta \textit{matrice
dei termini e dei documenti}\index{matrice dei termini e dei
documenti}.  Si può pensare a questa matrice come una griglia nella
quale i documenti vengono messi sull'asse orizzontale, mentre i
termini --- ossia le parole contenute nei documenti --- vengono posti
sull'asse verticale. Per ogni parola della nostra lista, si inserisce
un numero che indica la frequenza relativa del termine in
corrispondenza dei documenti nei quali tale parola compare, cioè il
rapporto fra le sue occorrenze nel documento e quelle nell'intera collezione.

Una volta compilata, tale griglia riporta in maniera concisa pressoché
tutto quel che sappiamo sulla nostra collezione di documenti:
potremmo, volendolo fare, elencare le parole di cui è composto un
documento andando a vedere, per quel documento, le parole a cui
corrisponde un valore non nullo, e potremmo anche, per ogni parola,
dire in quali documenti essa compare con un procedimento analogo.

Quel che accade è che l'algoritmo LSI\index{LSI}, durante la parte
precomputata, tramite la decomposizione in valori singolari della
matrice originale calcola un'approssimazione di rango basso della
matrice dei termini e dei documenti che consente un calcolo
approssimato ma più efficiente delle misure di affinità.  Si osserva
empiricamente che la riduzione di rango oltre ad aumentare la velocità
migliora anche la precisione semantica riducendo il rumore.

Al momento dell'interrogazione invece l'interrogazione dell'utente
viene rappresentata come un vettore $k$-dimensionale e confrontata con
i documenti.  Per determinare la somiglianza tra l'interrogazione e i
documenti possono venire utilizzate varie misurazioni, ma quella in
assoluto più nota e largamente più usata è una misura basata sul
coseno e definita come segue.

Sia $t$ il numero di termini trovati nella prima fase dell'algoritmo.
Ad ogni pagina $p$ è associato un vettore di $t$ elementi $d_p$ dove
$(d_p)_j$ è il numero di occorrenze del termine $j$ in $p$

Ad ogni interrogazione $q$ si associa un analogo vettore $d_q$ che ha
un $1$ in corrispondenza dei termini che compaiono nell'interrogazione,
e uno $0$ altrimenti.
Si tratta di determinare l'affinità tra $p$ e $q$:

\begin{equation}
\cos(\vec{d_p} \vec{d_q}) = \frac{\vec{d_p} \times \vec{d_q}}{\|d_p\| \cdot \|d_q\|}
\end{equation}

Per capire il significato di questa formula, si pensi allo spazio
multidimensionale rappresentato dai possibili argomenti a cui una
pagina può appartenere: in tal caso, per ciascuna dimensione dello
spazio vi sarà un vettore che rappresenta ``quanto'' la pagina tratta
di quell'argomento. Considerando l'angolo compreso tra i vettori degli
argomenti dell'interrogazione e quello della pagina tramite il coseno
si riesce a stimare la vicinanza dei due vettori.

Viste le prestazioni di LSI e le critiche che gli sono state mosse in
letteratura si è scelto di non implementare questo algoritmo.

%%%
%%% Fine LSI
%%%

\newpage

\section{Proximity}
\label{paragrafo_proximity}

L'algoritmo di Proximity\index{Proximity} è un altro noto algoritmo
endogeno che nell'opinione comune si ritiene sia utilizzato da quasi
tutti i motori di ricerca commerciali.  Questo algoritmo si basa su
una idea molto semplice: più le parole dell'interrogazione sono vicine
tra loro in una data pagina, più il punteggio di
Proximity\index{Proximity} sarà alto per quella pagina.  Escludendo le
pagine con Proximity\index{Proximity} nulla, si ottiene una lista dei
documenti che soddisfano l'interrogazione dell'utente; tale lista
potrà poi essere passata ad algoritmi quali PageRank\index{PageRank}
per affinarne il punteggio in una qualche maniera.

Si possono dare varie definizioni che permettono di ricavare il
punteggio di Proximity\index{Proximity}: questo fatto --- sicuramente
non positivo --- è dovuto sia alla possibile scelta dell'insieme di
operatori inseribili in una data interrogazione sia alla scelta,
completamente libera per chi fornisce l'implementazione, di
considerare gli intervalli minimali o massimali.

Ad esempio, un punteggio\index{punteggio} possibile della
Proximity\index{Proximity} è dato dalla formula:

\begin{equation}
s = \frac{N_q}{L_{min}}
\end{equation}

dove $N_q$ rappresenta il numero di parole presenti nella
interrogazione $q$, mentre $L_{min}$ rappresenta la lunghezza
dell'intervallo minimo. L'intervallo minimo risulta essere definito
come l'intervallo di lunghezza minima che si può trovare in un
documento soddisfacente l'interrogazione. Tale grandezza può essere
pari al numero di parole della \textit{query} se l'\textit{operatore
booleano} utilizzato nell'interrogazione è il solo \texttt{AND},
minore se si utilizzano anche altri operatori booleani (ad esempio se
si usa l'\texttt{OR}).

Nel caso del motore Ubi\index{Ubi} si è scelto di fornire una
implementazione basata sui seguenti operatori logici\index{operatori
logici}, come presentato in~\cite{boldi02proximity}:

\begin{itemize}
\item \texttt{AND} --- operatore di default, vengono ricercate
corrispondenze con tutti i due blocchi forniti come argomento;
\item \texttt{OR} --- richiede la presenza di almeno uno dei due blocchi
forniti come argomento;
\item \texttt{BLOCK} --- richiede che le parole inserite come argomento
del \texttt{BLOCK} tra loro siano consecutive, nell'ordine in cui
vengono fornite;
\item \texttt{CLUSTER} --- si comporta come l'operatore \texttt{BLOCK},
ma non richiede che l'ordine delle parole sia quello fornito dall'utente;
\item \texttt{NEAR} --- permette di selezionare gli intervalli tali che
le parole fornite come argomento siano al più ad una certa distanza
l'una dall'altra (la distanza è fornita dall'utente);
\item \texttt{LOWPASS} --- permette di scegliere soltanto gli intervalli
di al più una certa lunghezza all'interno di un documento che
soddisfa l'interrogazione.
\end{itemize}

Si noti la mancanza dell'operatore \texttt{NOT}, non implementato a
causa della difficoltà di interpretazione della sua semantica
(vedi~\cite{boldi02proximity}).  In particolare, tutti questi
operatori sono quelli presenti nel software utilizzato per determinare
i documenti che soddisfano le interrogazioni degli utenti.  Questo
software in particolare ha la caratteristica, dato un documento che
soddisfa l'interrogazione, di restituire l'intervallo minimo
all'interno di tale documento che soddisfa tale interrogazione.
Alternativamente, possono venire fornite informazioni quali
l'intervallo minimo e massimo, la lunghezza media degli intervalli, il
numero di intervalli contenuti all'interno di un documento, ecc.

La formula fornita precedentemente per il calcolo del punteggio di
Proximity\index{Proximity} nel nostro caso è stata leggermente variata
e raffinata. Infatti, volendo avere valori di
Proximity\index{Proximity} normalizzati tra $0.0$ e $1.0$ non si può
considerare il numero di parole dell'interrogazione: cosa succederebbe
con una interrogazione fatta di parole tutte in \texttt{OR}?  In tal
caso il numero delle parole sarebbe sicuramente molto maggiore della
lunghezza dell'intervallo minimo, che sarà pari ad $1$ essendo la
lunghezza minima una qualunque delle parole. Si è perciò scelto di
considerare, in luogo del numero delle parole dell'interrogazione, la
minima lunghezza possibile di un intervallo soddisfacente la
\textit{query}.  Ad esempio, si consideri l'interrogazione:
\texttt{(microsoft corporation) OR (gates)}
In tal caso il numero di parole è $3$. Se trovassimo una pagina con la
parola \texttt{gates}, usando la prima formula otterremmo un punteggio
pari a $3$; usando invece l'idea illustrata prima la minima lunghezza
possibile sarebbe pari a $1$ (ossia il numero di parole contenute nel
secondo \textit{token} dell'interrogazione, cioè la parola
\texttt{gates}).  In tal caso il punteggio sarà quindi pari a
$1/1 = 1.0$.

L'algoritmo di Proximity\index{Proximity} è di indubbia utilità nel
fornire un punteggio significativo alle pagine, e senza alcun dubbio
il punteggio fornito è uno dei più chiari e meglio funzionanti tra
quelli esistenti.  Tuttavia anche questo algoritmo ha una pecca: ci
stiamo riferendo al punteggio che viene fornito nel caso di
un'interrogazione mono-parola o di un'interrogazione con parole
singole tutte in \texttt{OR}.

In tali situazioni infatti il punteggio fornito da
Proximity\index{Proximity} per i documenti che soddisfano
l'interrogazione sarà sempre $1.0$ (mentre sarà sempre $0.0$ nel caso
dei documenti che non la soddisfano), come si può facilmente
verificare dalla formula appena introdotta. Questo significa che, per
ogni documento che soddisfa l'interrogazione, il suo punteggio sarà
sempre lo stesso.  Questa è ovviamente una limitazione piuttosto
pesante, essendo le interrogazioni mono-parola molto diffuse --- si
vedano, ad esempio, i rapporti periodici dei motori di
ricerca~\cite{zeitgeist}.

Come comportarsi in tali casi? Vi sono due semplici soluzioni:
aggiungere un'altra euristica, oppure non farlo.  La scelta di
abbinare un altro algoritmo od un'altra euristica a
Proximity\index{Proximity} non è stata tuttavia fatta in funzione
della seguente considerazione: le interrogazioni mono-parola sono, per
loro stessa natura, tese ad individuare pagine molto generali, molto
note e facilmente raggiungibili. Stiamo parlando di pagine che
corrispondono alle \textit{home-page} di aziende famosissime, o di
enti governativi, o di siti rinomati. I termini che si possono cercare
sono quindi tutti marchi famosi, oppure parole comuni della lingua in
cui viene fatta l'interrogazione.

In tali casi vi sono ben altri algoritmi che sopperiscono alle
mancanze di Proximity\index{Proximity}, e non ha senso introdurre una
ulteriore euristica per questi casi speciali (seppur diffusi).  Il
rischio è non soltanto quello di appesantire un software che deve
lavorare a ritmi serrati e a velocità impressionanti, ma anche quello
--- ben peggiore --- di introdurre \textit{spam}\index{spam} nei
risultati.

Va infine notato che recentemente alcuni motori di ricerca, in
particolare Google\index{Google}, sembrano aver introdotto una
variante all'algoritmo di Proximity\index{Proximity} basata sul fatto
che gli intervalli di
\textit{match} soddisfino l'interrogazione dell'utente con le parole in
ordine o meno rispetto all'interrogazione originaria.  Ossia, se
inseriamo la \textit{query} \texttt{bill gates} sarà preferita una
pagina contenente le due parole in questo ordine piuttosto che quella
con le parole in ordine inverso.  Non è parso tuttavia strettamente
necessario implementare una tale euristica: infatti accade spesso che
un utente non inserisca i termini nell'ordine nel quale si aspetta che
essi compaiano, ma capita talvolta che li inserisca nell'ordine in cui
gli vengono in mente pensando all'argomento per cui sta facendo la sua
interrogazione.  Va anche segnalato che non risulta affatto chiaro
come si possa calcolare il numero di inversioni su interrogazioni
complicate, ossia su interrogazioni che contengano alcuni degli
operatori \textit{booleani} visti.

Si potrebbero, ad esempio, adottare sofisticate tecniche di calcolo
della massima sottosequenza crescente, ma questo esula dagli
obbiettivi della tesi.

%%%
%%% Fine Proximity
%%%

\newpage

\section{Frequency-count}
\label{paragrafo_frequencycount}

L'algoritmo di Frequency-count\index{Frequency-count} è un algoritmo
endogeno molto semplice ma che nel contempo risulta decisamente
inaffidabile nei risultati restituiti quando applicato alle pagine
web. L'idea su cui l'algoritmo di Frequency-count si basa, come dice
il nome stesso, è il conteggio della frequenza con cui una parola
compare in un dato documento.  Anch'esso, come gli altri algoritmi,
fornisce soltanto i documenti che soddisfano l'interrogazione
dell'utente.  Tuttavia, in maniera ben peggiore degli altri casi, tale
algoritmo fornisce punteggi davvero improbabili che non rispecchiano
minimamente --- salvo rarissime circostanze --- la sensazione di
``aver centrato l'obbiettivo'' che il tipico utente di un motore di
ricerca ha quando vede comparire i risultati della sua interrogazione.

In realtà questo algoritmo non è inadatto in ogni situazione: vi sono
infatti casi particolari in cui la sua adozione può realmente portare
benefici (si veda, ad esempio, l'algoritmo di
AnchorRank\index{AnchorRank} che si presenterà successivamente), ma in
generale non è un algoritmo ottimale per dare un primo punteggio alle
pagine web.

Un interessante quesito sorge nel momento in cui ci si chiede come sia
definito il punteggio di tale algoritmo.  Si è detto che esso conta il
numero di volte nella quale l'interrogazione compare nel documento
considerato, e pertanto ne conta il numero di intervalli. Ma come
assegnare il punteggio\index{punteggio}?  Una buona scelta risulta
essere quella di dividere tale numero di intervalli per il numero di
parole presenti all'interno del documento considerato. In tal caso si
otterrà un punteggio normalizzato tra $0.0$ (ossia nessun
\textit{match}) e $1.0$ (ossia tutto il documento è un unico grande
\textit{match}).

Si è detto che Frequency-count\index{Frequency-count} non va bene
per assegnare un punteggio significativo ad una pagina, almeno in prima
battuta. Questo è dovuto al fatto che spesso e volentieri le pagine realmente
importanti contengono poche volte le parole delle tipiche interrogazioni che
dovrebbero invece individuarle: si pensi a pagine fatte di sole immagini,
oppure a pagine quali \texttt{http://www.fiat.com/} nelle quali la parola
``automobili'' non compare neppure una volta.

Conteggiare il numero di occorrenze di una parola all'interno di un
documento è inoltre rischioso perché si rischia di incappare molto
facilmente nel fenomeno dello \textit{spamming}\index{spam}.  Sarebbe
infatti sufficiente inserire una parola con la quale si desidera
essere trovati per un numero molto elevato di volte all'interno di una
pagina web per salire rapidamente in cima all'indice del motore di
ricerca.  Pur non essendovi fonti nella letteratura di settore al
riguardo che provino quanto si sta affermando, la conoscenza comune
riporta un esempio storico di questa spiacevole situazione in una
delle primissime versioni del motore di ricerca
Altavista~\cite{altavista}\index{Altavista}, il quale si pensa che
adottasse algoritmi basati su Frequency-count\index{Frequency-count}
(prima di passare ad LSI\index{LSI}).  Ancora oggi, a distanza di
anni, si trovano in rete testimonianze di quanto fosse facile ``salire
in cima'' agli indici riempiendo le proprie pagine di parole che non
riguardavano il contenuto della pagina.  Di tale sfruttamento si sono
ben presto resi conto anche gli ingegneri stessi di
Altavista\index{Altavista}, che hanno provveduto poi a migliorare i
loro algoritmi di ranking rendendo così la ``scalata'' molto più
difficile.

Si è anche detto che l'algoritmo di
Frequency-count\index{Frequency-count} non è del tutto inutilizzabile,
almeno in alcune circostanze. Infatti tale algoritmo ha senso quando
si considerano pagine che non sono legate tra loro, e che quindi non
sono facilmente influenzabili.  Tale situazione è quella tipica delle
ancore, ossia del testo utilizzato per i collegamenti
ipertestuali. Per maggiori informazioni si rimanda al
paragrafo~\ref{paragrafo_ancore}.

%%%
%%% Fine Frequency-count
%%%

\newpage

\section{Prominence}

Un algoritmo endogeno a cui si può pensare ma che in letteratura non è
stato mai presentato è l'algoritmo di Prominence, ossia un algoritmo
che dà più importanza al fatto che le parole ricercate dall'utente
siano relativamente ``in testa'' ad un documento piuttosto che ``in
coda''.

Si pensi, ad esempio, a quel che accade nei documenti estratti
seguendo i link dei quotidiani che pubblicano i loro articoli in
Internet: in tal caso il fatto che le parole ricercate siano
relativamente in testa al documento piuttosto che in coda fa una
differenza sostanziale, in quanto nel primo caso le parole faranno
probabilmente parte del titolo dell'articolo oppure del suo occhiello,
mentre nel secondo caso potrebbero essere contenute nel testo del
documento come semplice riferimento, e non essere pertanto centrali
all'interno dell'articolo in questione.

A questo punto vale la pena di chiedersi se un tale algoritmo scali
sul web, ossia se tale algoritmo può essere applicato anche ad insiemi
di documenti differenti ed eterogenei come quelli presenti sulle
miriadi di \textit{host} sparsi per il pianeta.  Una formula che si
potrebbe proporre è la seguente:

\begin{equation}
s = \frac{{E_{dx}}_{min}}{L_{doc}}
\end{equation}

dove ${E_{dx}}_{min}$ indica l'estremo destro dell'intervallo minimo
di \textit{match} mentre $L_{doc}$ indica la lunghezza del documento
in numero di parole.

Analizziamo tale formula. Anzitutto vorremmo in generale che il
punteggio\index{punteggio} assegnato ad una data pagina non dipendesse
dalle dimensioni del documento, altrimenti sarebbero sempre
privilegiati i documenti più brevi in quanto, per forza di cose, gli
intervalli di
\textit{match} sarebbero sempre nelle prime ``righe'' dei documenti.
Per evitare questo si è introdotta la normalizzazione per la lunghezza
del documento (tale lunghezza è espressa in parole, uniformemente con
la misura dell'estremo destro dell'intervallo, anch'essa espressa in
numero di parole).

Il numeratore dell'espressione appena vista è invece dato dalla
posizione (espressa sempre in numero di parole) dell'ultima parola tra
quelle contenute nell'interrogazione e presente nel documento in
questione. Questa misura ci dà una chiara indicazione della posizione
fino alla quale l'intervallo si estende, e la formula vista risulta
pertanto essere una ragionevole misura per stimare la posizione
relativa in cui compare l'intervallo di \textit{match} all'interno del
documento considerato.

Si noti che nella stima fornita non è presente alcuna indicazione
riguardo alla lunghezza dell'intervallo; questo, che apparentemente
può sembrare un grosso difetto, viene in realtà già calcolato nella
normalizzazione del punteggio rispetto alla lunghezza del documento.
Si supponga infatti di trovare un documento in cui le parole
dell'interrogazione sono abbastanza distanti tra di loro, ma si
trovano ragionevolmente in testa al documento. In una tale situazione
l'unico modo per avere un punteggio elevato è che il documento sia
molto lungo, come discende direttamente dalla formula.  In generale
quindi vengono prediletti intervalli brevi rispetto ad intervalli
lunghi, in quanto gli intervalli lunghi tendono ad essere centrati
verso la fine o la metà del documento che si sta considerando.

%%%
%%% Fine Prominence
%%%

%%%
%%% Fine algoritmi di matching testuali
%%%

\newpage

\newchap{L'aggregatore di ranker}
\label{paragrafo_aggregazione}

Per rispondere in maniera efficiente alle interrogazioni degli utenti
è necessario utilizzare più algoritmi di \textit{ranking}, uno in
contemporanea all'altro.  Purtroppo\footnote{O per fortuna, se si
pensa al fatto che fare \textit{spamming}\index{spam} con più
algoritmi è più difficile.} non esiste al giorno d'oggi un algoritmo
ideale che restituisca i risultati che si desidererebbero; esistono
solamente buoni algoritmi che, combinati tra loro, restituiscono i
risultati sperati.

Per tali ragioni si è creato una sorta di \emph{aggregatore di
ranker}, ossia uno strumento software in grado di interrogare
simultaneamente i vari algoritmi ed eseguire le varie euristiche per
poi restituire i risultati così ottenuti ed opportunamente pesati.

Per poter aggregare i risultati di vari \textit{ranker} vi sono varie
tecniche. Noi abbiamo deciso di adottare una aggregazione lineare.  Il
vantaggio di questa soluzione è che questo approccio permette di
essere facilmente cambiato se si desiderano cambiare i pesi dei vari
algoritmi per fare esperimenti.

Gli algoritmi che sono stati utilizzati per dare i punteggi alle pagine
sono cinque:
\begin{itemize}
\item PageRank\index{PageRank}, per scremare i punteggi delle pagine;
\item Proximity\index{Proximity}, usata per dare un punteggio
al testo delle pagine;
\item TitleRank\index{TitleRank}, per dare un punteggio al
titolo delle pagine;
\item URLRank\index{URLRank}, utilizzato per considerare nel
punteggio totale anche l'URL del sito web;
\item AnchorRank\index{AnchorRank}, per considerare l'esogenicità
derivante dalle ancore con cui i documenti sono puntati.
\end{itemize}

Si noti che ciascuno di questi algoritmi opera sull'intera collezione
delle pagine web recuperate; tuttavia, poiché si desidera restituire
all'utente soltanto una frazione di esse, è compito dell'aggregatore
scartare le pagine con punteggio nullo in qualcuno dei vari ranker a
disposizione.

Inizialmente l'insieme di risultati da restituire era fornito dai soli
documenti che soddisfacevano l'interrogazione nel testo con un
punteggio non nullo, basandosi per la scelta sui punteggi restituiti
dall'algoritmo di Proximity\index{Proximity}.  Successivamente, nel
momento in cui si sono implementate le ancore, si è deciso di fondere
questa lista di documenti assieme a quella restituita dall'algoritmo
di AnchorRank\index{AnchorRank} (sempre soltanto per i documenti con
punteggio non nullo in quest'ultimo ranker). Questa scelta è stata
fatta perché spesso una pagina non contiene tutti i termini con i
quali in realtà è conosciuta, un po' come accade per l'algoritmo di
Hits\index{Hits} (paragrafo~\ref{paragrafo_hits}.  Ad esempio, il sito
dell'agenzia ANSA (\texttt{http://www.ansa.it/}) non soddisfa
l'interrogazione \texttt{agenzia ansa}, mentre le ancore con le quali
viene puntata sì (per maggiori dettagli si veda il
paragrafo~\ref{paragrafo_ancore}).

I motori di ricerca commerciali, come ad esempio Google\index{Google},
si ritiene comunemente che utilizzino un algoritmo simile al seguente
per trovare i documenti da restituire (ad esempio, si vedano le
discussioni su~\cite{searchengineforums}):

\begin{itemize}
\item trova i documenti che soddisfano la \textit{query} il testo (Proximity);
\item trova i documenti che soddisfano la \textit{query} nei titoli (TitleRank);
\item trova i documenti che soddisfano la \textit{query} nelle URL (URLRank);
\item trova i documenti che soddisfano la \textit{query} nelle ancore (AnchorRank);
\item unisci gli insiemi così trovati;
\item calcola PageRank su questo insieme di documenti;
\item combina linearmente i punteggi trovati dai vari algoritmi
per ogni pagina;
\item restituisci l'insieme di pagine con il punteggio associato
a ciascuna di esse,
\end{itemize}

\noindent
mentre l'algoritmo da noi utilizzato risulta essere il seguente:

\begin{itemize}
\item trova i documenti che soddisfano la \textit{query} nel testo (Proximity);
\item trova i documenti che soddisfano la \textit{query} nelle ancore (AnchorRank);
\item unisci gli insiemi così trovati;
\item calcola PageRank, TitleRank ed URLRank su questo insieme
di documenti;
\item combina linearmente i punteggi trovati dai vari algoritmi
per ogni pagina;
\item restituisci l'insieme di pagine con il punteggio associato
a ciascuna di esse.
\end{itemize}

Il motivo per cui si è scelto un algoritmo più semplice è stato quello
di voler evitare di introdurre troppe pagine che non
necessariamente riguardano l'interrogazione in esame e che pertanto
rischiano di introdurre rumore nei risultati.

%%%
%%% Fine introduzione sull'aggregatore di ranker
%%%

\newpage

\section{Algoritmi utilizzati}

Nelle prossime sezioni verranno presentati tutti gli algoritmi di
\textit{ranking} che sono stati implementati nel lavoro
di tesi e per ciascuno di essi verrà data un'accurata descrizione del
suo funzionamento.

Per la presentazione di PageRank\index{PageRank} e
Proximity\index{Proximity}, algoritmi noti in letteratura ed inseriti
all'interno del motore di ricerca Ubi\index{Ubi}, si rimanda alle
rispettive sezioni.

%%%
%%% Fine presentazione algoritmi utilizzati
%%%

\subsection{PageRank e Proximity non bastano}
\label{paragrafo_pagerank_proximity_non_bastano}

Una prima aggregazione elementare consiste nell'associare i risultati
forniti dall'algoritmo di PageRank con quelli restituiti
dall'algoritmo di Proximity. Tuttavia, una aggregazione basata
solamente su questi due algoritmi risulta piuttosto inefficace,
essendo questi due \textit{ranker} insufficienti da soli per mettere
in piedi un motore di ricerca sufficientemente preciso e raffinato.
In particolare, il difetto più macroscopico risulta essere quello che
spesso e volentieri tra i primi dieci risultati restituiti non compare
la pagina cercata laddove la medesima interrogazione fatta ai motori
commerciali individua immediatamente in prima posizione il documento
ricercato.

La strada che si è seguita è stata utilizzare una combinazione lineare
dei risultati di questi due algoritmi, pesando maggiormente
PageRank\index{PageRank} poiché i suoi punteggi sono normalizzati a
$1$ sull'intera collezione delle pagine, mentre i punteggi di
Proximity\index{Proximity} non lo sono. Pur potendo cambiare i pesi
assegnati a questi due algoritmi, la situazione ugualmente non
migliora anche scegliendo altre combinazioni di pesi.

Si riporta qui di seguito, ad esempio, quel che accade inserendo
l'interrogazione \texttt{microsoft} al motore con in uso solamente gli
algoritmi di PageRank e Proximity (vengono mostrati solamente i primi
dieci risultati). I pesi scelti erano, in questo caso, di $8110.67$
per PageRank (numero ottenuto grazie alla
proporzione~\ref{proporzione_peso_pagerank}) e di $0.5$ per Proximity
(scelto per via di alcuni esperimenti condotti --- si veda il
paragrafo~\ref{paragrafo_pesi_ranker}).

{\footnotesize
{\setstretch{0.5}
\hspace*{\fill}\\
\Line
\begin{enumerate}
\item \texttt{http://www.adobe.it/products/acrobat/readstep.html}\\\hspace*{0.5cm}Acrobat Family
\item \texttt{http://www.canalerisparmio.it/}\\\hspace*{0.5cm}risparmio le offerte di lavoro e gli annunci
\item \texttt{http://www.html.it/}\\\hspace*{0.5cm}HTML.it
\item \texttt{http://www.repubblica.it/}\\\hspace*{0.5cm}Repubblica.it
\item \texttt{http://www.multiplayer.it/}\\\hspace*{0.5cm}Welcome to Multiplayer.it
\item \texttt{http://www.msn.it/global/incompbrowser.asp}\\\hspace*{0.5cm}Nota per gli utenti
\item \texttt{http://arianna.libero.it/}\\\hspace*{0.5cm}ARIANNA --- Il Motore di Ricerca Italiano
\item \texttt{http://www.marcomedia.it/}\\\hspace*{0.5cm}Marco Medi@ --- Computer Palmari - Pocket PC --- GPS WAP GSM GPRS UMTS
\item \texttt{http://www.xtend.it/}\\\hspace*{0.5cm}Xtend new media Italia --- soluzioni Internet, eBusiness e communication, [...]
\item \texttt{http://www.asm-settimo.it/informatica/}\\\hspace*{0.5cm}ASM Informatica Settimo Torinese\\
\hspace*{\fill}\\
\Line
\end{enumerate}
}
}

La stessa interrogazione, sottoposta a Google, individua invece subito
il sito desiderato.

Come si vede invece, non soltanto nei primi dieci risultati non
compare alcuna pagina afferente a Microsoft Corporation\footnote{Se
escludiamo l'unica pagina del sito \texttt{http://www.msn.it/}, che
peraltro contiene solamente una nota informativa per gli utenti.}, ma
non si trova neppure traccia di pagine di terzi riguardanti l'azienda
o i suoi prodotti. Questa interrogazione risulta emblematica nel
dimostrare che, da soli, PageRank e Proximity non bastano.  L'unico
fatto positivo di questo esperimento è che le pagine trovate ruotano
perlomeno attorno all'argomento ricercato, l'informatica; si sono
quindi sviluppate ulteriori euristiche, che verranno illustrate nei
prossimi capitoli, e che sono state introdotte proprio per affinare i
risultati.

%%%
%%% Fine spiegazione del perché PageRank e Proximity da soli non bastano
%%%

\subsection{TitleRank - punteggio sui titoli}
\label{paragrafo_titlerank}

Come si è visto PageRank\index{PageRank} e Proximity\index{Proximity}
da soli non possono assolutamente bastare per restituire all'utente
risultati mirati e competitivi.  Essendo i titoli una risorsa
preziosissima per le pagine, dovendo tipicamente riassumere in poche
parole il contenuto dell'intero documento, si è pensato di adottare
una qualche tipologia di punteggio che fosse basata sui titoli.

In generale, ciò che saremmo interessati a fare è vedere se il titolo
della pagina soddisfa --- interamente od in parte --- l'interrogazione
dell'utente, e in caso affermativo darne una qualche forma di
punteggio.

All'inizio si era pensato di dare un punteggio anche ai titoli che
soddisfacevano solamente in parte l'interrogazione dell'utente; un
esempio potrebbe essere un titolo del tipo: \texttt{microsoft
corporation} in relazione alla \textit{query}
\texttt{microsoft corp}.
In tal caso infatti l'interrogazione non sarebbe contenuta nel titolo
esaminato, tuttavia una parte di essa (la parola \texttt{microsoft})
lo sarebbe.  

Questa scelta non è tuttavia stata seguita da noi dal momento che il
software utilizzato per le interrogazioni e per determinare i
documenti che soddisfano una interrogazione dalla lettura degli indici
inversi non restituisce i \textit{match} parziali, ma solamente quelli
completi.  Non volendo complicare un codice che viene usato anche in
altri ambiti e che deve essere efficiente si è perciò scelto di
considerare solamente i titoli che soddisfano completamente
l'interrogazione.

Il punteggio\index{punteggio} che viene assegnato ad un titolo che
risulta soddisfare l'interrogazione è calcolato in maniera simile al
punteggio di Proximity\index{Proximity}, essendo dato da

\begin{equation}
s = \frac{1}{(L_{min} - N + 1)}
\end{equation}

dove $L_{min}$ è la lunghezza dell'intervallo minimo di \textit{match}
mentre $N$ è il numero di parole dell'interrogazione\footnote{O
meglio, come visto con la Proximity\index{Proximity}, la lunghezza
minima possibile di un intervallo di \textit{match}.}.

Si noti che qui, contrariamente a quel che accade con l'algoritmo di
Proximity\index{Proximity}, i valori che vengono assegnati sono, nella
stragrande maggioranza dei casi, valori vicini a $1.0$ oppure $0.0$,
essendo i titoli molto corti; infatti sotto tale ipotesi le parole
inserite nella \textit{query} dell'utente verranno a trovarsi molto
vicine nel titolo, quasi sempre a distanza $1$ o comunque a breve
distanza, ed il punteggio conseguente sarà normalmente più elevato di
quello della Proximity\index{Proximity}.

Inserendo anche questo algoritmo di \textit{ranking} all'interno del
sistema di aggregazione dei risultati discusso in precedenza (si veda
il capitolo~\ref{paragrafo_aggregazione}) la qualità delle ricerche
migliora sensibilmente fino ad arrivare già, su ricerche particolari,
a potersi confrontare con i motori di ricerca commerciali.

Vengono qui riportati i primi dieci risultati ottenuti attivando
l'algoritmo di TitleRank con peso $1.0$ sulla \textit{query}
\texttt{governo} (i risultati vengono combinati con PageRank e
Proximity, con i pesi discussi nel
paragrafo~\ref{paragrafo_pesi_ranker}):

{\footnotesize
{\setstretch{0.5}
\hspace*{\fill}\\
\Line
\begin{enumerate}
\item \texttt{http://www.governo.it/}\\\hspace*{0.5cm}Governo Italiano --- Home page
\item \texttt{http://www.governo.it/servizi/ricerca.asp}\\\hspace*{0.5cm}Governo Italiano --- Ricerca nel sito
\item \texttt{http://www.palazzochigi.it/}\\\hspace*{0.5cm}Governo Italiano --- Home page
\item \texttt{http://www.palazzochigi.it/servizi/ricerca.asp}\\\hspace*{0.5cm}Governo Italiano --- Ricerca nel sito
\item \texttt{http://www.maggioli.it/}\\\hspace*{0.5cm}MAGGIOLI - Il Partner del Governo Locale
\item \texttt{http://www.maggioli.it/index.htm}\\\hspace*{0.5cm}MAGGIOLI - Il Partner del Governo Locale
\item \texttt{http://www.progettoecotur.it/}\\\hspace*{0.5cm}Il Governo del Territorio tra beni culturali, [...]
\item \texttt{http://www.parusia.it}\\\hspace*{0.5cm}Il Governo del Territorio tra beni culturali, [...]
\item \texttt{http://www.prefettura.messina.it/default.htm}\\\hspace*{0.5cm}Ufficio Territoriale del Governo di Messina --- Homepage
\item \texttt{http://www.prefettura.messina.it/}\\\hspace*{0.5cm}Ufficio Territoriale del Governo di Messina --- Homepage\\
\hspace*{\fill}\\
\Line
\end{enumerate}
}
}

Come si può osservare, le pagine trovate grazie al peso ulteriore
attribuito contengono tutte quante nel titolo la parola ricercata,
\texttt{governo}. I documenti restituiti sono inoltre incentrati
sull'argomento ricercato, e non sono più estremamente fuorvianti come
accadeva con il solo utilizzo di PageRank e Proximity (si veda il
paragrafo~\ref{paragrafo_pagerank_proximity_non_bastano})

%%%
%%% Fine TitleRank
%%%

\subsection{URLRank - punteggio sulle URL}
\label{paragrafo_urlrank}

Nonostante l'introduzione di un algoritmo quale
TitleRank\index{TitleRank} all'interno del sistema di aggregazione
(vedi paragrafo~\ref{paragrafo_aggregazione}) un problema persistente è
quello di non trovare il sito web che si desidera trovare pur
fornendone il suo dominio.  Ad esempio, può capitare che ricercando
\texttt{html} su un insieme di pagine estratte dalla gerarchia
\texttt{.it} non si trovi in prima posizione il noto
\texttt{http://www.html.it/}.

Tale fenomeno è spiacevole e frustrante per l'utente medio,
che non si vede comparire in prima posizione il sito che
cercava. Questo è tuttavia proprio il comportamento che vogliamo
simulare, ossia quello di trovare un sito dando il suo nome a dominio.

Per sopperire a questa mancanza, al giorno d'oggi oramai presente solo
in motori di ricerca dai risultati molto scadenti, si è scelto di
creare un nuovo algoritmo che desse un punteggio anche alle URL delle
pagine. Questo \textit{ranker} è nella forma del tutto simile al già
presentato TitleRank\index{TitleRank}, dove, come si è detto, al posto
dei titoli vengono usate le URL delle pagine.

La parte interessante dell'intero algoritmo è tuttavia a monte, ossia
risiede nel recupero delle parole contenute nelle URL delle pagine.
Infatti alcuni domini, come ad esempio
\texttt{http://www.comunemilano.it/}, contengono due parole: nel
nostro caso le parole \texttt{comune} e
\texttt{milano}.  Quel che vorremmo è che la \textit{query}
\texttt{comune milano} risultasse positiva a questo algoritmo, e che
pertanto trovasse il sito in questione.  Per fare tutto ciò sarà
ovviamente necessario estrarre le parole di cui sono composte le URL,
per poter poi creare gli indici inversi\index{indici inversi}
associando ad ogni documento le parole di cui è composta la sua URL
(ammesso che per un dato documento vi siano parole contenute
nell'URL).  L'idea che si è sfruttata è stata quella di costruire un
\textit{ternary search tree}\index{ternary search tree}~\cite{trie}
con tutte le parole che si sono trovate analizzando il contenuto delle
pagine web\footnote{Eliminando preventivamente i
probabili\index{probabilità} errori di battitura, ossia considerando
solamente parole con frequenza superiore ad una certa soglia, posta a
100 nel nostro caso.} ed andando a fare una ricerca 
all'interno dell'albero così costruito.  

In tal modo le parole vengono emesse solamente nel momento in cui la
ricerca arriva ad una foglia dell'albero di ricerca, e si è sicuri di
non emettere nulla nel caso in cui una URL non contenga parole
esistenti.

Un punto importante risulta essere il seguente: cosa va considerato
come URL? Ossia, va scomposto soltanto il nome a dominio oppure anche
il percorso che contraddistingue un documento?  Nel nostro caso
abbiamo scelto il secondo approccio, essendo il primo alle volte
ingannevole e più suscettibile al fenomeno dello
\textit{spam}\index{spam} più volte illustrato: si pensi, ad esempio, ad
un utente malizioso che creasse percorsi lunghissimi contenenti
moltissime parole comuni.  Nell'implementazione fornita, si è inoltre
scelto di eliminare dalle URL prefissi e suffissi comuni, come
\texttt{http://}, \texttt{.com}, \texttt{.it}, \texttt{www.}, ecc.

Si noti infine che tenendo questi prefissi e suffissi e non eliminando
il percorso dalle URL si potrebbe fare ricerca per indirizzo delle
pagine: questo vorrebbe dire che se un utente inserisse l'URL di una
pagina web, tale pagina potrebbe venire trovata (cosa che, al momento,
non avviene).

Viene qui riportato un esempio di come URLRank riesca ad inviduare con
ragionevole precisione pagine degli \textit{host} che contengono, nel
nome del dominio, le parole ricercare. Nel caso in esame si è ricercato
\texttt{html} e i risultati con URLRank attivato (con peso\footnote{Si veda il
paragrafo~\ref{paragrafo_pesi_ranker}.} $1.0$) sono stati i seguenti:

{\footnotesize
{\setstretch{0.5}
\hspace*{\fill}\\
\Line
\begin{enumerate}
\item \texttt{http://www.html.it/}\\\hspace*{0.5cm}HTML.it
\item \texttt{http://www.html.it/guida/index.html}\\\hspace*{0.5cm}HTML.it --- Guida all'HTML
\item \texttt{http://www.virgilio.it/}\\\hspace*{0.5cm}VIRGILIO
\item \texttt{http://hosting.html.it/}\\\hspace*{0.5cm}HTML HOSTING --- le migliori offerte di hosting e housing in Italia
\item \texttt{http://download.html.it/}\\\hspace*{0.5cm}HTML DOWNLOAD --- Software per webmaster
\item \texttt{http://adsl.html.it}\\\hspace*{0.5cm}ADSL.html.it --- le offerte italiane di ADSL e HDSL
\item \texttt{http://webnews.html.it/}\\\hspace*{0.5cm}WEBnews.html.it --- L'informazione dall'internet News e articoli [...]
\item \texttt{http://webnews.html.it/focus/focus.htm}\\\hspace*{0.5cm}Tutti gli articoli | Focus | WEBNEWS.HTML.it
\item \texttt{http://webtool.html.it/forum/index.html}\\\hspace*{0.5cm}WEBtool.it --- Forum di discussione
\item \texttt{http://webtool.html.it/}\\\hspace*{0.5cm}WEBtool.it --- Servizi gratuiti per webmaster\\
\hspace*{\fill}\\
\Line
\end{enumerate}
}
}

Come si può vedere sono state trovate molte pagine tratte da un noto
sito italiano sul linguaggio di \textit{markup} HTML; le pagine
restituite hanno la particolarità di contenere\footnote{Tutte tranne
una, l'\textit{home-page} di Virgilio, in quella posizione a causa del
suo alto valore di PageRank.}, nel nome dello \textit{host} la parola
ricercata tramite il motore.  In prima posizione, in particolare,
appare l'\textit{home-page} vera e propria del sito in questione.
Questo fatto ben rappresenta l'idea alla base di URLRank: spesso ciò
che un utente vuole ricercare è il sito la cui URL contiene le parole
che ha inserito\footnote{Si veda, ad esempio, l'analisi
di~\cite{google911} nella sezione intitolata ``What Users Expected
from Google''.}.

%%%
%%% Fine URLRank
%%%

\subsection{AnchorRank - punteggio sulle ancore}
\label{paragrafo_ancore}

Una euristica che in letteratura~\cite{bharat01when} si ritiene sia
piuttosto buona è quella basata sul punteggio da associare alle
ancore, ossia al testo dei collegamenti ipertestuali presenti nei
documenti.

Il motivo per cui una tale euristica può funzionare è lo stesso per
cui funziona l'algoritmo di Hits\index{Hits} (vedi
paragrafo~\ref{paragrafo_hits}): le autorità\index{autorità}, ossia
le pagine importanti, sono puntate da molte altre pagine. Inoltre le
parole con le quali vengono puntate sono anche le parole con le quali
tali pagine sono conosciute, come ad esempio:

\begin{flushleft}
\texttt{agenzia ansa --- http://www.ansa.it/}\\
\texttt{george bush --- http://www.whitehouse.gov/president/}\\
\end{flushleft}

Tale euristica è utilizzata nel noto motore di ricerca
Google\index{Google} per stessa ammissione dei loro creatori (si
vedano~\cite{brin98anatomy} e~\cite{brin98what}) e mira ad individuare
le pagine in maniera esogena, ossia non sulla base di ciò che le
pagine dicono di trattare\footnote{Il che può essere fuorviante, nel
caso di pagine che fanno \textit{spamming}\index{spam}, oppure non
essere sufficiente, come in pagine fatte interamente di immagini, che
non sono ad oggi ancora pienamente indicizzabili.} ma sulla base di
ciò che gli altri affermano che loro trattino.

I risultati che si ottengono applicando questo algoritmo sono
sorprendenti, e utilizzare AnchorRank\index{AnchorRank} riesce a
migliorare di molto la sensazione di qualità che si ha andando a
vedere i link restituiti.  Inoltre questo algoritmo, almeno nella
nostra implementazione, rivaluta in un certo senso il cosiddetto
Frequency-count\index{Frequency-count} (vedi
paragrafo~\ref{paragrafo_frequencycount}), un
\textit{ranker} basato sul semplice conteggio del numero di intervalli
che soddisfano una data interrogazione.  Come si era notato,
l'algoritmo di conteggio della frequenza era molto suscettibile a
\textit{spam}\index{spam} quando veniva applicato alle pagine web; in
questo caso tuttavia il fatto che le ancore siano tra loro poco legate
rende il conteggio della frequenza un approccio realistico, in quanto
i creatori di tali ancore generalmente non si conoscono e quindi i
testi sono difficilmente influenzabili salvo casi molto particolari.
Va comunque tenuto conto che il punteggio dato dalle ancore viene in
realtà influenzato solamente in parte (e per di più in maniera
logaritmica) da questo punteggio dato da
Frequency-count\index{Frequency-count}. La parte più significativa è
ancora data da un punteggio basato su un conteggio di
Proximity\index{Proximity}.

In dettaglio, l'algoritmo di AnchorRank\index{AnchorRank} fornisce un
punteggio\index{punteggio} così calcolato:

\begin{equation}
s = \frac{N}{L_{min}} \cdot (1 + \ln(C))
\end{equation}

dove $C$ è il numero di occorrenze della interrogazione all'interno
delle ancore che puntano al documento, $N$ il numero di parole
contenute nella \textit{query}\footnote{O meglio, la lunghezza minima
possibile di un intervallo soddisfacente l'interrogazione.} e
$L_{min}$ la lunghezza minima dell'intervallo che soddisfa
l'interrogazione.  Il fatto che al logaritmo venga sommato $1$ non è
un caso: il motivo è che il comportamento desiderato è che nel caso di
un documento puntato con una sola ancora si desidera che il punteggio
del secondo fattore dia come risultato $1$, ossia che il suo valore
non influenzi il primo fattore. Nel caso invece che un documento
contenga più ancore, l'addendo aggiunto (cioè $1$) verrà di molto
mitigato dalla presenza del logaritmo.

Ad esempio, si supponga che la pagina
\texttt{http://www.microsoft.com/} sia puntata da altre $1000$ pagine
tramite $1000$ ancore. Tra queste $1000$ ancore, ve ne siano $900$
che, tra le altre parole, contengono le parole ``microsoft
corporation''. Si analizzi ora l'interrogazione
\texttt{microsoft corporation}. Sotto tali ipotesi il punteggio di
AnchorRank\index{AnchorRank} sarà quindi dato da: $(2/2) \cdot
(1 + \ln(900)) = 7.8023948$.

Questo è naturalmente soltanto uno dei possibili punteggi
assegnabili. Gli unici problemi riscontrati sono su alcune pagine che
sono state create \textit{ad hoc} per influenzare proprio algoritmi
basati su questa tecnica. L'unico modo per eliminare tali influenze
sembra essere quello di ricorrere a sofisticate quanto pericolose
euristiche di \textit{anti-spam}\index{spam} che non sono state
implementate prevalentemente perché non si è voluto correre il rischio
di proporre euristiche basate solamente sull'intuito di chi le scrive
e non necessariamente funzionanti in ogni caso.  Quel che però è certo
è che i motori di ricerca commerciali adottano tecniche del genere,
anche per loro stessa ammissione, ma spesso tali tecniche vengono
criticate proprio per la loro inefficacia e per il fatto di
penalizzare siti che non dovevano esserlo in quanto non portatori di
\textit{spam}\index{spam} al motore. A proposito di tali critiche
si possono leggere gli archivi di~\cite{searchengineforums}.

Altre tecniche possibili per l'assegnazione del punteggio e
probabilmente più resistenti allo \textit{spam}\index{spam} sarebbero
quelle di considerare soltanto il numero di ancore verso una data
pagina che contengono le parole dell'interrogazione, oppure
considerare soltanto il numero di pagine che contengono le ancore
verso una data pagina con testo che soddisfa l'interrogazione. In
questi due modi, ad esempio, il fattore logaritmico sarebbe meno
facilmente manipolabile.

\subsubsection{Estrazione di ancore}

L'estrazione delle ancore dai documenti raccolti è un procedimento non
solo lento ma anche non banale. Infatti, a parte le difficoltà
tecniche di fornire una implementazione adeguata, non è chiaro cosa
vada considerato testo dell'ancora e cosa no. Ad esempio, si potrebbe
considerare come testo dell'ancora tutto il testo del collegamento
ipertestuale più un altro numero di parole prima e dopo il link,
oppure da un segno di interpunzione precedente al link fino ad un
segno di interpunzione successivo, oppure ancora considerare come
ancora non soltanto il testo del link ma anche il testo attorno che
non sia separato da \textit{tag} HTML.  Di tutte le euristiche provate
quella di gran lunga più efficace è stata anche la più semplice
possibile: il testo di un ancora è solamente il testo contenuto
all'interno dell'elemento \texttt{A} dell'HTML.  Questa euristica
sembra anche essere quella adottata da Google\index{Google}, almeno da
quanto risulta dal \textit{reverse-engineering} fatto sulle
\textit{query}.

L'estrazione delle ancore tra l'altro può anche essere condizionata
dal fatto di considerare le ancore derivanti da pagine affiliate o
meno, od eventualmente dal come pesarle.  Nel nostro caso, per ridurre
il più possibile la possibilità di \textit{spam}\index{spam} diretto
od indiretto abbiamo scelto di considerare solamente le ancore
\textit{extra-host}, ossia i testi dei link esterni allo \textit{host}
su cui le pagine risiedono.  Il tal modo, ad esempio, un link
contenente la parola \texttt{microsoft} proveniente da
\texttt{http://www.microsoft.com/italy/} e diretto verso
\texttt{http://www.microsoft.com/} non verrebbe affatto considerato, mentre
verrebbe considerato tale link nel caso lo stesso link provenisse da
\texttt{http://www.zdnet.com/}.
Si possono ovviamente proporre anche altre euristiche, come viene
accennato in~\cite{brin98anatomy}.

I rischi dell'utilizzo delle altre tecniche appena esposte e di altre
che si potrebbero pensare è quello di introdurre molto rumore nei
risultati portando, ad esempio, il sito
\texttt{http://www.virgilio.it/}, molto puntato sulla rete italiana in
una vasta varietà di contesti e di parole utilizzate, in cima a
pressoché qualunque ricerca (cosa realmente successa in fase di
sperimentazione).

In un capitolo precedente si è accennato al fatto che le ancore
possono portare nuovi risultati rispetto a quelli trovati
dall'algoritmo di Proximity\index{Proximity}: questo può accadere nel
caso in cui una pagina non contenga le parole con cui è conosciuta,
come ad esempio i motori di ricerca che, al loro interno, non
contengono le parole ``motori di ricerca''.  Nel nostro caso abbiamo
deciso l'introduzione di questa variante, visti i benefici che si
hanno.  Sempre basandosi sul \textit{reverse-engineering}, sembra che
anche i motori di ricerca noti adottino questa politica.

A questo punto si potrebbe delineare una ulteriore euristica, che però
non è stata implementata: eliminare dal testo delle pagine le parole
inserite all'interno di un'ancora, visto che esse si riferiscono
generalmente alla pagina puntata e non alla pagina contenitore.
Questo porterebbe probabilmente non soltanto ad avere meno parole
all'interno delle pagine web e conseguentemente ad indicizzazione e
ricerche più rapide, ma anche a una migliore precisione nei risultati
visto che verrebbero eliminati i \emph{falsi positivi} causati dalla
comparsa delle parole dell'interrogazione all'interno dei link.  Se si
scegliesse di implementare tale euristica sarebbe tuttavia da valutare
con attenzione cosa farne esattamente dei testi delle ancore che
puntano a pagine non recuperate, vuoi perché il
\textit{crawl}\index{crawling} è stato interrotto, vuoi perché il link
non è più valido. Forse la miglior soluzione sarebbe di eliminare le
parole contenute nelle ancore delle pagine puntatori solamente se
la pagina puntata è stata recuperata.

\subsubsection{Risultati di AnchorRank}

Si riporta qui di seguito un esempio di ciò che accade chiedendo al
motore di ricerca \texttt{repubblica}, con e senza ancore.

Questo è il risultato utilizzando le ancore:

{\footnotesize
{\setstretch{0.5}
\hspace*{\fill}\\
\Line
\begin{enumerate}
\item \texttt{http://www.repubblica.it/}\\\hspace*{0.5cm}Repubblica.it
\item \texttt{http://www.repubblica.it/auto/index.htm}\\\hspace*{0.5cm}RepubblicAuto.com
\item \texttt{http://repubblica.extra.kataweb.it/repubblica/frameset[...]}\\\hspace*{0.5cm}la Repubblica Extra --- Il giornale in edicola
\item \texttt{http://www.quirinale.it/}\\\hspace*{0.5cm}Presidenza della Repubblica
\item \texttt{http://www.quirinale.it/presidente/ciampi.htm}\\\hspace*{0.5cm}Carlo Azeglio Ciampi - biografia
\item \texttt{http://www.senato.it/senato.htm}\\\hspace*{0.5cm}Senato della Repubblica - Italia
\item \texttt{http://www.senato.it/}\\\hspace*{0.5cm}Home page Parlamento Italiano
\item \texttt{http://www.finanza.repubblica.it/}\\\hspace*{0.5cm}Borsa
\item \texttt{http://www.repubblica.napoli.it/}\\\hspace*{0.5cm}Repubblica edizione di Napoli
\item \texttt{http://testo.camera.it/\_presidenti/}\\\hspace*{0.5cm}L'elezione del Presidente della Repubblica\\
\hspace*{\fill}\\
\Line
\end{enumerate}
}
}

Questo invece è il risultato se non si utilizzano le ancore:

{\footnotesize
{\setstretch{0.5}
\hspace*{\fill}\\
\Line
\begin{enumerate}
\item \texttt{http://www.repubblica.it/}\\\hspace*{0.5cm}Repubblica.it
\item \texttt{http://www.repubblica.it/speciale/formulauno/index.html}\\\hspace*{0.5cm}Repubblica.it/Formula Uno
\item \texttt{http://www.repubblicarts.repubblica.it/reparts/ita/index.jsp}\\\hspace*{0.5cm}la Repubblica of the ARTS
\item \texttt{http://www.repubblicarts.repubblica.it/reparts/ita/cal[...]}\\\hspace*{0.5cm}la Repubblica of the ARTS
\item \texttt{http://poll.repubblica.it/cgi-bin/sendpage/tellafriend.pl}\\\hspace*{0.5cm}Repubblica.it
\item \texttt{http://www.bologna.repubblica.it/}\\\hspace*{0.5cm}Repubblica edizione di Bologna
\item \texttt{http://www.bologna.repubblica.it/archivio/index.htm}\\\hspace*{0.5cm}Repubblica edizione di Bologna
\item \texttt{http://www.napoli.repubblica.it/}\\\hspace*{0.5cm}Repubblica edizione di Napoli
\item \texttt{http://www.milano.repubblica.it/}\\\hspace*{0.5cm}Repubblica edizione di Milano
\item \texttt{http://www.milano.repubblica.it/speciali/colaprico/}\\\hspace*{0.5cm}Repubblica edizione di Milano\\
\hspace*{\fill}\\
\Line
\end{enumerate}
}
}

Come si può notare, i primi dieci risultati restituiti dal motore sono
decisamente più significativi e di sicuro maggior interesse nel caso
si utilizzino le ancore.

%%%
%%% Fine AnchorRank
%%%

\newpage

\section{Pesi dei ranker nella combinazione lineare}
\label{paragrafo_pesi_ranker}

Si è detto in precedenza (vedi introduzione del
capitolo~\ref{paragrafo_aggregazione}) che gli algoritmi proposti sono
stati aggregati tra loro utilizzando una combinazione lineare.  I pesi
di tale combinazione lineare sono stati determinati empiricamente sia
sulla base degli effetti pratici ottenuti sui risultati a causa di
loro variazioni, sia sulla base di alcune considerazioni che verranno
qui di seguito esposte.  L'ideale sarebbe stato poter disporre di una
utenza eterogenea su cui fare dei veri e propri \textit{test}, ma non
avendo \textit{beta-tester} a cui far provare il motore i pesi sono
stati determinati principalmente sulla base del nostro
\textit{feedback}.

Per scegliere i pesi a tavolino ci si è resi conto che i valori di
PageRank\index{PageRank} delle pagine sono molto bassi dal momento che
il vettore di PageRank\index{PageRank} viene normalizzato ad $1$ dopo
ogni iterazione; gli altri algoritmi invece forniscono punteggi molto
più elevati, e pertanto il coefficiente di PageRank\index{PageRank}
nella combinazione lineare dovrà essere molto più alto degli altri,
per poter ristabilire l'equilibrio.  In seguito ad alcuni esperimenti
si è notata che vale la seguente proporzione per determinare il peso
$x$ da attribuire a PageRank\index{PageRank}:

\begin{equation}
1.000.000 : 500 = N : x
\label{proporzione_peso_pagerank}
\end{equation}

dove $N$ è il numero di pagine indicizzate per il quale si desidera
calcolare il peso dell'algoritmo di PageRank\index{PageRank}.

Per quanto riguarda gli altri algoritmi di \textit{ranking}, si
è scelto di dare dei pesi che fossero il più possibile equamente
distribuiti tra di loro.

Facendo alcune prove si è trovato, ad esempio, che il valore da
attribuire a Proximity\index{Proximity} può variare tra $0.5$ e $1.0$
senza sbalzi significativi nei risultati, come ad esempio
TitleRank\index{TitleRank} può venire pesato con un coefficiente pari
a $1.0$ o anche a $2.0$ mentre URLRank\index{URLRank} dovrebbe avere
un peso sempre pari ad $1.0$: un peso maggiore spingerebbe
artificiosamente in alto i documenti che sono ospitati su
\textit{host} contenenti nomi comuni; d'altra parte un valore inferiore
renderebbe vano l'uso dell'euristica basata sulle parole contenute nelle URL
che sta alla base di URLRank\index{URLRank}.

Discorso a parte meritano invece le ancore, che come si è visto non forniscono
un punteggio limitabile superiormente. Proprio per la mancanza di un
maggiorante per il punteggio, non si è potuto trovare, come nel caso di
PageRank\index{PageRank}, un fattore che andasse bene in ogni circostanza per
normalizzare in qualche modo il punteggio.  Se tuttavia si tiene conto del
fatto che AnchorRank\index{AnchorRank} offre un punteggio (molto) elevato solo
a nomi di dominio conosciuti o almeno abbastanza famosi e a pagine importanti,
il fatto di alzare --- anche di molto --- il punteggio complessivo di quei
documenti può non essere così spiacevole.  Pertanto si è scelto di assegnare un
coefficiente pari a $1.0$ anche ad AnchorRank\index{AnchorRank}.

%%%
%%% Fine discussione pesi dei ranker
%%%

\newpage

\section{Valutazione lazy dei match}

Nell'introduzione di questo capitolo si è spiegato che per determinare
le pagine che soddisfano una interrogazione bisogna almeno considerare
le pagine che l'algoritmo di Proximity\index{Proximity} restituisce.
Tuttavia per certe interrogazioni risulta problematica questa
valutazione, in quanto può portare rapidamente ad alzare i tempi di
ricerca fino a svariate decine di secondi; ciò accade solitamente con
interrogazioni contenenti \textit{stop-word}\index{stop-word}.

Esempi di interrogazioni particolarmente pesanti per il sistema su un
insieme di pagine italiane sono:

\begin{flushleft}
\texttt{comune di milano} (circa $158\,000$ pagine)\\
\texttt{la gazzetta dello sport} (circa $111\,000$ pagine)\\
\texttt{il corriere della sera} (circa $120\,000$ pagine)\\
\end{flushleft}

Inoltre vi sono delle interrogazioni che, pur non contenendo al loro
interno delle vere e proprie \textit{stop-word}\index{stop-word},
sono molto pesanti per venir valutate completamente. Ne è un esempio
lampante la
\textit{query} \texttt{milano} che su appena 16 milioni di pagine
italiane compare in oltre un milione e duecentomila documenti.

Per affrontare situazioni simili si è pertanto pensato di sviluppare
una euristica che permettesse di tagliare questa valutazione dopo
poche migliaia (o decine di migliaia) di \textit{match}.  Euristiche
simili vengono utilizzate anche dai motori di ricerca commerciali.

L'idea dalla quale si è partiti è che non ha senso calcolare tutti i
\textit{match} quando invece molte ricerche~\cite{silverstein98} affermano che gli
utenti di un motore di ricerca non vanno a vedere più in là delle
prime poche decine di risultati.  Questa considerazione non è affatto
errata: nelle pagine successive alla prima decina è solitamente molto
difficile trovare ciò che ci interessa.

Da un punto di vista tecnico, si è deciso anzitutto di fissare la
nostra attenzione sui primi quattrocento risultati, una cifra ridotta
rispetto a quella restituita dai motori di ricerca commerciali ma
comunque piuttosto abbondante per l'uso medio che se ne fa.  Quel che
si voleva fare era predire con una precisione elevata (diciamo al
95\%) i primi 400 risultati di una interrogazione senza dover però
valutare fino in fondo i risultati.  Come si è ampiamente illustrato,
nel nostro motore di ricerca sono stati implementati cinque differenti
algoritmi di ricerca. I più importanti sono tuttavia
PageRank\index{PageRank} e Proximity\index{Proximity} perché sono
quelli che riescono meglio di ogni altro a restituire i documenti più
importanti ai fini di una ricerca; gli altri algoritmi si limitano a
perturbare l'ordinamento restituito.

Pertanto si è scelto di valutare i \textit{match} dei documenti in
ordine di PageRank\index{PageRank} degli stessi; per fare ciò è stato
necessario introdurre il concetto di permutazione degli indici
inversi, un concetto che permette di leggere da disco i numeri di
pagina che soddisfano l'interrogazione in maniera che essi siano
ordinati in ordine di PageRank\index{PageRank} decrescente --- prima
le pagine più importanti, poi le meno importanti.

Una volta fatto questo si è provveduto a creare un strumento
automatico che, data una interrogazione, calcolasse il numero di
risultati che sarebbe stato necessario ``chiedere'' agli indici
inversi (permutati in ordine di PageRank\index{PageRank}) affinché si
inviduassero i primi 400 risultati con la precisione richiesta.  Come
insieme di interrogazioni non è stato purtroppo possibile avere a
disposizione un insieme eterogeneo di \textit{query} da sottoporre
allo strumento automatico; pertanto si è stilata una lista di circa
200 interrogazioni il più possibile diversificate da sottoporre al
motore per stimare la funzione voluta\footnote{Le \textit{query} erano
tutte di almeno due parole perché nel caso di interrogazioni
mono-parola sarebbe bastato analizzare i primi 400 risultati, essendo
solamente il PageRank\index{PageRank} a ``contare''.}.

I risultati trovati ci permettono di concludere che basta valutare
poche pagine per stimare ragionevolmente le prime 400 con una
precisione del 95\%, una precisione molto spesso più che sufficiente.
Questo risultato lo si può spiegare pensando al modo in cui vengono
create le pagine e alle parole con le quali vengono sottoposte le
interrogazioni.  Infatti, se consideriamo interrogazioni con parole
molto poco diffuse o che mirano ad individuare un argomento molto ben
preciso potremmo magari dover analizzare tutte le occorrenze, ma tali
occorrenze sarebbero poche per via dell'argomento ristretto e poco
trattato\footnote{Si pensi, ad esempio, a ciò che accade cercando il
nome di una persona che conosciamo ma che non è famosa nel vero senso
della parola.}. Si è detto che potremmo dover analizzare tutte le
occorrenze: questo perché probabilmente non vi saranno punti di
riferimento sull'argomento che permettano di scremare pagine con un
punteggio di PageRank\index{PageRank} elevato.

Le interrogazioni vaste invece, cioè quelle che tendono a portare con
sé milioni di \textit{match}, sono invece estremamente generali e
trattate da moltissime pagine web. In tal caso, pur essendo moltissime
le pagine che trattano di quell'argomento, ciò che l'utente tende a
voler individuare sono le pagine principali sull'argomento, quelle di
riferimento, quelle ``ufficiali'' o ``ufficialmente non
ufficiali''. Stiamo parlando di interrogazioni contenenti, ad esempio,
i nomi di \textit{star} di \textit{Hollywood}, di aziende famosissime,
di capi di stato o di nomi di nazioni.  In tale ipotesi è proprio
PageRank\index{PageRank} a fare la differenza, e grazie a questo
punteggio si può fermare la valutazione ancora prima di quanto accada
nel caso illustrato in precedenza: solitamente su interrogazioni molto
vaste si è visto che basta analizzare pochissime migliaia di
occorrenze per individuare quelle giuste, cioè quelle che l'utente si
aspetta di trovare.

Viene mostrato un grafico delle occorrenze da richiedere in funzione
delle occorrenze totali; tale grafico è stato ottenuto tramite la
valutazione appena illustrata, e mostra sull'asse delle \texttt{Y} il
numero di risultati che andrebbero richiesti per una interrogazione
che è soddisfatta dal numero di documenti riportato sull'asse delle
\texttt{X}. Sono state tracciate nello stesso grafico anche altre
due curve: la prima rappresenta l'\emph{inviluppo convesso} mentre la
seconda rappresenta quella che empiricamente sembra essere la miglior
approssimazione di tale inviluppo.

\begin{figure}[htp]
\begin{center}
\includegraphics{fig/rank_aggregation.eps}
\caption{La funzione di aggregazione $y = 80 \cdot \sqrt{x}$.}
\end{center}
\end{figure}

La funzione scelta appartiene alla classe di funzioni della forma:

\begin{equation}
y = A \cdot \sqrt{x}
\label{funzione_aggregazione}
\end{equation}

L'analisi empirica dei dati relativi allo \textit{store} del web
italiano utilizzato ha portato a scegliere $A = 80$.

La valutazione di questa funzione in realtà potrebbe essere
estremamente più precisa se si analizzassero un insieme di
interrogazioni più vasto, come ad esempio quello che si può ottenere
da qualche mese di analisi dei \textit{log} delle interrogazioni di
cui un motore di ricerca pubblico può disporre.  In particolare,
la curva trovata potrebbe anche appartenere in realtà ad altre
famiglie di funzioni continue note (come ad esempio la famiglia
logaritmica) oppure potrebbe essere molto più complessa.  Tuttavia va
altresì notato che, sebbene la funzione sia in un certo qual modo molto
approssimata essa, empiricamente, sembra ben comportarsi all'interno
dell'intera infrastruttura del motore di ricerca Ubi\index{Ubi}.

Rimane a questo punto solamente da chiarire come si possa determinare
il numero approssimativo di riscontri di una data interrogazione,
visto che la valutazione viene fermata tipicamente dopo qualche
migliaio di iterazioni. Questo dato inoltre è di vitale importanza per
valutare la funzione (\ref{funzione_aggregazione}) in quanto esso
rappresenta il valore $x$.

Per stimare tale quantità si assume che le pagine contengano le parole
in maniera più o meno distribuita. Ossia si assume che l'ordinamento
di PageRank\index{PageRank} permuti in maniera sufficientemente
casuale le pagine in maniera tale che pagine consecutive secondo tale
ordine trattino argomenti differenti.  A questo punto, in fase di
valutazione si esaminano tutti i \textit{match} fino a che l'indice di
un documento tra essi non supera una soglia $C$ prestabilita (nel
nostro caso $C = 1000$). Superata tale soglia, si determina il numero
$x$ di \emph{occorrenze previste} in modo lineare, secondo
l'equazione:

\begin{equation}
x = \frac{m}{C} \cdot N
\end{equation}

dove $m$ è il numero di \textit{match} trovati tra i documenti con
indice minore di $c$ e $N$ è il numero di documenti indicizzati.

%%%
%%% Fine valutazione lazy dei match
%%%

\newpage

\section{Un raffinamento: il clustering}
\label{paragrafo_clustering}

Nel corso dello sviluppo delle euristiche illustrate si è notato che
troppo spesso tra i risultati restituiti vi erano delle lunghe serie
di indirizzi appartenenti tutti al medesimo \textit{host}.

Ad esempio, interrogando il motore di ricerca per recuperare le pagine
che corrispondono all'interrogazione \texttt{parlamento} e senza
utilizzare l'euristica del \textit{clustering}\index{clustering}, ai
primi dieci posti si trovano queste pagine:

{\footnotesize
{\setstretch{0.5}
\hspace*{\fill}\\
\Line
\begin{enumerate}
\item \texttt{http://www.parlamento.it/}\\\hspace*{0.5cm}Home page Parlamento Italiano
\item \texttt{http://www.parlamento.it/home.htm}\\\hspace*{0.5cm}Home page Parlamento Italiano
\item \texttt{http://www.parlamento.it/parlam/leggi/home.htm}\\\hspace*{0.5cm}Indici delle leggi
\item \texttt{http://www.parlamento.it/senato.htm}\\\hspace*{0.5cm}Senato della Repubblica --- Italia
\item \texttt{http://www.parlamento.it/parlam/bicam/home.htm}\\\hspace*{0.5cm}Organismi bicamerali --- XIV legislatura
\item \texttt{http://www.parlamento.it/parlam/leggi/deleghe/dlattcee.htm}\\\hspace*{0.5cm}Elenco attuativi di direttive comunitarie
\item \texttt{http://www.parlamento.it/parlam/leggi/}\\\hspace*{0.5cm}Indici delle leggi
\item \texttt{http://www.parlamento.it/parlam/leggi/97059l.htm}\\\hspace*{0.5cm}Legge n. 59 del 1997
\item \texttt{http://www.parlamento.it/parlam/leggi/deleghe/99079dl.htm}\\\hspace*{0.5cm}Dlgs 79/99
\item \texttt{http://www.parlamento.it/parlam/leggi/98269l.htm}\\\hspace*{0.5cm}Legge n. 269 del 1998\\
\hspace*{\fill}\\
\Line
\end{enumerate}
}
}

Come si può vedere, tutte le pagine restituite risultano appartenere
al medesimo \textit{host}, ossia \texttt{parlamento.it}. Tuttavia
questo non è il risultato che desidereremmo, in quanto spesso si è più
interessati ad ottenere una lista più eterogenea di risultati,
comprendendo tra le prime dieci posizioni anche siti non afferenti
direttamente al Parlamento Italiano, ma ad esempio siti di terze parti
che trattino di leggi e di argomenti simili.

I risultati alla medesima interrogazione ma elaborati utilizzando
l'euristica qui proposta risultano essere invece i seguenti:

{\footnotesize
{\setstretch{0.5}
\hspace*{\fill}\\
\Line
\begin{enumerate}
\item \texttt{http://www.parlamento.it/}\\\hspace*{0.5cm}Home page Parlamento Italiano
\item \texttt{http://www.parlamento.it/home.htm}\\\hspace*{0.5cm}Home page Parlamento Italiano
\item \texttt{http://www.europadonna-parlamento.it/}\\\hspace*{0.5cm}Benvenuti su Europa Donna Parlamento
\item \texttt{http://www.europarl.it/}\\\hspace*{0.5cm}PARLAMENTO EUROPEO: Ufficio per l'Italia
\item \texttt{http://www.gruppi.margheritaonline.it/}\\\hspace*{0.5cm}Margherita in Parlamento
\item \texttt{http://www.senato.it/}\\\hspace*{0.5cm}Home page Parlamento Italiano
\item \texttt{http://www.senato.it/home.htm}\\\hspace*{0.5cm}Home page Parlamento Italiano
\item \texttt{http://www.cnnitalia.it/2002/MONDO/mediooriente/[...]}\\\hspace*{0.5cm}CNNItalia.it --- Parlamento iracheno disapprova la risoluzione dell'Onu [...]
\item \texttt{http://www.radio.rai.it/grparlamento/}\\\hspace*{0.5cm}GR Parlamento
\item \texttt{http://www.governo.it/rapp\_parlamento/index.html}\\\hspace*{0.5cm}Governo italiano --- Rapporti con il Parlamento\\
\hspace*{\fill}\\
\Line
\end{enumerate}
}
}

Dal momento che è poco utile ricevere una lista monotematica di siti
web si è pensato di sviluppare alcune euristiche atte a ridurre il
numero di indirizzi appartenenti al medesimo \textit{host} che
compaiono consecutivamente tra i risultati.

Sono state due le euristiche sviluppate nel corso della tesi, la prima
mirata a diminuire il numero di risultati dello stesso \textit{host} e
la seconda mirata a privilegiare pagine con una URL più breve.  Qui di
seguito vengono illustrate le due euristiche:

\begin{itemize}
\item la prima euristica prevede la scansione della lista degli URL
da restituire alla ricerca di URL appartenenti allo stesso
\textit{host}; per ottenere questo effetto si mantiene la posizione
dell'ultima URL vista per ogni \textit{host} incontrato, e nel momento
in cui si incontra un'altra URL di un host visto
recentemente\footnote{Quanto recentemente è descritto da una soglia
parametrizzabile.} la si accoda alla prima, indipendentemente dal suo
punteggio. In questo modo la lista dei risultati non è più
necessariamente ordinata per punteggio composito decrescente, ma i
risultati sono molto migliori;
\item la seconda euristica che è stata proposta è stata abbandonata
al momento dell'introduzione di AnchorRank\index{AnchorRank} perché
tendeva a peggiorare la qualità dei risultati; ad ogni modo, se
utilizzata senza AnchorRank\index{AnchorRank}, essa è una buona idea e
si basa sul fatto che generalmente URL più brevi sono anche più
rilevanti. Pertanto veniva fatto un ordinamento della lista trovata
tramite l'euristica al punto precedente ordinando in base alla
lunghezza in numero di \textit{slash} contenuti nelle URL e, se uguali
da questo punto di vista, in base alla lunghezza in numero di
caratteri.
\end{itemize}

In figura~\ref{codice_clustering} viene riportato il codice Java
utilizzato per fare \textit{clustering}\index{clustering} dei
risultati.

\newpage

\begin{figure}
{\tiny\tt
\begin{tabbing}
\hspace{0.3cm} \= \hspace{0.3cm} \= \hspace{0.3cm} \= \hspace{0.3cm} \= \hspace{0.3cm} \=\kill
\kw{private} IntArrayList clusterResults( \kw{final} int toBeClustered[] ) \{\\
\>long startTime, endTime;\\
\>startTime = System.currentTimeMillis();\\
\\
\>ObjectArrayList objectList = new ObjectArrayList();\comment{// temporary}\\
\>IntArrayList list = \kw{new} IntArrayList();\comment{// where we store results}\\
\>int i;\\
\\
\>\comment{/* the maximum distance used for clustering: if two URLs from the same host}\\
\>\comment{ * appear in this distance, then they are clustered together}\\
\>\comment{ */}\\
\>\kw{final} int DELTA = 100;\\
\\
\>Int2IntMap host2pos = \kw{new} Int2IntOpenHashMap();\\
\>host2pos.defaultReturnValue( -1 );\\
\>Int2IntMap lastFound = \kw{new} Int2IntOpenHashMap();\\
\>lastFound.defaultReturnValue( -1 );\\
\\
\>\kw{for} ( i = 0; i {\char60} toBeClustered.length; i++ ) \{\\
\>\>int host = graph2urlinfo[ toBeClustered[ i ] ];\\
\\
\>\>\kw{if} ( host2pos.get( host ) == -1 ) \{// host not yet seen\\
\>\>\>IntArrayList tmp = \kw{new} IntArrayList(); tmp.add( toBeClustered[ i ] );\\
\>\>\>objectList.add( tmp );\\
\\
\>\>\>host2pos.put( host, objectList.size() - 1 );\\
\>\>\}\\
\>\>\>\kw{else} \kw{if} ( i - lastFound.get( host ) {\char60}= DELTA ) \{\comment{// host already seen recently}\\
\>\>\>\>\kw{if} ( ( (IntArrayList)objectList.get( host2pos.get( host ) ) ).size() == 1 ) \{\\
\>\>\>\>\>( (IntArrayList)objectList.get( host2pos.get( host ) ) ).add( toBeClustered[ i ] );\\
\>\>\>\>\}\\
\>\>\>\}\\
\>\>\kw{else} \{\comment{// host already seen, but many results ago (i.e., more than DELTA)}\\
\>\>\>IntArrayList tmp = new IntArrayList(); tmp.add( toBeClustered[ i ] );\\
\>\>\>objectList.add( tmp );\\
\\
\>\>\>host2pos.put( host, objectList.size() - 1 );\\
\>\>\}\\
\>\>lastFound.put( host, i );\\
\>\}\\
\\
\>\comment{// now we sort each objectList so that {\char34}shortest{\char34} URLs from each host are shown first}\\
\>\comment{// ALERT: the heuristic below only works WITHOUT AnchorRank (you will receive bad results otherwise!)}\\
\>\kw{for} ( i = 0; i {\char60} objectList.size(); i++ ) \{\\
\>\>IntArrayList tmp = ( IntArrayList ) objectList.get( i );\\
\\
\>\>jal.INT.Sorting.sort( tmp.elements(), 0, tmp.size(), \kw{new} jal.INT.BinaryPredicate() \{\\
\>\>\>\>\kw{public} boolean apply( int x, int y ) \{\\
\>\>\>\>\>\kw{if} ( numSlashes[ x ] != numSlashes[ y ] ) \kw{return} numSlashes[ x ] < numSlashes[ y ];\\
\>\>\>\>\>\kw{else} \kw{return} urlLength[ x ] < urlLength[ y ];\\
\>\>\>\>\\
\>\>\>\>\}\\
\>\>\>);\\
\>\}\\
\>\kw{for} ( i = 0; i {\char60} objectList.size(); i++ ) list.addAllOf( ( IntArrayList ) objectList.get( i ) );\\
\>\kw{return} list;\\
\}
\end{tabbing}
}
\caption{Codice per il \textit{clustering}.}
\label{codice_clustering}
\end{figure}

%%%
%%% Fine discussione sul clustering dei risultati
%%%

\newpage

\newchap{Architettura ed implementazione del sistema}
\label{paragrafo_architettura}

In questa sezione si presenterà l'intera architettura che sta dietro
al motore di ricerca che si è sviluppato, il cui nome è
Ubi\index{Ubi}, nome derivato dal latino \textit{Ubicumque}, ossia
``per ogni dove''.

Si tenga presente che non tutto quel che verrà presentato in questo
capitolo è stato sviluppato nel corso della tesi: tutta
l'infrastruttura di \textit{crawling}\index{crawling}, ad esempio, è
stata sviluppata in precedenza da~\cite{boldi02ubicrawler} mentre gli
indici inversi\index{indici inversi} si appoggiano sul progetto
MG4J~\cite{mg4j}, che fornisce una implementazione Java di gran parte
delle tecniche di indicizzazione descritte in~\cite{witten99managing}.

\newpage

\section{Linguaggio utilizzato e considerazioni varie}

L'intero progetto è stato sviluppato nel linguaggio ad oggetti
Java\index{Java} e le motivazioni dietro a tale scelta sono
essenzialmente dettate dalle richieste insite nello sviluppo del
progetto: comodità di avere un linguaggio ad alto livello, orientato
agli oggetti, con codice prodotto che può essere eseguito su
architetture differenti senza alcun tipo di \textit{porting} e su cui
si possa facilmente fare \textit{debugging} in tempi ragionevoli
grazie ad un substrato di macchina virtuale che permetta di rilevare
agevolmente gli errori, disponendo di uno stack di esecuzione
facilmente accessibile e descritto in una forma intelligibile.
Inoltre un'ulteriore richiesta era quella del
\textit{rapid prototyping}, ossia della possibilità di scrivere
rapidamente prototipi di codice basandosi sull'abbondanza delle
librerie disponibili: strutture dati, \textit{multi-threading}, RMI e
rete.

Per alcune fasi dell'elaborazione dei dati sono già presenti
componenti che possono essere eseguite in
parallelo\index{parallelizzazione} su più macchine.  Per rendere
semplice l'interazione e la comunicazione tra le varie componenti e
per non dover gestire esplicitamente \textit{multi-threading} su parti
di codice si è scelto di sviluppare tali componenti tramite l'utilizzo
di RMI, ossia della \textit{Remote-Method Invocation}, una
funzionalità del linguaggio Java\index{Java} che permette di
richiamare metodi remoti e di eseguirli in maniera trasparente, come
se fossero noti alla macchina virtuale su cui gira il chiamante.

Il progetto può al momento contare su varie macchine: otto
\textit{personal computer} in \textit{rack} dislocati all'Istituto di
Informatica e Telematica del Consiglio Nazionale delle Ricerche
(presso l'Area di Ricerca di Pisa)~\cite{ubi} utilizzati per la fase
di \textit{crawling}\index{crawling}, tre \textit{personal computer
end-user} e un server SUN di grosse dimensioni dislocati al
Dipartimento di Scienze dell'Informazione dell'Università degli Studi
di Milano~\cite{dsi}. Quest'ultimo server, essendo un
quadri-processore dotato di quasi un terabyte di dischi e di 16
gigabyte di memoria RAM è la macchina su cui vengono elaborati e
mantenuti tutti gli indici di cui si discuterà più avanti.

Per avere degli standard con cui confrontarsi, durante tutto il corso
dello sviluppo della parte di analisi si è guardato al motore di
ricerca in assoluto più famoso e più preciso,
Google\index{Google}. Sebbene la tecnologia dietro a questo motore ---
e a tutti gli altri --- sia mantenuta segreta per ovvi motivi
commerciali, si è tentato più volte di fare del
\textit{reverse-engineering} tramite una attenta analisi delle
risposte alle interrogazioni.

\begin{figure}[htp]
\begin{center}
\includegraphics[width=12cm]{eps/metriche.eps}
\caption{L'evoluzione del codice prodotto, con evidenziato il periodo di tesi.}
\label{figura_metriche}
\end{center}
\end{figure}

Le classi di cui si compone il progetto nel momento della stesura di
questa tesi sono quasi $180$, per un totale di oltre $40000$ linee
complessive di codice. In figura~\ref{figura_metriche} viene riportato
l'andamento nel tempo delle linee di codice dell'intero progetto, e
viene evidenziato il periodo di questa tesi. Le metriche calcolate dal
programma ``statcvs'' riportano un totale di oltre $25000$ linee di
codice scritte o modificate per questa tesi.

In figura~\ref{figura_dataflow} viene visualizzato il flusso dei dati
soggiacente alla indicizzazione di uno \textit{store}.  I rettangoli
rappresentano gli strumenti utilizzati per l'indicizzazione delle
pagine e del testo in esse contenuto, mentre gli ovali rappresentano i
file prodotti (e successivamente utilizzati) da questi
strumenti\footnote{Per ragioni tipografiche non vengono riportati i
nomi degli strumenti utilizzati e dei file prodotti nelle apposite
caselle.}.

\begin{figure}[htp]
\begin{center}
\scalebox{0.1}{\epsfig{file=eps/codedataflow.eps}}
\caption{Flusso dei dati durante un'indicizzazione.}
\label{figura_dataflow}
\end{center}
\end{figure}

\newpage

\section{Tecniche e dettagli implementativi}
\label{paragrafo_dettagli_implementativi}

Si vuole qui fare un breve accenno ad alcune delle tecniche utilizzate
per scrivere il codice del progetto e, in particolare, per poter
gestire quantità di dati voluminose come quelle che si sono trattate.
Il problema forse più difficile di fronte al quale ci si è scontrati è
che purtroppo nel linguaggio Java non esiste una chiamata per
deallocare la memoria esplicitamente in stile \texttt{free()}, come
invece accade nel linguaggio C/C++; tutto viene demandato alla
\textit{garbage collector}, ossia ad un \textit{thread}
lanciato periodicamente ed in maniera automatica dalla Java Virtual
Machine (JVM) che si occupa di reclamare la memoria non più in uso dal
processo su cui viene invocato e libera lo spazio utilizzato dagli
oggetti non più in uso.  Sviluppando in Java ci si è accorti poi di
quanto fossero pesanti tali chiamate nel momento in cui si aveva
qualche milione di oggetti caricati in memoria (ad esempio quando si
costruiva o si gestiva il grafo): le chiamate al \textit{garbage
collector} erano, in questi casi, estremamente dispendiose in tempo,
fino a superare i dieci minuti\footnote{Ripetuti qualche volta nel
corso di varie ore di calcolo.}.  Pur accadendo in fasi di
precomputazione, tali tempi sono ben presto diventati inaccettabili e
si sono dovute trovare soluzioni più complicate per la simulazione
degli oggetti, come, ad esempio, l'allocazione di un unico vettore
atto a contenere il contenuto dei milioni di oggetti che venivano
utilizzati precedentemente.

Altri problemi che si sono riscontrati sono quelli dovuti al fatto che
gli oggetti e le strutture dati fornite dal linguaggio Java sono per
la maggior parte sincronizzate per il \textit{multi-threading} oppure
non propriamente pensate per gestire tipi primitivi\footnote{Anzi,
normalmente sono pensate per gestire oggetti.}.  Per tali ragioni ci
si è dovuti alle volte creare delle strutture dati ausiliarie, spesso
mere copie non sincronizzate o leggermente riadattate di quelle
fornite con il linguaggio Java, oppure quando si è dovuto ricorrere a
librerie esterne, come le COLT del CERN di Ginevra~\cite{colt}.

In particolare, il massiccio impiego di strutture dati per il
mantenimento di insiemi e funzioni ha richiesto l'utilizzo di
strutture specifiche per il trattamento dei tipi primitivi, come le
liste di COLT e gli insiemi e funzioni di fastutil~\cite{fastutil}
(essendo i dati primitivi non soggetti a \textit{garbage
collection}). Ad esempio, nella lettura delle liste di adiacenza dei
grafi si è scelto di restituire un iteratore di tipo
\texttt{IntIterator}, dopo aver opportunamente scompresso tale lista.

Per ottenere le prestazioni migliori possibili si è spesso ricorsi ad
opzioni poco documentate della JVM, come, ad esempio, quelle per
cambiare il tipo di \textit{garbage collector} (\texttt{-Xincgc},
\texttt{-Xconcgc}); una opzione che è stata molto utilizzata per
andare oltre i 32 bit di indirizzamento è stata l'opzione
\texttt{-d64}.  Altre opzioni (\texttt{-XX:+UseMPSS},
\texttt{-XX:+AggressiveHeap}) sono state accompagnate da un
\textit{tuning} della macchina su cui veniva eseguito il codice,
dovendo esse sfruttare al meglio il sistema operativo sottostante
(Solaris, della stessa SUN).

Nel corso della tesi siamo anche incappati in qualehe baco della JVM,
tutti riportati a SUN.  In particolare è interessante notare che il
primo di essi (bug-id: 4\,797\,189) era una
\textit{memory-leak} di cui ci si è accorti probabilmente per primi
solo perché il nostro codice eseguiva milioni di iterazioni, e la
perdita di anche soltanto poche decine byte di memoria ad ogni
iterazione può farsi sentire a lungo andare.

\newpage

\section{Fasi dell'indicizzazione}

Vengono qui di seguito presentate tutte le fasi necessarie al recupero
dei dati, alla loro elaborazione fino ad arrivare all'ultima fase,
l'utilizzo di tali dati da parte dell'utente finale.

\subsection{Prima fase: il crawling}

La prima fase dell'intero processo che porta ad avere un motore di
ricerca funzionante è il \textit{crawling}\index{crawling}, ossia il
recupero delle pagine web dalla rete.

Come già detto, i dati utilizzati in questa tesi sono stati ottenuti
tramite UbiCrawler~\cite{boldi02ubicrawler}, un sistema ad agenti che
si coordinano in maniera distribuita ed autonoma.

Ogni agente, che potenzialmente può girare su un \textit{host}
differente, inizia a recuperare le pagine web partendo da un
\emph{seme}, ossia da un insieme di URL inserite
manualmente.  L'agente visita tramite una ricerca mista in ampiezza ed
in profondità del grafo\index{grafo} del web queste pagine e le
recupera, salvandole in formato compresso in un file su disco.

\begin{figure}[htp]
\begin{center}
\scalebox{0.75}{\includegraphics{fig/crawling.eps}}
\caption{Un diagramma di flusso semplificato che illustra la fase di \textit{crawling}.}
\end{center}
\end{figure}

In tale file sono contenuti vari \textit{record}, uno per pagina
visitata. Ciascun record contiene l'indirizzo della pagina in
questione, gli \textit{header} restituiti dal web server
quando si è recuperata la pagina, la data di visita, la pagina
compressa più altre informazioni facilmente recuperabili anche a
posteriori ma che è utile, per vari scopi, avere già pronte --- questo
ovviamente a scapito dello spazio su disco.

In particolare ogni agente mentre recupera le pagine fa anche il
\textit{parsing} delle stesse per cercare nuove URL da
recuperare e visitare; tali URL, se nuove al sistema (ossia se nessun
altro agente le ha già incontrate) vengono inserite in una lista che
sarà la coda di visita.  Il procedimento continua fintantoché non
termina o, come avviene nel nostro caso, non viene interrotto perché
si è raggiunto, ad esempio, il massimo spazio su disco.

Si noti che in questa fase si potrebbero anche già fare altre analisi
che invece vengono fatte a posteriori: ad esempio, ogni agente, mentre
analizza la pagina recuperata per cercare nuove URL, potrebbe estrarre
il testo in esse contenuto, oppure il testo contenuto nei collegamenti
(ossia il testo contenuto nell'elemento \texttt{A} dell'HTML) ---
tutte operazioni che vengono al momento svolte a posteriori. Il motivo
per cui ciò non viene ancora fatto è che non solo è logicamente più
corretto suddividere le varie fasi --- anche per banali ragioni di
\textit{debugging} --- ma anche che lo spazio disco sulle macchine
utilizzate per il recupero delle pagine è abbastanza limitato.

Tipicamente, le collezioni di pagine così ottenute (chiamate anche
\textit{store}) vengono unite insieme con il comando UNIX
\texttt{cat} dopo essere state spostate sul server SUN sopra
menzionato. Il file risultante tipicamente supera di gran lunga il
centinaio di gigabyte se il numero di pagine recuperate è di circa
quindici milioni.

Un aspetto interessante che vale la pena di osservare è che la
gestione di enormi quantità di dati evidenzia quanto l'architettura
hardware e software dei comuni \textit{personal computer} sia poco
affidabile; nel corso degli esperimenti, e nella manipolazione dei
dati, abbiamo spesso verificato problemi di varia natura, sia
nell'hardware che nel software.  Infatti non una sola volta gli
\textit{store} sono stati recuperati senza errori e senza problemi
inspiegabili che inseriscono un bit errato una volta ogni qualche
centinaio di miliardi, causando problemi per l'elaborazione
successiva.  Probabilmente per gli usi che si fanno tutti i giorni i
personal computer non hanno bisogno di controlli di parità sulle
memorie, \textit{array} ridondanti di dischi, ecc. ma per gestire
quantità di dati così elevate tali controlli risultano vitali.

\subsection{Seconda fase: l'estrapolazione dei dati}

Una volta che si hanno questi enormi \textit{store} a disposizione, la
prima cosa da fare è estrarre da essi il maggior numero di dati
possibili per poterli poi indicizzare. Molti degli strumenti che si
andranno ad elencare qui di seguito sono stati sviluppati nel corso
della tesi.

Anzitutto si estraggono gli indirizzi delle pagine web recuperate e li
si ordina alfabeticamente.  Il motivo di tale scelta è stato spiegato
nel paragrafo~\ref{paragrafo_compressione_grafo_web}.

Una volta estratte tali URL si calcola il \emph{codice di ridondanza
ciclica} (CRC) a 64 bit di ciascuna di esse; il CRC viene usato per
poter occupare meno spazio in memoria nel momento in cui esse andranno
caricate. L'idea del CRC è molto utilizzata in letteratura, ma uno dei
problemi in cui si è incappati è il cosiddetto \emph{paradosso del
compleanno}\index{paradosso del compleanno}.  Infatti nel nostro caso
l'insieme su cui vengono calcolati i CRC spesso è un insieme molto
grande di elementi, e la probabilità che due di essi abbiano il
medesimo CRC diventa molto alta piuttosto in fretta all'aumentare
delle dimensioni dell'insieme base.  Questo problema si è in
particolare verificato tentando di mantenere in memoria tramite i CRC
le parole contenute nelle pagine: questo approccio, pur funzionando
con piccole quantità di dati, è diventato immediatamente ingestibile
non appena si sono recuperati qualche milione di pagine. Al momento
l'approccio a CRC viene ancora utilizzato per le URL poiché, essendo
esse in numero molto minore, il paradosso del compleanno ha una esigua
probabilità di verificarsi.

Si noti che a questo punto, avendo ordinato le URL, non c'è più una
corrispondenza tra l'ordine di visita --- che è anche l'ordine in cui
tali indirizzi sono salvati nello \textit{store} --- ed il nuovo
ordine, che chiameremo l'ordinamento del grafo\index{grafo}.  Dal
momento che è importante poter tornare indietro nell'associazione si
rende pertanto necessaria una mappa da interi ad interi per poter
ricostruire l'ordinamento originale.

Vi sono numerosi altri dati da estrarre dallo \textit{store}: non si è
infatti ancora fatta menzione di tutto il testo delle pagine --- testo
che ovviamente vorremmo in qualche modo tenere.  A causa degli
algoritmi implementati nel motore di ricerca, servono quattro tipi di
dati testuali:

\begin{itemize}
\item il testo delle pagine, punteggiatura esclusa, con ogni
parola nell'ordine in cui essa compare;
\item il testo dei titoli delle pagine, ossia le parole --- sempre
punteggiatura esclusa --- racchiuse all'interno dell'elemento
\texttt{TITLE} dell'HTML;
\item le parole contenute nelle URL incontrate: ad esempio,
nel caso dell'URL \texttt{http://www.comunemilano.it/} vorremmo
individuare al suo interno le parole \texttt{comune} e \texttt{milano};
\item le parole con cui una pagina viene puntata dalle altre, ossia
i testi delle ancore con cui una pagina è puntata.
\end{itemize}

Si consideri una sequenza massimale di lettere e numeri che si trova
in una pagina web. Sia ora una \emph{parola} una sottosequenza delle
precedenti e che inoltre rispetti regole più stringenti sulla sua
lunghezza massima, sul numero di cifre contenute, ecc.\footnote{Non
viene qui fornita la regola esatta con la quale vengono ricavati i
termini.}. Sono proprio le parole ad essere utilizzate in tutte le
fasi dell'elaborazione, in quanto le sequenza massimali di lettere e
numeri\footnote{Ossia quelle che comunemente sono riferite come
parole.} nelle pagine web sono troppe.  Recuperando infatti pochi
milioni di pagine e considerando una parola come una sequenza di
lettere e numeri separata da simboli di punteggiatura (spazi inclusi),
le sequenze massimali che vengono estratte dalle pagine web tendono a
diventare un numero ben presto ingestibile. In questo spiacevole fatto
entrano in gioco non solo le molteplici lingue in cui tali pagine sono
scritte, ma anche gli errori di punteggiatura, i numeri di telefono, i
numeri di codice dei prodotti, ecc.  Non potendo in alcun modo gestire
per 10 milioni di pagine recuperate 20 milioni di sequenze massimali,
si sono studiate alcune euristiche per ridurne il numero, come ad
esempio quella di spezzare le parole più lunghe di una certa lunghezza
o che contengono troppi numeri.  Si noti che una tale euristica non
pregiudica nella maniera più assoluta la possibilità di trovare, ad
esempio, una sigla farmaceutica se essa è stata spezzata: infatti, se
pervenisse al sistema una interrogazione contenente, ad esempio, la
sigla farmaceutica in questione, il sistema provvederebbe a spezzare
in vari termini la parola, e a ricercare ciascuno di essi
consecutivamente.  Il grosso vantaggio di questo modo di procedere,
essendo la lunghezza massima di una sequenza massimale in qualche modo
ridotta da un maggiorante ben stabilito, è che il numero di parole
estratte è forzatamente limitato.

\subsection{Terza fase: costruire il grafo}

Una volta effettuata l'estrazione dei dati è tempo di costruire il
grafo\index{grafo} sul sottoinsieme di pagine che sono state
recuperate. Il grafo\index{grafo}, come visto nei capitoli precedenti,
è fondamentale per poter eseguire gli algoritmi presentati nel
capitolo~\ref{capitolo_algoritmi_su_grafo}.  Il grafo\index{grafo} che
verrà prodotto\footnote{In realtà in forma compressa, come visto nel
paragrafo~\ref{paragrafo_compressione_grafo_web}.} avrà come nodi le
pagine web recuperate, mentre come archi orientati i link che
collegano una pagina alle altre. A seconda del numero di pagine
recuperate, tale grafo\index{grafo} potrà essere sparso oppure
denso. Studi~\cite{BFS_high_quality} dimostrano che una visita in
ampiezza\index{visita in ampiezza} porta in fretta a recuperare
comunque pagine di qualità, e quindi potenzialmente molto puntate
anche da una ristretta porzione del web; basandosi su tale risultato
anche \textit{crawl}\index{crawling} interrotte prematuramente
porteranno alla produzione di grafi significativi.  Durante la
costruzione del grafo\index{grafo} si eliminano i link che puntano a
pagine esterne alla nostra collezione o non esistenti.  Vengono invece
mantenute le pagine con grado uscente\index{grado uscente} nullo,
delegando pertanto ad algoritmi quali PageRank\index{PageRank} il
compito di gestire opportunamente queste situazioni, peraltro
piuttosto comuni (per esempio tutti i documenti di puro testo
rientrano in questa categoria).  La costruzione del grafo\index{grafo}
produce anche una mappa ulteriore, che va dai numeri di nodo (ossia il
numero progressivo che viene assegnato durante l'ordinamento delle URL
dello
\textit{store}) alle posizioni delle pagine corrispondenti nello
\textit{store} su disco.

Creato il grafo\index{grafo}, il passo successivo è di eseguire gli
algoritmi che richiedono il grafo\index{grafo} per funzionare.

\subsection{Quarta fase: gli indici inversi}

Dai dati testuali estrapolati nella seconda fase è necessario
costruire gli indici inversi\index{indici inversi}, ossia degli indici
che, per ogni parola, forniscano i documenti (e le posizioni) nelle
quali essa compare. Tali indici vengono utilizzati per la ricerca
testuale quando viene fatta un'interrogazione al motore di ricerca, e
sono uno strumento molto diffuso usato per rispondere in maniera
efficiente e rapida a qualunque interrogazione. Per la loro creazione
e il loro utilizzo come già detto ci si è avvalsi del progetto
MG4J~\cite{mg4j}.  La creazione di tali indici non è al momento
distribuita, e pertanto questa fase risulta essere relativamente
laboriosa e lenta; non essendo tuttavia il tempo un requisito al
momento stringente, la parallelizzazione\index{parallelizzazione} del
calcolo non ci è sembrata essenziale ed è stata rimandata a futuri
sviluppi dell'intera architettura.

Si noti che gli indici inversi\index{indici inversi} vengono creati
permutando gli indici dei documenti in ordine del punteggio di
PageRank\index{PageRank} (ordinamento decrescente).  Il motivo di
questa scelta verrà spiegato nella sezione dedicata all'aggregazione
(vedi capitolo~\ref{paragrafo_aggregazione}) ma si fa notare qui che
tale permutazione richiede un'ulteriore mappa che indica in quale
ordine rispetto a PageRank si trova ciascuna pagina dello
\textit{store}.

\subsection{Quinta fase: gli agenti e la servlet}
\label{paragrafo_agenti_servlet}

Una volta che tutti gli indici sono stati creati e che tutti gli
algoritmi precedentemente eseguiti hanno prodotto i loro output è
possibile eseguire gli agenti preposti a rispondere alle
interrogazioni ed è possibile anche usufruire dell'interfaccia web
creata come \textit{front-end}.

\begin{figure}[htp]
\begin{center}
\includegraphics[width=13cm]{eps/ubi_in_azione.eps}
\caption{L'output della \textit{servlet} all'interno di un \textit{browser}.}
\end{center}
\end{figure}

Gli agenti comunicano con l'interfaccia \textit{front-end}, anch'essa
scritta in Java\index{Java}, tramite RMI e pertanto tutta
l'infrastruttura è \textit{multi-threaded} e risulta quindi possibile
già fin d'ora rispondere a più richieste contemporaneamente.

Gli agenti sono al momento due, e possono essere eseguiti su macchine
differenti, a patto che ovviamente su entrambe vi siano i file
necessari:

\begin{itemize}
\item l'agente che risponde alle richieste indicando i punteggi delle
pagine e i numeri di pagina che soddisfano la ricerca;
\item l'agente che, dato un numero di pagina, estrae la pagina
dallo \textit{store}, ne produce il sommario in relazione
all'interrogazione (evidenziando nella pagina l'intervallo che
soddisfa l'interrogazione) e permette di accedere alla copia della
pagina che è stata recuperata a tempo di
\textit{crawl}\index{crawling} dalla rete.
\end{itemize}

\newpage

\section{Il processo di vita di una query}

In questo paragrafo verrà esaminato l'intero processo che parte
dall'inserimento di una interrogazione nel motore di ricerca fino alla
relativa risposta che viene fornita all'utente.

Quando un utente apre la pagina principale di Ubi\index{Ubi} in realtà
fa eseguire dal \textit{webserver} una \textit{servlet} Java che si
occupa di produrre il codice HTML che comprende il modulo per inserire
le interrogazioni.  Una volta introdotta l'interrogazione e sottoposta
al \textit{webserver}, la \textit{servlet} si occupa semplicemente di
spedirla al primo degli agenti descritti nella
sezione~\ref{paragrafo_agenti_servlet}, ossia all'agente in grado di
restituire le pagine che soddisfano l'interrogazione in un ordine che
rispecchi il più possibile le aspettative dell'utente. Come detto
precedentemente, la servlet comunica con l'agente tramite una chiamata
RMI, che secondo alcune misurazioni compiute durante la tesi non
inficia particolarmente le prestazioni ma porta notevoli benefici in
termini di semplicità di progetto e di facilità di
\textit{debugging}. Eventualmente, in fase di produzione, tale protocollo
potrebbe essere sostituito a favore di protocollo di comunicazione più
leggero.

\begin{figure}[htp]
\begin{center}
\scalebox{0.5}{\includegraphics{fig/query_process.eps}}
\caption{Il processo di vita di una \textit{query}.}
\end{center}
\end{figure}

Una volta giunta all'agente, l'interrogazione viene inviata in serie a
vari \textit{ranker}, ossia a vari algoritmi che sono in grado di
assegnare un punteggio ad ogni pagina dell'intera collezione. In
particolare il primo \textit{ranker} ad essere chiamato in gioco è
l'algoritmo di Proximity\index{Proximity}, che, come visto in un
capitolo precedente (vedi paragrafo~\ref{paragrafo_proximity}),
seleziona le pagine che soddisfano l'interrogazione e assegna loro un
punteggio compreso tra $0.0$ e $1.0$.  Successivamente, un algoritmo
basato sulle ancore e che è stato discusso nel
paragrafo~\ref{paragrafo_ancore}, permette di trovare ulteriori pagine
che soddisfano l'interrogazione, e l'insieme di pagine da valutare e
da restituire viene così ampliato. Assieme alle nuove pagine compare
anche una indicazione sul nuovo punteggio di questo algoritmo.

I successivi algoritmi con i quali vengono raffinati i risultati sono
però diversi dai due appena presentati in quanto non portano nuove
pagine all'interno dell'insieme di risultati, ma bensì si limitano a
dare un punteggio alle pagine già trovate.  Si noti che la prima
scrematura operata da questi due \textit{ranker} è di fondamentale
importanza perché permette di limitare la valutazione a poche migliaia
o decine di migliaia di pagine web contro vari milioni o miliardi
disponibili, mentre un algoritmo quale PageRank\index{PageRank} da
solo non potrebbe far altro che restituire tutte le pagine recuperate
in fase di \textit{crawl}\index{crawling}, ciascuna con il suo
punteggio.

Gli algoritmi chiamati successivamente in causa sull'insieme di pagine
così determinato sono:

\begin{itemize}
\item PageRank\index{PageRank}, vedi capitolo~\ref{paragrafo_pagerank};
\item TitleRank\index{TitleRank}, vedi paragrafo~\ref{paragrafo_titlerank};
\item URLRank\index{URLRank}, vedi paragrafo~\ref{paragrafo_urlrank}.
\end{itemize}

Una volta ottenuti tutti questi risultati, essi vengono tra loro
combinati tramite una combinazione lineare con pesi forniti a priori
od eventualmente scelti dall'utente che ha inserito la
\textit{query}. In questo modo, una volta ordinati i risultati
in base a questo punteggio composito così calcolato, si ha già una
lista di risultati abbastanza corretti da poter restituire all'utente.

Tuttavia una cosa che si è notata è che è necessario
\textit{clusterizzare} questo insieme di risultati per \textit{host} di
appartenenza per prevenire la spiacevole situazione nella quale una
serie molto lunga di risultati siano tutte pagine dello stesso sito
web.  Per evitare ciò si sono ideate alcune euristiche che verranno
presentate successivamente (vedi paragrafo~\ref{paragrafo_clustering})
e che permettono di restituire pochi risultati consecutivi per
ogni sito web, offrendo così una lista che spazi il più possibile
sulle pagine realmente rilevanti.  Per elaborare questa euristica ci
si è ispirati a quello che accade con i motori di ricerca commerciali
--- Google\index{Google} \textit{in primis} --- che mostrano soltanto
poche pagine per \textit{host} consecutivamente.

A questo punto i risultati così elaborati vengono restituiti, sempre
via RMI, alla \textit{servlet} che estrae la parte che interessa
all'utente (ossia la parte che corrisponde alle pagina di risultati
che sta visualizzando) e si occupa di invocare il secondo agente,
quello che permette di risolvere i numeri di documento nelle pagine
vere e proprie e nei relativi sommari.

Il secondo agente viene invocato pertanto sempre via RMI su ogni
pagina da risolvere, passandogli un numero di documento ed un
intervallo da evidenziare nel sommario. Tale agente si occupa di
posizionarsi nello \textit{store} in corrispondenza della pagina che
interessa e di estrarne il contenuto, creando poi il sommario basato
sull'intervallo di parole richiesto.  Viene quindi restituito un
oggetto che rappresenta un singolo risultato dell'interrogazione fatta
dall'utente e che la \textit{servlet} si occuperà di visualizzare
all'interno del \textit{browser} dell'utente.

Esiste poi la possibilità di recuperare le pagine che sono state
salvate nello \textit{store} per poter vedere i contenuti che sono
stati restituiti al \textit{crawler}. Questa possibilità è data da un
ulteriore agente che, se invocato con un numero di pagina, si occupa
di recuperare e restituire al chiamante il testo HTML della pagina
così come essa appare nello \textit{store}.

%%%
%%% Fine architettura ed implementazione del sistema
%%%

\newpage

\newchap{Conclusioni e sviluppi futuri}

Il campo dell'\textit{information retrieval} su web è oggi in pieno
fermento.  Quel che si è cercato di mostrare con la stesura di questa
tesi è che non esiste un solo modo per ricercare le informazioni che
interessano all'interno di quella enorme banca dati che è il web, ma
che al contrario esistono una miriade di euristiche più o meno
complicate che si possono adottare per migliorare la qualità delle
ricerche effettuate dal proprio motore.

Al momento della stesura di questa tesi, la componente del
grafo\index{grafo} del web raggiungibile e conosciuta dai
\textit{robot} dei vari motori di ricerca commerciali supera i tre
miliardi di pagine, ma come detto tale quantità è destinata ad
aumentare sempre più.  Essendo il materiale indicizzabile in crescita
esponenziale, l'unico modo per poter sperare di riuscire a mantenere
il passo di questa crescita è quello di continuare a studiare
euristiche che limitino il fenomeno dello \textit{spam}\index{spam},
che trovino tra le prime posizioni i documenti di interesse e che
riescano ad indicizzare quantità di dati sempre più voluminose.

Il software che è stato sviluppato nel corso della tesi ha permesso di
indicizzare notevoli quantità di dati e di recuperare all'interno di
questo consistente numero di pagine e a fronte di una interrogazione
dell'utente, le pagine più significative. Su questa strada si può
ancora procedere rendendo distribuite tutte le procedure in maniera
che possano più agilmente scalare quando il numero di pagine raggiunga
dimensioni realistiche per un motore commerciale. Altre tecnologie da
esplorare sono quelle rivolte all'eliminazione dello
\textit{spam}\index{spam}, alla ricerca e alla catalogazione di
immagini, video, suoni e notizie di giornale.

%%%
%%% Fine conclusione e sviluppi futuri
%%%

\newpage
\pagestyle{plain}

\listoffigures

%%%
%%% Fine indice delle figure
%%%

\newpage

{\setstretch{1}
\printindex
}

%%%
%%% Fine indice analitico
%%%

\newpage

\bibliography{biblio}
\bibliographystyle{unsrt}

%%%
%%% Fine bibliografia
%%%

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
